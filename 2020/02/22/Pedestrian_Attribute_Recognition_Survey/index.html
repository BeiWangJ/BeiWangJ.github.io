<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Pisces","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="梳理 Human Parsing(人体解析) 相关介绍，数据集，算法">
<meta property="og:type" content="article">
<meta property="og:title" content="Pedestrian Attribute Recognition Survey">
<meta property="og:url" content="http://yoursite.com/2020/02/22/Pedestrian_Attribute_Recognition_Survey/index.html">
<meta property="og:site_name" content="Bei&#39;s Blog">
<meta property="og:description" content="梳理 Human Parsing(人体解析) 相关介绍，数据集，算法">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/images/PAR/foreword.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/Intro2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/Intro3.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/Intro4.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/Intro5.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/ACN1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/DEEPSAR1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/DEEPSAR2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/DEEPSAR3.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/DEEPSAR4.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/MTCNN1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/MTCNN2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/HydraPlus-Net1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/HydraPlus-Net2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/HydraPlus-Net3.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/HydraPlus-Net4.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/HydraPlus-Net5.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/VeSPA1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/VeSPA2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/VeSPA3.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/DIAA1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/DIAA2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/DIAA3.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/DIAA4.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/DIAA5.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/DIAA6.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/CAM1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/CAM2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/CAM3.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/CAM4.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/CAM5.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/CAM6.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/MTCT1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/CILICIA1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/CILICIA2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/DCSA1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/A-AOG1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/A-AOG2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/A-AOG3.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/PARGCN1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/PARGCN2.jpg">
<meta property="og:image" content="http://yoursite.com/images/PAR/PARGCN3.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/PARGCN4.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/VSGR1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/VSGR2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/WPAL1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/WPAL2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/WPAL3.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/WPAL4.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/WPAL5.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/AWMT1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/AWMT2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/AWMT3.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/Poselets1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/Poselets2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/Poselets3.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/RAD1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/RAD2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/PANDA1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/PANDA2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/PANDA3.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/MLCNN1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/AAWP1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/AAWP2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/AAWP3.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/ARAP1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/ARAP2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/ARAP3.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/ARAP4.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/ARAP5.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/DeepCAMP1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/DeepCAMP2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/DeepCAMP3.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/DeepCAMP4.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/DeepCAMP5.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/PGDM1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/PGDM2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/PGDM3.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/PGDM4.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/DHC1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/DHC2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/LGNET1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/LGNET2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/LGNET3.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/CNNRNN1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/CNNRNN2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/JRL1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/JRL2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/GRL1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/GRL2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/GRL3.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/JCM1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/JCM2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/JCM3.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/JCM4.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/JCM5.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/JCM6.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/RCRA1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/RCRA2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/RCRA3.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/RCRA4.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/RCRA5.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/RCRA6.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/dataset_overview.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/PETA1.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/PETA2.png">
<meta property="og:image" content="http://yoursite.com/images/PAR/WIDER1.png">
<meta property="article:published_time" content="2020-02-22T07:20:01.463Z">
<meta property="article:modified_time" content="2020-02-22T13:12:18.054Z">
<meta property="article:author" content="Bei">
<meta property="article:tag" content="PAR">
<meta property="article:tag" content="cv">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/images/PAR/foreword.png">

<link rel="canonical" href="http://yoursite.com/2020/02/22/Pedestrian_Attribute_Recognition_Survey/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>Pedestrian Attribute Recognition Survey | Bei's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Bei's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-right"></div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/22/Pedestrian_Attribute_Recognition_Survey/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Bei">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bei's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Pedestrian Attribute Recognition Survey
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-02-22 15:20:01 / 修改时间：21:12:18" itemprop="dateCreated datePublished" datetime="2020-02-22T15:20:01+08:00">2020-02-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/cv/" itemprop="url" rel="index">
                    <span itemprop="name">cv</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>梳理 Human Parsing(人体解析) 相关介绍，数据集，算法</p>
<a id="more"></a>
<!-- toc -->

<hr>
<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><hr>
<h2 id="Foreword"><a href="#Foreword" class="headerlink" title="Foreword"></a>Foreword</h2><ul>
<li>（reference <a href="https://arxiv.org/pdf/1901.07474.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1901.07474.pdf</a> <a href="https://github.com/wangxiao5791509/Pedestrian-Attribute-Recognition-Paper-List）" target="_blank" rel="noopener">https://github.com/wangxiao5791509/Pedestrian-Attribute-Recognition-Paper-List）</a></li>
<li><img src="/images/PAR/foreword.png" alt="foreword.png"></li>
</ul>
<h2 id="What-is-Pedestrian-Attribute-Recognition"><a href="#What-is-Pedestrian-Attribute-Recognition" class="headerlink" title="What is Pedestrian Attribute Recognition"></a>What is Pedestrian Attribute Recognition</h2><ul>
<li>Pedestrian attributes, are humanly searchable semantic descriptions and can be used as soft-biometrics in visual surveillance, with applications in person re-identification, face verification, and human identification. Pedestrian attributes recognition (PAR) aims at mining the attributes of target people when given person image, as shown in follow.</li>
<li>行人属性是人为的可搜索的拥有语义信息的描述，可以用来作为一种软生物识别技术在视频监控领域，在re-id，人脸识别，人体识别上都有应用。下面是行人属性识别的展示</li>
<li><img src="/images/PAR/Intro2.png" alt="Intro2.png"></li>
</ul>
<h2 id="Traditional-practice-and-more"><a href="#Traditional-practice-and-more" class="headerlink" title="Traditional practice and more"></a>Traditional practice and more</h2><ul>
<li>Traditional pedestrian attributes recognition methods usually focus on developing robust feature representation from the perspectives of hand-crafted features, powerful classifiers or attributes relations. Some milestones including HOG [1], SIFT [2], SVM [3] or CRF model [4]. However, the reports on large-scale benchmark evaluations suggest that the performance of these traditional algorithms is far from the requirement of realistic applications. Over the past several years, deep learning have achieved an impressive performance due to their success on automatic feature extraction using multi-layer nonlinear transformation, especially in computer vision, speech recognition and natural language processing. Several deep learning based attribute recognition algorithms has been proposed based on these breakthroughs.</li>
<li>传统的做法基本就是 手工设计的特征 + 强力的分类器/属性关联 ，如 HOG SIFT SVM CRF，但是这些在大规模的benchmark上就没那么好使了。在过去的这些年里，深度学习方法因其自动强大的特征提取器带来了令人印象深刻的表现，在行人属性识别上也是如此。</li>
</ul>
<h2 id="PROBLEM-FORMULATION-AND-CHALLENGES"><a href="#PROBLEM-FORMULATION-AND-CHALLENGES" class="headerlink" title="PROBLEM FORMULATION AND CHALLENGES"></a>PROBLEM FORMULATION AND CHALLENGES</h2><ul>
<li>Multi-views 对同一对象，不同视角观察带来的差异</li>
<li>Occlusion 遮挡</li>
<li>Unbalanced Data Distribution 不平衡的 数据|lable 分布</li>
<li>Low Resolution 低分辨率</li>
<li>Illumination 亮暗在一日内变化大</li>
<li>Blur 模糊</li>
</ul>
<h2 id="Evaluation-Criteria"><a href="#Evaluation-Criteria" class="headerlink" title="Evaluation Criteria"></a>Evaluation Criteria</h2><ul>
<li><img src="/images/PAR/Intro3.png" alt="Intro3.png"></li>
</ul>
<h2 id="REGULAR-PIPELINE-FOR-PAR"><a href="#REGULAR-PIPELINE-FOR-PAR" class="headerlink" title="REGULAR PIPELINE FOR PAR"></a>REGULAR PIPELINE FOR PAR</h2><ol>
<li>Multi-task Learning<ul>
<li><img src="/images/PAR/Intro4.png" alt="Intro4.png"></li>
<li>deep learning based multi-task learning, i.e. the hard and soft parameter sharing. The hard parameter sharing usually take the shallow layers as shared layers to learn the common feature representations of multiple tasks, and treat the high-level layers as taskspecific layers to learn more discriminative patterns. This mode is the most popular framework in the deep learning community. The illustration of hard parameter sharing can be found in Figure 4 (left sub-figure). For the soft parameter sharing multi-task learning (as shown in Figure 4 (right sub-figure)), they train each task independently, but make the parameters between different tasks similar via the introduced regularization constrains, such as L2 distance [53] and trace norm [54].</li>
<li>深度学习里多任务学习往往分 硬|软 参数共享<ul>
<li>硬参数共享共享一个backbone，后面接多任务</li>
<li>软参数共享几乎完全独立，通过使不同任务中的参数相似（例如 L2-Distance, trace norm）来建立任务间的联系</li>
</ul>
</li>
</ul>
</li>
<li>Multi-label Learning<ul>
<li><img src="/images/PAR/Intro5.png" alt="Intro5.png"></li>
<li>problem transformation<ul>
<li>binary relevance algorithm<ul>
<li>将问题转化为 后接多个二分类 的分类问题</li>
<li>简单，符合直觉</li>
<li>忽略了标签间的关联关系</li>
</ul>
</li>
<li>classifier chain algorithm<ul>
<li>将问题转化为 二分类链 问题，每一个二分类结果以来之前的结果</li>
</ul>
</li>
<li>calibrated label ranking algorithm.<ul>
<li>将问题转化为 标签排序 问题</li>
</ul>
</li>
<li>random k-Labelsets algorithm<ul>
<li>将问题转化为 多组标签 分类问题 </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>algorithm adaptation <ol>
<li>multi-label k-nearest neighbour</li>
<li>multi-label decision tree</li>
<li>ranking support vector machine  Rank-SVM</li>
<li>collective multilabel classifier</li>
</ol>
</li>
</ol>
<h2 id="APPLICATIONS"><a href="#APPLICATIONS" class="headerlink" title="APPLICATIONS"></a>APPLICATIONS</h2><ul>
<li>Visual attributes can be seen as a kind of mid-level feature representation which may provide important information for high-level human related tasks, such as person re-identification [128], [129], [130], [131], [132], pedestrian detection [133], person tracking [134], person retrieval [135], [136], human action recognition[137], scene understanding [138]. Due to the limited space of this paper, we only review some works in the rest of this subsections.</li>
<li>视觉属性可以视作一种中间等级的特征，可以用来协助高级的与人相关的任务，例如 re-id 行人检测 行人跟踪 行人回复 行人动作识别 场景理解等。此外，行人属性也作为一种标签用于统计单体和群体的画像，在商业场景下用途广泛</li>
</ul>
<hr>
<h1 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h1><hr>
<h2 id="Global-Image-based-Method"><a href="#Global-Image-based-Method" class="headerlink" title="Global Image-based Method"></a>Global Image-based Method</h2><h3 id="ACN"><a href="#ACN" class="headerlink" title="ACN"></a>ACN</h3><ul>
<li>paper <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015_workshops/w11/papers/Sudowe_Person_Attribute_Recognition_ICCV_2015_paper.pdf" target="_blank" rel="noopener">Person Attribute Recognition with a Jointly-trained Holistic CNN Model</a></li>
<li><img src="/images/PAR/ACN1.png" alt="ACN1.png"></li>
<li>they adopt a pre-trained AlexNet as basic feature extraction sub-network, and replace the last fully connected layer with one loss per attribute using the KL-loss. </li>
<li>In addition, they also propose a new dataset named as PARSE-27k to support their evaluation. This dataset contains 27000 pedestrians and annotated with 10 attributes. Different from regular person attribute dataset, they propose a new category annotation, i.e., not decidable (N/A). Because for most input images, some attributes are not decidable due to occlusion, image boundaries, or any other reason.</li>
<li>最早期的文章之一，想法简单直接，推出了新的数据集PARSE-27k用做评估</li>
</ul>
<h3 id="DeepSAR-DeepMAR"><a href="#DeepSAR-DeepMAR" class="headerlink" title="DeepSAR/DeepMAR"></a>DeepSAR/DeepMAR</h3><ul>
<li>paper <a href="http://doc.startdt.net/download/attachments/37640056/Multi-attributeLearningforPedestrianAttributeRecognitioninSurveillanceScenarios.pdf?version=1&modificationDate=1571305310000&api=v2" target="_blank" rel="noopener">Multi-attribute Learning for Pedestrian Attribute Recognition in Surveillance Scenarios</a></li>
<li>git <a href="https://github.com/dangweili/pedestrian-attribute-recognition-pytorch" target="_blank" rel="noopener">https://github.com/dangweili/pedestrian-attribute-recognition-pytorch</a></li>
<li><img src="/images/PAR/DEEPSAR1.png" alt="DEEPSAR1.png"></li>
<li>Single Attribute Recognition (SAR)<ul>
<li>DeepSAR model is proposed to recognize each attribute one by one.Treating each attribute as an independent component, the DeepSAR method is proposed to predict each attribute.</li>
<li><img src="/images/PAR/DEEPSAR2.png" alt="DEEPSAR2.png"></li>
</ul>
</li>
<li>Multi-attribute Recognition (MAR)<ul>
<li>To better utilize the relationship among attributes, the unified multi-attribute jointly learning model (DeepMAR) is proposed to learn all the attributes at the same time.</li>
<li>Different from DeepSAR, the input of the DeepMAR is an image with its attribute label vector and the loss function considers all the attributes jointly.</li>
<li><img src="/images/PAR/DEEPSAR3.png" alt="DEEPSAR3.png"></li>
</ul>
</li>
<li>Experiment<ul>
<li><img src="/images/PAR/DEEPSAR4.png" alt="DEEPSAR4.png"></li>
<li>属性间的关联关系有助于提升平均准确率，但在具体各项上，有提升的有下降的</li>
</ul>
</li>
</ul>
<h3 id="MTCNN"><a href="#MTCNN" class="headerlink" title="MTCNN"></a>MTCNN</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1601.00400.pdf" target="_blank" rel="noopener"> Multi-task CNN Model for Attribute Prediction</a></li>
<li><img src="/images/PAR/MTCNN1.png" alt="MTCNN1.png"></li>
<li>Because our model requires more than one CNN model, we remove the last fully connected layers, as we substitute these layers with our own joint MTL objective loss, depending on the weight parameter matrix learned within.<ul>
<li>移除原始网络最后一层的fc，使用了都有独特的 joint MTL objective loss 来联合训练</li>
<li>Feature Sharing and Competition in MTL<ul>
<li><img src="/images/PAR/MTCNN2.png" alt="MTCNN2.png"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Attention-based-Method"><a href="#Attention-based-Method" class="headerlink" title="Attention-based Method"></a>Attention-based Method</h2><h3 id="HydraPlus-Net"><a href="#HydraPlus-Net" class="headerlink" title="HydraPlus-Net"></a>HydraPlus-Net</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1709.09930.pdf" target="_blank" rel="noopener">HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis</a></li>
<li>git <a href="https://github.com/xh-liu/HydraPlus-Net" target="_blank" rel="noopener">https://github.com/xh-liu/HydraPlus-Net</a></li>
<li>提出了PA-100K</li>
<li><img src="/images/PAR/HydraPlus-Net1.png" alt="HydraPlus-Net1.png"></li>
<li>在行人分析里，不同的分析对象所需要的特征级别不同，尺度不同<ul>
<li>语义级 不同人之间有所区分，但是咋看上去还挺相似例如 长头发vs短头发 长袖vs短袖</li>
<li>低层级 例如 clothing stride 就可以很好地在低层特征算出，比高层算出的结果要好</li>
<li>尺度差异 有些任务的关注点在手部而有些是全身，尺度变化大</li>
</ul>
</li>
<li><img src="/images/PAR/HydraPlus-Net2.png" alt="HydraPlus-Net2.png"></li>
<li>由 MainNet AttentiveFeatureNet 构成，由MNet生成特征由AFNet生成attention mask，其中F函数对应的incept模块是MNet中三个incept模块的复制（MNet会先被预训练，主体是当年流行的inception-v2）</li>
<li><img src="/images/PAR/HydraPlus-Net3.png" alt="HydraPlus-Net3.png"></li>
<li>一个MDA的例子，对于每一个在MNet中的incept模块，通过和三层incept的相互关联得到新的结果</li>
<li><img src="/images/PAR/HydraPlus-Net4.png" alt="HydraPlus-Net4.png"></li>
<li>举个例子如上图所示，原始输出结果结合毗邻特征，结合低层特征就变得更加惊喜噪音也更多，结合高层特征整体更加突出完整信息也变少更集中，对于不同的任务可以各取所需</li>
<li><img src="/images/PAR/HydraPlus-Net5.png" alt="HydraPlus-Net5.png"></li>
</ul>
<h3 id="VeSPA"><a href="#VeSPA" class="headerlink" title="VeSPA"></a>VeSPA</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1707.06089.pdf" target="_blank" rel="noopener">Deep viewsensitive pedestrian attribute inference in an end-to-end model</a></li>
<li><img src="/images/PAR/VeSPA1.png" alt="VeSPA1.png"></li>
<li>We adapt a deep neural network for joint pose and multi-label attribute classification. The overall design of our approach is shown in Figure 1. The main network is based on the GoogleNet inception architecture [20]. As shown, the network contains a view classification branch and three view-specific attribute predictor units. The view classifier and attribute predictors are both trained with separate loss functions. Prediction scores from weighted view-specific predictors are aggregated to generate the final multi-class attribute predictions. The whole network is a unified framework and is trained in an end-to-end manner.<ul>
<li>只要使用了GoogleNet inception architecture</li>
<li>分成了正面 背面 侧面三个分支预测属性</li>
<li>引入额外分支用于分类是正面背面侧面，并将其分值分别乘在三个分支上，最终综合得到最后结果</li>
</ul>
</li>
<li>这里引入了front back side，下面引用一张图说明</li>
<li><img src="/images/PAR/VeSPA2.png" alt="VeSPA2.png"></li>
<li><img src="/images/PAR/VeSPA3.png" alt="VeSPA3.png"></li>
<li>从结果上看效果很理想，甚至超过了花里胡哨的HPNET</li>
</ul>
<h3 id="DIAA"><a href="#DIAA" class="headerlink" title="DIAA"></a>DIAA</h3><ul>
<li>paper <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Nikolaos_Sarafianos_Deep_Imbalanced_Attribute_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Deep Imbalanced Attribute Classification using Visual Attention Aggregation</a></li>
<li><img src="/images/PAR/DIAA1.png" alt="DIAA1.png"></li>
<li>简单直接，且看看具体有什么不同</li>
<li><img src="/images/PAR/DIAA2.png" alt="DIAA2.png"></li>
<li>特殊的attention机制，一边是sigmoid的权重层，另一边是常规卷积层加上空间的正则化</li>
<li><img src="/images/PAR/DIAA3.png" alt="DIAA3.png"></li>
<li>通过spatial softmax达到单层的normalization，结果在iv中展示，确实能使attention更集中了</li>
<li><img src="/images/PAR/DIAA4.png" alt="DIAA4.png"></li>
<li>和focal loss的变种，其中Wc被设定为与类别属性分布相关的权重值，是每一类不同的，作者称其为 Deep Imbalanced Classification</li>
<li><img src="/images/PAR/DIAA5.png" alt="DIAA5.png"></li>
<li>对于非主分支的两个分支使用了常规的分类loss，并记录前值计算标准差作为系数加大loss</li>
<li><img src="/images/PAR/DIAA6.png" alt="DIAA6.png"></li>
<li>PETA上实验结果比较理想</li>
</ul>
<h3 id="CAM"><a href="#CAM" class="headerlink" title="CAM"></a>CAM</h3><ul>
<li>paper <a href="https://cse.sc.edu/~songwang/document/prl17.pdf" target="_blank" rel="noopener">Human attribute recognition by refining attention heat map</a></li>
<li>git <a href="https://github.com/hguosc/Human-attribute-recognition-by-refining-attention-heat-map" target="_blank" rel="noopener">https://github.com/hguosc/Human-attribute-recognition-by-refining-attention-heat-map</a></li>
<li><img src="/images/PAR/CAM1.png" alt="CAM1.png"></li>
<li>展示了效果</li>
<li><img src="/images/PAR/CAM2.png" alt="CAM2.png"></li>
<li><img src="/images/PAR/CAM3.png" alt="CAM3.png"></li>
<li>在backbone各attribute共享，在FC开始分离，通过weight average layer（The customization of the weighted average layer is used to embed the attention heat map into the network training and the modification of the fully-connected layer is used to output both features and weights.）处理得到attention map，继续常规操作得到结果</li>
<li><img src="/images/PAR/CAM4.png" alt="CAM4.png"></li>
<li><img src="/images/PAR/CAM5.png" alt="CAM5.png"></li>
<li>可见这个exponential loss主要是为了使最大值更大，使关注点更集中</li>
<li><img src="/images/PAR/CAM6.png" alt="CAM6.png"></li>
<li>只有 wider 的结果，较baseline有所提升</li>
</ul>
<h2 id="Curriculum-Learning-Method"><a href="#Curriculum-Learning-Method" class="headerlink" title="Curriculum Learning Method"></a>Curriculum Learning Method</h2><h3 id="MTCT"><a href="#MTCT" class="headerlink" title="MTCT"></a>MTCT</h3><ul>
<li>paper <a href="http://www.eecs.qmul.ac.uk/~xiatian/papers/WACV17/DongEtAl_WACV2017.pdf" target="_blank" rel="noopener">Multi-Task Curriculum Transfer Deep Learning of Clothing Attributes</a></li>
<li><img src="/images/PAR/MTCT1.png" alt="MTCT1.png"></li>
<li>训练分两个阶段，一阶段是常规的属性学习MTN，使用常规的分类loss；第二阶段将一阶段训练完成的MTN重新构建成为三元组，使用e t-distribution Stochastic Triplet Embedding (t-STE) loss进行训练</li>
<li>文章不是针对行人属性的，不做拓展</li>
</ul>
<h3 id="CILICIA"><a href="#CILICIA" class="headerlink" title="CILICIA"></a>CILICIA</h3><ul>
<li>paper <a href="https://nsarafianos.github.io/assets/Curriculum_Learning_Clusters.pdf" target="_blank" rel="noopener">Curriculum learning for multi-task classification of visual attributes</a></li>
<li><img src="/images/PAR/CILICIA1.png" alt="CILICIA1.png"></li>
<li>这篇文章的主要思想就是先学习得到task间的相关性，然后定义下个阶段的task组，循环学习到只有2个任务</li>
<li><img src="/images/PAR/CILICIA2.png" alt="CILICIA2.png"></li>
<li>类似这样，是多stage的做法，不是现在的主流</li>
</ul>
<h2 id="Graph-based-Method"><a href="#Graph-based-Method" class="headerlink" title="Graph based Method"></a>Graph based Method</h2><h3 id="DCSA"><a href="#DCSA" class="headerlink" title="DCSA"></a>DCSA</h3><ul>
<li>paper <a href="http://chenlab.ece.cornell.edu/people/Andy/publications/ECCV2012_ClothingAttributes.pdf" target="_blank" rel="noopener">Describing clothing by semantic attributes</a></li>
<li><img src="/images/PAR/DCSA1.png" alt="DCSA1.png"></li>
<li>整体的思路即使放在现在也不过时，12年的文章。先通过pose estimation得到大致的区域，然后划分区域特征抽取，得到各属性分类结果，然后构建多属性图使用CRF进行推论得到结果。具体方法如SIFT SVM等已不适用现在，不展开</li>
</ul>
<h3 id="A-AOG"><a href="#A-AOG" class="headerlink" title="A-AOG"></a>A-AOG</h3><ul>
<li>paper <a href="http://www.stat.ucla.edu/~sczhu/papers/PAMI_Attribute_Grammar.pdf" target="_blank" rel="noopener"> Attribute and-or grammar for joint parsing of human pose, parts and attributes</a></li>
<li><img src="/images/PAR/A-AOG1.png" alt="A-AOG1.png"></li>
<li>人体分割，按约定的树构建层级关系，各部位分别出各自的属性，效果很炫酷</li>
<li><img src="/images/PAR/A-AOG2.png" alt="A-AOG2.png"></li>
<li>低层组件定义了14个，中间件2个，root件一个。每个部件拥有9个相关属性，通过属性关联链连接各部分属性。</li>
<li><img src="/images/PAR/A-AOG3.png" alt="A-AOG3.png"></li>
<li>如果结果理想的话，单个属性会被某几个部分突出地代表</li>
<li>本文对于loss只是一句 CE 带过，整体结果也没有在常见的数据集上，只有思路供参考</li>
</ul>
<h3 id="PAR-w-GCN"><a href="#PAR-w-GCN" class="headerlink" title="PAR w/ GCN"></a>PAR w/ GCN</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1904.03582.pdf" target="_blank" rel="noopener">Multi-Label Image Recognition with Graph Convolutional Networks</a></li>
<li>git <a href="https://github.com/2014gaokao/pedestrian-attribute-recognition-with-GCN" target="_blank" rel="noopener">https://github.com/2014gaokao/pedestrian-attribute-recognition-with-GCN</a></li>
<li><img src="/images/PAR/PARGCN1.png" alt="PARGCN1.png"></li>
<li><img src="/images/PAR/PARGCN2.jpg" alt="PARGCN2.png"></li>
<li>b图是原始论文网络框架图，c图是git项目的图，原理是相同的。上面分支是backbone出特征，下面是GCN分支出关联关系，点乘得到结果。那么这个GCN的输入输出是什么呢？究竟完成了一个什么事情？</li>
<li>理解GCN 推荐 <a href="https://www.zhihu.com/question/54504471/answer/332657604" target="_blank" rel="noopener">https://www.zhihu.com/question/54504471/answer/332657604</a></li>
<li>介绍下GCN代码和glove<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GCN方法，通过矩阵乘改变输出维度</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GraphConvolution</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, out_features, bias=False)</span>:</span></span><br><span class="line">        super(GraphConvolution, self).__init__()</span><br><span class="line">        self.in_features = in_features</span><br><span class="line">        self.out_features = out_features</span><br><span class="line">        self.weight = Parameter(torch.Tensor(in_features, out_features))</span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.bias = Parameter(torch.Tensor(<span class="number">1</span>, <span class="number">1</span>, out_features))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_parameter(<span class="string">'bias'</span>, <span class="literal">None</span>)</span><br><span class="line">        self.reset_parameters()</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_parameters</span><span class="params">(self)</span>:</span></span><br><span class="line">        stdv = <span class="number">1.</span> / math.sqrt(self.weight.size(<span class="number">1</span>))</span><br><span class="line">        self.weight.data.uniform_(-stdv, stdv)</span><br><span class="line">        <span class="keyword">if</span> self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.bias.data.uniform_(-stdv, stdv)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, adj)</span>:</span></span><br><span class="line">        support = torch.matmul(input, self.weight)</span><br><span class="line">        output = torch.matmul(adj, support)</span><br><span class="line">        <span class="keyword">if</span> self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> output + self.bias</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> output</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__class__.__name__ + <span class="string">' ('</span> \</span><br><span class="line">               + str(self.in_features) + <span class="string">' -&gt; '</span> \</span><br><span class="line">               + str(self.out_features) + <span class="string">')'</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># GLOVE其实指的是 原始label word2vec 后得到的库，文章里称glove</span></span><br><span class="line">word_to_ix = &#123;j: i <span class="keyword">for</span> i, j <span class="keyword">in</span> enumerate(select_name)&#125;</span><br><span class="line">embeds=nn.Embedding(<span class="number">60</span>,<span class="number">300</span>)</span><br><span class="line">word2vec=torch.tensor([])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(select)):</span><br><span class="line">    lookup_tensor=torch.tensor([word_to_ix[select_name[i]]],dtype=torch.long)</span><br><span class="line">    embed=embeds(lookup_tensor)</span><br><span class="line">    word2vec=torch.cat((word2vec,embed),<span class="number">0</span>)</span><br></pre></td></tr></table></figure></li>
<li><img src="/images/PAR/PARGCN3.png" alt="PARGCN3.png"></li>
<li>这张图说明了GCN想要的结果，主要是想要这个条件概率。以下是adj的初始化方式<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化</span></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> partition[<span class="string">'train'</span>][<span class="number">0</span>]:</span><br><span class="line">    t=np.array(dataset[<span class="string">'att'</span>][idx])[dataset[<span class="string">'selected_attribute'</span>]]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(np.array(dataset[<span class="string">'att'</span>][idx])[dataset[<span class="string">'selected_attribute'</span>]])):</span><br><span class="line">        <span class="keyword">if</span> t[i]==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(len(np.array(dataset[<span class="string">'att'</span>][idx])[dataset[<span class="string">'selected_attribute'</span>]])):</span><br><span class="line">                <span class="keyword">if</span> t[j]==<span class="number">1</span> <span class="keyword">and</span> j!=i:</span><br><span class="line">                    concur[i][j]+=<span class="number">1</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 初始化A norm化卡阈值 + 1 得到初始值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_A</span><span class="params">(num_classes, t, adj_file)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    result = pickle.load(open(adj_file, <span class="string">'rb'</span>))</span><br><span class="line">    _adj = result[<span class="string">'adj'</span>]</span><br><span class="line">    _nums = result[<span class="string">'nums'</span>]</span><br><span class="line">    _nums = _nums[:, np.newaxis]</span><br><span class="line">    _adj = _adj / _nums</span><br><span class="line">    _adj[_adj &lt; t] = <span class="number">0</span></span><br><span class="line">    _adj[_adj &gt;= t] = <span class="number">1</span></span><br><span class="line">    <span class="comment">#_adj = _adj * 0.9 / (_adj.sum(0) + 1e-6)</span></span><br><span class="line">    _adj = _adj + np.identity(num_classes, np.int)</span><br><span class="line">    <span class="keyword">return</span> _adj</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 真实使用时，对应矩阵运算就是 D^(-1)AD</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_adj</span><span class="params">(A)</span>:</span></span><br><span class="line">    D = torch.pow(A.sum(<span class="number">1</span>).float(), <span class="number">-0.5</span>)</span><br><span class="line">    D = torch.diag(D)</span><br><span class="line">    adj = torch.matmul(torch.matmul(A, D).t(), D)</span><br><span class="line">    <span class="keyword">return</span> adj</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># adj其实指的是 adjacency matrix, 用于描述图节点的矩阵。预处理后使用</span></span><br></pre></td></tr></table></figure></li>
<li><img src="/images/PAR/PARGCN4.png" alt="PARGCN4.png"></li>
<li>rap的结果，原始论文不是拿来做这个用途没有相关的结果。从结果上看还是很顶的，基本达到了sota水平</li>
</ul>
<h3 id="VSGR"><a href="#VSGR" class="headerlink" title="VSGR"></a>VSGR</h3><ul>
<li>paper <a href="http://doc.startdt.net/download/attachments/37640056/4884-Article%20Text-7950-1-10-20190709.pdf?version=1&modificationDate=1571743975000&api=v2" target="_blank" rel="noopener">Visua-semantic graph reasoning for pedestrian attribute recognition</a></li>
<li><img src="/images/PAR/VSGR1.png" alt="VSGR1.png"></li>
<li>Visual-to-semantic Sub-network + Semantic-to-visual Sub-network fuse得到结果<ul>
<li>目前无开源代码，原文里操作多且复杂，难以非常清晰地理解</li>
</ul>
</li>
<li>Experiment<ul>
<li><img src="/images/PAR/VSGR2.png" alt="VSGR2.png"></li>
</ul>
</li>
<li>有待开源后进一步研究</li>
</ul>
<h2 id="Loss-Function-based-Method"><a href="#Loss-Function-based-Method" class="headerlink" title="Loss Function based Method"></a>Loss Function based Method</h2><h3 id="WPAL"><a href="#WPAL" class="headerlink" title="WPAL"></a>WPAL</h3><ul>
<li>paper <a href="http://www.bmva.org/bmvc/2017/papers/paper069/paper069.pdf" target="_blank" rel="noopener">Weakly-supervised Learning of Mid-level Features for Pedestrian Attribute Recognition and Localization</a></li>
<li><img src="/images/PAR/WPAL1.png" alt="WPAL1.png"></li>
<li>从结构上看主要的贡献是 multi-level feature + FSPP + weighted-CE（Flexible Spatial Pyramid Pooling ）</li>
<li><img src="/images/PAR/WPAL2.png" alt="WPAL2.png"></li>
<li>level 1是全图的conv，level 2是用了预设的9个bins去框特征得到的结果</li>
<li><img src="/images/PAR/WPAL3.png" alt="WPAL3.png"></li>
<li>Wi是label在数据集中相对数量的百分比，平衡数据</li>
<li><img src="/images/PAR/WPAL4.png" alt="WPAL4.png"></li>
<li>提出了新的衡量指标 IoP，然而并没有多少人用</li>
<li><img src="/images/PAR/WPAL5.png" alt="WPAL5.png"></li>
<li>在RAP上表现不错，但是这个方法对头发极为不鲁邦</li>
</ul>
<h3 id="AWMT"><a href="#AWMT" class="headerlink" title="AWMT"></a>AWMT</h3><ul>
<li>paper <a href="https://craigie1996.github.io/2018/05/11/Pedestrian-Attribute-Recognition-%E8%B0%83%E7%A0%94%E7%AC%94%E8%AE%B0/papers/Adaptively%20Weighted.pdf" target="_blank" rel="noopener">Adaptively weighted multi-task deep network for person attribute classification</a></li>
<li>git <a href="https://github.com/qiexing/adaptive_weighted_attribute" target="_blank" rel="noopener">https://github.com/qiexing/adaptive_weighted_attribute</a></li>
<li><img src="/images/PAR/AWMT1.png" alt="AWMT1.png"></li>
<li>结构上只是常规的res50，输入同时有train和val，在loss上有所技巧</li>
<li><img src="/images/PAR/AWMT2.png" alt="AWMT2.png"></li>
<li>任务目标是使 Θ 最小，其中 λ 为可更新超参（权重），Lkj是阈值</li>
<li><img src="/images/PAR/AWMT3.png" alt="AWMT3.png"></li>
<li>主题算法一共有两个部分<ul>
<li>输入 train val 进行forward，train_loss * λ 作为backward的loss; 权重λ在到达更新period时更新</li>
<li>λ 的 更新方法</li>
</ul>
</li>
<li>其实还是用了 val的数据，还间接参与了backward</li>
</ul>
<h2 id="Part-based-Method"><a href="#Part-based-Method" class="headerlink" title="Part-based Method"></a>Part-based Method</h2><h3 id="Poselets"><a href="#Poselets" class="headerlink" title="Poselets"></a>Poselets</h3><ul>
<li>paper <a href="https://people.cs.umass.edu/~smaji/papers/attributes-iccv11.pdf" target="_blank" rel="noopener">Describing People: A Poselet-Based Approach to Attribute Classification</a></li>
<li><img src="/images/PAR/Poselets1.png" alt="Poselets1.png"></li>
<li>先使用poselets讲身体分为几张图<ul>
<li>poselet paper <a href="http://www.cs.utexas.edu/~cv-fall2012/slides/dinesh-paper.pdf" target="_blank" rel="noopener">http://www.cs.utexas.edu/~cv-fall2012/slides/dinesh-paper.pdf</a></li>
<li>We use the method of Bourdev et al. [3] to train 1200 poselets using images from the training and validation sets. Instead of all poselets having the same aspect ratios, we used four aspect ratios: 96x64, 64x64, 64x96 and 64x128 and trained 300 poselets of each. For each poselet, during training, we build a soft mask for the probability of each body component (such as hair, face, upper clothes, lower clothes, etc) at each location within the normalized poselet patch (Figure 5) using body component annotations on the H3D dataset [4].</li>
<li><img src="/images/PAR/Poselets2.png" alt="Poselets2.png"></li>
<li><img src="/images/PAR/Poselets3.png" alt="Poselets3.png"></li>
<li>后接3个SVM用于分类</li>
</ul>
</li>
</ul>
<h3 id="RAD"><a href="#RAD" class="headerlink" title="RAD"></a>RAD</h3><ul>
<li>paper <a href="https://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Joo_Human_Attribute_Recognition_2013_ICCV_paper.pdf" target="_blank" rel="noopener">Human Attribute Recognition by Rich Appearance Dictionary</a></li>
<li><img src="/images/PAR/RAD1.png" alt="RAD1.png"></li>
<li><img src="/images/PAR/RAD2.png" alt="RAD2.png"></li>
</ul>
<h3 id="PANDA"><a href="#PANDA" class="headerlink" title="PANDA"></a>PANDA</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1311.5591.pdf" target="_blank" rel="noopener">PANDA: Pose Aligned Networks for Deep Attribute Modeling</a></li>
<li><img src="/images/PAR/PANDA1.png" alt="PANDA1.png"></li>
<li>与Poselets很相似，但是有了独立的CNN支持</li>
<li><img src="/images/PAR/PANDA2.png" alt="PANDA2.png"></li>
<li>在有独立CNN支持后性能改善较多</li>
<li><img src="/images/PAR/PANDA3.png" alt="PANDA3.png"></li>
</ul>
<h3 id="MLCNN"><a href="#MLCNN" class="headerlink" title="MLCNN"></a>MLCNN</h3><ul>
<li>paper <a href="http://www.cbsr.ia.ac.cn/users/zlei/papers/ICB2015/Zhu-ICB-15.pdf" target="_blank" rel="noopener">Multi-label CNN Based Pedestrian Attribute Learning for Soft Biometrics</a></li>
<li><img src="/images/PAR/MLCNN1.png" alt="MLCNN1.png"></li>
<li>直接把一张图划分为有overlap的15个块，对不同的块进行卷积操作，使用特定块结果得到特定标签结果，如mid-hair标签使用了1,2,3块。</li>
</ul>
<h3 id="AAWP"><a href="#AAWP" class="headerlink" title="AAWP"></a>AAWP</h3><ul>
<li>paper <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Gkioxari_Actions_and_Attributes_ICCV_2015_paper.pdf" target="_blank" rel="noopener">Actions and Attributes from Wholes and Parts</a></li>
<li><img src="/images/PAR/AAWP1.png" alt="AAWP1.png"></li>
<li>这篇文章开始使用 R-CNN 检测，从图像中得到 全身，头部，上身，下身 的具体位置，切割出来进行属性识别</li>
<li><img src="/images/PAR/AAWP2.png" alt="AAWP2.png"></li>
<li>对检测部分，使用高斯金字塔处理过的图像作为输入，通过多尺度提高结果，已经非常类似与FPN</li>
<li><img src="/images/PAR/AAWP3.png" alt="AAWP3.png"></li>
<li>分类器还是很普通的 CNN + SVM 的做派</li>
</ul>
<h3 id="ARAP"><a href="#ARAP" class="headerlink" title="ARAP"></a>ARAP</h3><ul>
<li>paper <a href="http://www.bmva.org/bmvc/2016/papers/paper081/paper081.pdf" target="_blank" rel="noopener">Attribute Recognition from Adaptive Parts</a></li>
<li><img src="/images/PAR/ARAP1.png" alt="ARAP1.png"></li>
<li>通过关键点得到各部件，然后得到各部分属性</li>
<li><img src="/images/PAR/ARAP2.png" alt="ARAP2.png"></li>
<li>关键点检测和属性识别共享基础backbone，先进行关键点检测，得到感兴趣的部件区域，将特征层上采样与部件区域对其，最后fc分类得到结果</li>
<li><img src="/images/PAR/ARAP3.png" alt="ARAP3.png"></li>
<li>值得一提的是，作者不仅对关键点设置了回归loss，也对bbox的结果进行了 loss 约束，虽然形式有些怪异，不过确实改进了性能</li>
<li><img src="/images/PAR/ARAP4.png" alt="ARAP4.png"></li>
<li>毫无疑问的是，精准定位会带来的属性识别性能提升，尤其是ratio loss带来的对bbox的修正使整体方法更完整</li>
<li><img src="/images/PAR/ARAP5.png" alt="ARAP5.png"></li>
</ul>
<h3 id="DeepCAMP"><a href="#DeepCAMP" class="headerlink" title="DeepCAMP"></a>DeepCAMP</h3><ul>
<li>paper <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Diba_DeepCAMP_Deep_Convolutional_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Deepcamp: Deep convolutional action &amp; attribute mid-level patterns</a></li>
<li><img src="/images/PAR/DeepCAMP1.png" alt="DeepCAMP1.png"></li>
<li>看着这个示意图青一块紫一块的，其实是 聚类(cluster) 的结果，继续往下看</li>
<li><img src="/images/PAR/DeepCAMP2.png" alt="DeepCAMP2.png"></li>
<li>不同于后来的基于attention的无监督注意力机制，本文使用了聚类方法，对各patch特征进行无监督处理</li>
<li><img src="/images/PAR/DeepCAMP3.png" alt="DeepCAMP3.png"></li>
<li>伪代码简单明了很清晰，先抽特征，更新聚类器，得到分值，最后得到结果。值得一提的是，本文使用了LDA作为降维聚类工具</li>
<li><img src="/images/PAR/DeepCAMP4.png" alt="DeepCAMP4.png"></li>
<li>Mid-level Deep Patterns Network 具体如上图所示，图上没有标出的是作者使用了fast-RCNN，在conv4后面是roipooling层，对两个关注的patch点及整个人进行roipooling，concat后得到结果</li>
<li><img src="/images/PAR/DeepCAMP5.png" alt="DeepCAMP5.png"></li>
<li>大幅超过了PANDA</li>
</ul>
<h3 id="PGDM"><a href="#PGDM" class="headerlink" title="PGDM"></a>PGDM</h3><ul>
<li>paper <a href="http://dangweili.github.io/misc/pdfs/icme18.pdf" target="_blank" rel="noopener">POSE GUIDED DEEP MODEL FOR PEDESTRIAN ATTRIBUTE RECOGNITION IN SURVEILLANCE SCENARIOS</a></li>
<li><img src="/images/PAR/PGDM1.png" alt="PGDM1.png"></li>
<li>整体思路和16年的ARAP基本一致，在网络设计上做了改进</li>
<li><img src="/images/PAR/PGDM2.png" alt="PGDM2.png"></li>
<li>Lm:MainNet分类loss*热力weight </li>
<li>Lr:关键点回归smooth-l1loss </li>
<li>Lp:fuse后分类loss</li>
<li><img src="/images/PAR/PGDM3.png" alt="PGDM3.png"></li>
<li><img src="/images/PAR/PGDM4.png" alt="PGDM4.png"></li>
<li>展示了PETA RAP上对HP-Net的全方位压制</li>
</ul>
<h3 id="DHC"><a href="#DHC" class="headerlink" title="DHC"></a>DHC</h3><ul>
<li>paper <a href="http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2016_human.pdf" target="_blank" rel="noopener">Human Attribute Recognition by Deep Hierarchical Contexts</a></li>
<li>发布了 WIDER Attribute dataset，多人多属性数据集</li>
<li><img src="/images/PAR/DHC1.png" alt="DHC1.png"></li>
<li>对于一张input，将其经过高斯金字塔处理，一起送入网络。得到 1.目标人整体 2.目标人各部分检测 3.多尺度各部件近邻特征 4.全图</li>
<li><img src="/images/PAR/DHC2.png" alt="DHC2.png"></li>
<li>连乘概率得到最终结果</li>
</ul>
<h3 id="LGNet"><a href="#LGNet" class="headerlink" title="LGNet"></a>LGNet</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1808.09102.pdf" target="_blank" rel="noopener">Localization guided learning for pedestrian attribute recognition</a></li>
<li>git <a href="https://github.com/lpzjerry/Pedestrian-Attribute-LGNet" target="_blank" rel="noopener">https://github.com/lpzjerry/Pedestrian-Attribute-LGNet</a></li>
<li><img src="/images/PAR/LGNET1.png" alt="LGNET1.png"></li>
<li>介绍地很好，a.原图 b.按预设模板直接分块干 c.基于关键点 d.激活图方法(类似attention) e.依赖定位的方法</li>
<li><img src="/images/PAR/LGNET2.png" alt="LGNET2.png"></li>
<li>1.上面的是全局分支，得到属性  </li>
<li>2.1 特征层卷积得到激活图，联合Edge Boxes结果对结果进行多尺度多比例扩展，得到分类层特征  2.2 卷积 + Edge Boxes结果 进行roipooling得到基于边缘框的特征  2.3 联合Edge Boxes特征和分类层特征得到最终属性结果 </li>
<li>3 联合全局分支和Edge Boxes激活图分支，得到最终结果</li>
<li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2014/09/ZitnickDollarECCV14edgeBoxes.pdf" target="_blank" rel="noopener">Edge Boxes</a></li>
<li><img src="/images/PAR/LGNET3.png" alt="LGNET3.png"></li>
</ul>
<h2 id="Sequential-Prediction-based-Method"><a href="#Sequential-Prediction-based-Method" class="headerlink" title="Sequential Prediction based Method"></a>Sequential Prediction based Method</h2><h3 id="CNN-RNN"><a href="#CNN-RNN" class="headerlink" title="CNN-RNN"></a>CNN-RNN</h3><ul>
<li>paper <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Wang_CNN-RNN_A_Unified_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Cnn-rnn: A unified framework for multi-label image classification</a></li>
<li><img src="/images/PAR/CNNRNN1.png" alt="CNNRNN1.png"></li>
<li>提出了用 联合嵌入空间对标签关联关系的学习 The framework learns a joint embedding space to characterize the image-label relationship as well as label dependency</li>
<li><img src="/images/PAR/CNNRNN2.png" alt="CNNRNN2.png"></li>
<li>主要是通过标签隐向量间的关联关系加强结果，本文不是行人属性专门的文章，没有对应的结果</li>
</ul>
<h3 id="JRL"><a href="#JRL" class="headerlink" title="JRL"></a>JRL</h3><ul>
<li>paper <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Wang_Attribute_Recognition_by_ICCV_2017_paper.pdf" target="_blank" rel="noopener">Attribute recognition by joint recurrent learning of context and correlation</a></li>
<li><img src="/images/PAR/JRL1.png" alt="JRL1.png"></li>
<li>一方面是通过LSTM改善暴力切片分析中的上下文信息的关联性，另一方面使用了类似CNN-RNN类似想法的属性关联关系的LSTM，loss就是常规的CE</li>
<li><img src="/images/PAR/JRL2.png" alt="JRL2.png"></li>
<li>结果上看，相对于CNN-RNN的各种改版有较大优势</li>
</ul>
<h3 id="GRL"><a href="#GRL" class="headerlink" title="GRL"></a>GRL</h3><ul>
<li>paper <a href="https://www.ijcai.org/proceedings/2018/0441.pdf" target="_blank" rel="noopener">Grouping attribute recognition for pedestrian with joint recurrent learning</a></li>
<li>git <a href="https://github.com/slf12/GRLModel" target="_blank" rel="noopener">https://github.com/slf12/GRLModel</a></li>
<li><img src="/images/PAR/GRL1.png" alt="GRL1.png"></li>
<li>一个分支通过FCN生成骨骼点，然后通过区域生成层生成 头 上身 下身 三个区域，生成特征；另一个分支直接原图进入backbone得到全身特征，再对应FC出几个分支。通过LSTM对特征进行相互关联，最终得到结果</li>
<li><img src="/images/PAR/GRL2.png" alt="GRL2.png"></li>
<li>loss上带了正项的权重用于解决标签不平衡的问题</li>
<li><img src="/images/PAR/GRL3.png" alt="GRL3.png"></li>
<li>GRL但模型干过了JRL ensemble的结果，主要优势猜测还是骨骼点带来的更精准的语义信息比起JRL的暴力分割</li>
</ul>
<h3 id="JCM"><a href="#JCM" class="headerlink" title="JCM"></a>JCM</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1811.08115.pdf" target="_blank" rel="noopener">Sequence-based person attribute recognition with joint ctc-attention model </a></li>
<li><img src="/images/PAR/JCM1.png" alt="JCM1.png"></li>
<li><img src="/images/PAR/JCM2.png" alt="JCM2.png"></li>
<li>很简略的overview和structure，主要的contribution在于CTC loss和attention model</li>
<li><img src="/images/PAR/JCM3.png" alt="JCM3.png"></li>
<li>CTC loss是联立条件概率loss，是通过关联关系提高标签准确率的思路</li>
<li><img src="/images/PAR/JCM4.png" alt="JCM4.png"></li>
<li>没有单独画图，只有这一段话说明attention model，感觉不是很清楚</li>
<li><img src="/images/PAR/JCM5.png" alt="JCM5.png"></li>
<li><img src="/images/PAR/JCM6.png" alt="JCM6.png"></li>
<li>不仅在PETA上表现优秀，在re-id上同样有不俗的表现，同时也用实验证实了re-id任务和attention recognition任务的相辅相成</li>
<li>全文很有借鉴意义，但是attention model没有非常详细的说明，而且没有开源代码，是遗憾</li>
</ul>
<h3 id="RCRA"><a href="#RCRA" class="headerlink" title="RCRA"></a>RCRA</h3><ul>
<li>paper <a href="http://doc.startdt.net/download/attachments/37640056/AAAI2019-Recurrent%20Attention%20Model%20for%20Pedestrian%20Attribute%20Recognition.pdf?version=1&modificationDate=1571726400000&api=v2" target="_blank" rel="noopener">Recurrent attention model for pedestrian attribute recognition</a></li>
<li><img src="/images/PAR/RCRA1.png" alt="RCRA1.png"></li>
<li><img src="/images/PAR/RCRA2.png" alt="RCRA2.png"></li>
<li>上来先介绍了 ConvLSTM，和LSTM的主要区别是用 Conv 取代 Linear 以保留spatial 信息</li>
<li><img src="/images/PAR/RCRA3.png" alt="RCRA3.png"></li>
<li>这是作者提出的第一个网络结果叫做 RC ，是一个常规的recurrent 结构。作者设计这个结构的初衷是从中级别卷积特征图中去挖掘标签相关性（A Recurrent Convolutional (RC) framework is proposed to mine the attribute correlations from mid-level convolutional feature maps of attribute groups.）</li>
<li><img src="/images/PAR/RCRA4.png" alt="RCRA4.png"></li>
<li>这是作者提出的第二个网络称之为 RA，和RC非常相似。作者设计这个结构的初衷是希望同时在组间和组内的attention机制能帮助属性识别（And a Recurrent Attention (RA) framework is formulated to recognise pedestrian attributes by group step by step in order to pay attention to both the intra-group and inter-group attention relationship.）</li>
<li><img src="/images/PAR/RCRA5.png" alt="RCRA5.png"></li>
<li>效果都挺不错的，在不同的指标下RC RA各有千秋</li>
<li><img src="/images/PAR/RCRA6.png" alt="RCRA6.png"></li>
<li>这个图的意思是，预测label的顺序，实验证明，从全局到局部的预测顺序比随机预测的效果要好</li>
</ul>
<hr>
<h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><hr>
<h2 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h2><ul>
<li><img src="/images/PAR/dataset_overview.png" alt="dataset_overview.png"></li>
<li>按学术界喜爱排序<ol>
<li>PETA RAP RAP2.0(19年新出，新的paper会用)  PA-100K</li>
<li>Market-1501 DukeMTMC (主要用于联合reid使用)</li>
<li>其他</li>
</ol>
</li>
</ul>
<h2 id="PETA"><a href="#PETA" class="headerlink" title="PETA"></a>PETA</h2><ul>
<li>home <a href="http://mmlab.ie.cuhk.edu.hk/projects/PETA.html" target="_blank" rel="noopener">http://mmlab.ie.cuhk.edu.hk/projects/PETA.html</a> （港中文信息工程系14发布）</li>
<li>paper <a href="http://mmlab.ie.cuhk.edu.hk/projects/PETA_files/Pedestrian%20Attribute%20Recognition%20At%20Far%20Distance.pdf" target="_blank" rel="noopener">http://mmlab.ie.cuhk.edu.hk/projects/PETA_files/Pedestrian%20Attribute%20Recognition%20At%20Far%20Distance.pdf</a></li>
<li>download(drop box需要翻墙) <a href="https://www.dropbox.com/s/52ylx522hwbdxz6/PETA.zip?dl=0" target="_blank" rel="noopener">https://www.dropbox.com/s/52ylx522hwbdxz6/PETA.zip?dl=0</a></li>
<li>The capability of recognizing pedestrian attributes, such as gender and clothing style, at far distance, is of practical interest in far-view video surveillance scenarios where face and body close-shots are hardly available. We make two contributions in this paper. First, we release a new pedestrian attribute dataset, which is by far the largest and most diverse of its kind. We show that the large-scale dataset facilitates the learning of robust attribute detectors with good generalization performance. Second, we present the benchmark performance by SVM-based method and propose an alternative approach that exploits context of neighboring pedestrian images for improved attribute inference.</li>
<li><img src="/images/PAR/PETA1.png" alt="PETA1.png"></li>
<li><img src="/images/PAR/PETA2.png" alt="PETA2.png"></li>
</ul>
<h2 id="RAP-Richly-Annotated-Pedestrian"><a href="#RAP-Richly-Annotated-Pedestrian" class="headerlink" title="RAP Richly Annotated Pedestrian"></a>RAP Richly Annotated Pedestrian</h2><ul>
<li>home(无法正常访问) <a href="http://rap.idealtest.org/" target="_blank" rel="noopener">http://rap.idealtest.org/</a></li>
<li>git <a href="https://github.com/dangweili/RAP" target="_blank" rel="noopener">https://github.com/dangweili/RAP</a></li>
<li>1.0-paper <a href="https://arxiv.org/pdf/1603.07054.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.07054.pdf</a><ul>
<li>申请数据集的pdf RAP V1.0 Database License Agreement.pdf</li>
<li>申请数据集邮箱 <a href="mailto:jiajian2018@ia.ac.cn">jiajian2018@ia.ac.cn</a> </li>
<li>RAP has in total 41,585 pedestrian samples, each of which is annotated with 72 attributes as well as viewpoints, occlusions, body parts information. </li>
</ul>
</li>
<li>2.0-paper <a href="https://ieeexplore.ieee.org/document/8510891" target="_blank" rel="noopener">https://ieeexplore.ieee.org/document/8510891</a><ul>
<li>申请数据集的pdf RAP V2.0 Database License Agreement.pdf</li>
<li>申请数据集邮箱 <a href="mailto:jiajian2018@ia.ac.cn">jiajian2018@ia.ac.cn</a> </li>
<li>RAP is a large-scale dataset which contains 84928 images with 72 types of attributes and additional tags of viewpoint, occlusion, body parts, and 2589 person identities.</li>
</ul>
</li>
</ul>
<h2 id="PA-100K"><a href="#PA-100K" class="headerlink" title="PA-100K"></a>PA-100K</h2><ul>
<li>from sensetime</li>
<li>paper <a href="https://arxiv.org/pdf/1709.09930.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1709.09930.pdf</a></li>
<li>git <a href="https://github.com/xh-liu/HydraPlus-Net" target="_blank" rel="noopener">https://github.com/xh-liu/HydraPlus-Net</a></li>
<li>download-baiduyun <a href="https://pan.baidu.com/s/1l-5a__OTwZVkhm_A16HraQ#list/path=%2F" target="_blank" rel="noopener">https://pan.baidu.com/s/1l-5a__OTwZVkhm_A16HraQ#list/path=%2F</a></li>
<li>download-googledrive <a href="https://drive.google.com/drive/folders/0B5_Ra3JsEOyOUlhKM0VPZ1ZWR2M" target="_blank" rel="noopener">https://drive.google.com/drive/folders/0B5_Ra3JsEOyOUlhKM0VPZ1ZWR2M</a></li>
<li>we construct a new large-scale pedestrian attribute (PA) dataset named as PA-100K with 100, 000 pedestrian images from 598 scenes, and therefore offer a superiorly comprehensive dataset for pedestrian attribute recognition. To our best knowledge, it is to-date the largest dataset for pedestrian attribute recognition.</li>
</ul>
<h2 id="Market-1501-Attribute-amp-DukeMTMC-attribute"><a href="#Market-1501-Attribute-amp-DukeMTMC-attribute" class="headerlink" title="Market-1501 Attribute &amp; DukeMTMC-attribute"></a>Market-1501 Attribute &amp; DukeMTMC-attribute</h2><ul>
<li>paper <a href="https://arxiv.org/pdf/1703.07220.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1703.07220.pdf</a></li>
<li>We have manually labeled a set of pedestrian attributes for the Market-1501 dataset and the DukeMTMC-reID dataset.</li>
<li>download Market-1501 <a href="https://drive.google.com/file/d/1kbDAPetylhb350LX3EINoEtFsXeXB0uW/view" target="_blank" rel="noopener">https://drive.google.com/file/d/1kbDAPetylhb350LX3EINoEtFsXeXB0uW/view</a></li>
<li>download-annotation-git <a href="https://github.com/vana77/Market-1501_Attribute" target="_blank" rel="noopener">https://github.com/vana77/Market-1501_Attribute</a> (The annotations are contained in the file market_attribute.mat. “gallery_market.mat” is one prediction example. Then download the code “evaluate_market_attribute.m” in this repository, change the image path and run it to evaluate.)</li>
</ul>
<table>
<thead>
<tr>
<th align="center">attribute</th>
<th align="center">representation in file</th>
<th align="center">label</th>
</tr>
</thead>
<tbody><tr>
<td align="center">gender</td>
<td align="center">gender</td>
<td align="center">male(1), female(2)</td>
</tr>
<tr>
<td align="center">hair length</td>
<td align="center">hair</td>
<td align="center">short hair(1), long hair(2)</td>
</tr>
<tr>
<td align="center">sleeve length</td>
<td align="center">up</td>
<td align="center">long sleeve(1), short sleeve(2)</td>
</tr>
<tr>
<td align="center">length of lower-body clothing</td>
<td align="center">down</td>
<td align="center">long lower body clothing(1), short(2)</td>
</tr>
<tr>
<td align="center">type of lower-body clothing</td>
<td align="center">clothes</td>
<td align="center">dress(1), pants(2)</td>
</tr>
<tr>
<td align="center">wearing hat</td>
<td align="center">hat</td>
<td align="center">no(1), yes(2)</td>
</tr>
<tr>
<td align="center">carrying backpack</td>
<td align="center">backpack</td>
<td align="center">no(1), yes(2)</td>
</tr>
<tr>
<td align="center">carrying bag</td>
<td align="center">bag</td>
<td align="center">no(1), yes(2)</td>
</tr>
<tr>
<td align="center">carrying handbag</td>
<td align="center">handbag</td>
<td align="center">no(1), yes(2)</td>
</tr>
<tr>
<td align="center">age</td>
<td align="center">age</td>
<td align="center">young(1), teenager(2), adult(3), old(4)</td>
</tr>
<tr>
<td align="center">8 color of upper-body clothing</td>
<td align="center">upblack, upwhite, upred, uppurple, upyellow, upgray, upblue, upgreen</td>
<td align="center">no(1), yes(2)</td>
</tr>
<tr>
<td align="center">9 color of lower-body clothing</td>
<td align="center">downblack, downwhite, downpink, downpurple, downyellow, downgray, downblue, downgreen,downbrown</td>
<td align="center">no(1), yes(2)</td>
</tr>
</tbody></table>
<ul>
<li>download  DukeMTMC baiduyun <a href="https://pan.baidu.com/s/1jS0XM7Var5nQGcbf9xUztw" target="_blank" rel="noopener">https://pan.baidu.com/s/1jS0XM7Var5nQGcbf9xUztw</a> (密码 bhbh)</li>
<li>download DukeMTMC Google Drive <a href="https://drive.google.com/open?id=1jjE85dRCMOgRtvJ5RQV9-Afs-2_5dY3O" target="_blank" rel="noopener">https://drive.google.com/open?id=1jjE85dRCMOgRtvJ5RQV9-Afs-2_5dY3O</a></li>
<li>download-annotation-git <a href="https://github.com/vana77/DukeMTMC-attribute" target="_blank" rel="noopener">https://github.com/vana77/DukeMTMC-attribute</a> （The annotations are contained in the file duke_attribute.mat.）</li>
</ul>
<table>
<thead>
<tr>
<th align="center">attribute</th>
<th align="center">representation in file</th>
<th align="center">label</th>
</tr>
</thead>
<tbody><tr>
<td align="center">gender</td>
<td align="center">gender</td>
<td align="center">male(1), female(2)</td>
</tr>
<tr>
<td align="center">length of upper-body clothing</td>
<td align="center">top</td>
<td align="center">short upper body clothing(1), long(2)</td>
</tr>
<tr>
<td align="center">wearing boots</td>
<td align="center">boots</td>
<td align="center">no(1), yes(2)</td>
</tr>
<tr>
<td align="center">wearing hat</td>
<td align="center">hat</td>
<td align="center">no(1), yes(2)</td>
</tr>
<tr>
<td align="center">carrying backpack</td>
<td align="center">backpack</td>
<td align="center">no(1), yes(2)</td>
</tr>
<tr>
<td align="center">carrying bag</td>
<td align="center">bag</td>
<td align="center">no(1), yes(2)</td>
</tr>
<tr>
<td align="center">carrying handbag</td>
<td align="center">handbag</td>
<td align="center">no(1), yes(2)</td>
</tr>
<tr>
<td align="center">color of shoes</td>
<td align="center">shoes</td>
<td align="center">dark(1), light(2)</td>
</tr>
<tr>
<td align="center">8 color of upper-body clothing</td>
<td align="center">upblack, upwhite, upred, uppurple, upgray, upblue, upgreen, upbrown</td>
<td align="center">no(1), yes(2)</td>
</tr>
<tr>
<td align="center">7 color of lower-body clothing</td>
<td align="center">downblack, downwhite, downred, downgray, downblue, downgreen, downbrown</td>
<td align="center">no(1), yes(2)</td>
</tr>
</tbody></table>
<h2 id="WIDER"><a href="#WIDER" class="headerlink" title="WIDER"></a>WIDER</h2><ul>
<li>home <a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERAttribute.html" target="_blank" rel="noopener">http://mmlab.ie.cuhk.edu.hk/projects/WIDERAttribute.html</a></li>
<li>paper <a href="http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2016_human.pdf" target="_blank" rel="noopener">http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2016_human.pdf</a></li>
<li>download-images <a href="https://drive.google.com/file/d/0B-PXtfvNMLanWEVCaHZnR0RHSlE/view" target="_blank" rel="noopener">https://drive.google.com/file/d/0B-PXtfvNMLanWEVCaHZnR0RHSlE/view</a></li>
<li>download-annotation <a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERAttribute_files/wider_attribute_annotation.zip" target="_blank" rel="noopener">http://mmlab.ie.cuhk.edu.hk/projects/WIDERAttribute_files/wider_attribute_annotation.zip</a></li>
<li>WIDER Attribute is a large-scale human attribute dataset. It contains 13789 images belonging to 30 scene categories, and 57524 human bounding boxes each annotated with 14 binary attributes.</li>
<li><img src="/images/PAR/WIDER1.png" alt="WIDER1.png"></li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/PAR/" rel="tag"># PAR</a>
              <a href="/tags/cv/" rel="tag"># cv</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/02/22/Human_Parsing_Survey/" rel="prev" title="Human Parsing Survey">
      <i class="fa fa-chevron-left"></i> Human Parsing Survey
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Intro"><span class="nav-number">1.</span> <span class="nav-text">Intro</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Foreword"><span class="nav-number">1.1.</span> <span class="nav-text">Foreword</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#What-is-Pedestrian-Attribute-Recognition"><span class="nav-number">1.2.</span> <span class="nav-text">What is Pedestrian Attribute Recognition</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Traditional-practice-and-more"><span class="nav-number">1.3.</span> <span class="nav-text">Traditional practice and more</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PROBLEM-FORMULATION-AND-CHALLENGES"><span class="nav-number">1.4.</span> <span class="nav-text">PROBLEM FORMULATION AND CHALLENGES</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Evaluation-Criteria"><span class="nav-number">1.5.</span> <span class="nav-text">Evaluation Criteria</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#REGULAR-PIPELINE-FOR-PAR"><span class="nav-number">1.6.</span> <span class="nav-text">REGULAR PIPELINE FOR PAR</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#APPLICATIONS"><span class="nav-number">1.7.</span> <span class="nav-text">APPLICATIONS</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Algorithm"><span class="nav-number">2.</span> <span class="nav-text">Algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Global-Image-based-Method"><span class="nav-number">2.1.</span> <span class="nav-text">Global Image-based Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ACN"><span class="nav-number">2.1.1.</span> <span class="nav-text">ACN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DeepSAR-DeepMAR"><span class="nav-number">2.1.2.</span> <span class="nav-text">DeepSAR&#x2F;DeepMAR</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MTCNN"><span class="nav-number">2.1.3.</span> <span class="nav-text">MTCNN</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Attention-based-Method"><span class="nav-number">2.2.</span> <span class="nav-text">Attention-based Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#HydraPlus-Net"><span class="nav-number">2.2.1.</span> <span class="nav-text">HydraPlus-Net</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VeSPA"><span class="nav-number">2.2.2.</span> <span class="nav-text">VeSPA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DIAA"><span class="nav-number">2.2.3.</span> <span class="nav-text">DIAA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CAM"><span class="nav-number">2.2.4.</span> <span class="nav-text">CAM</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Curriculum-Learning-Method"><span class="nav-number">2.3.</span> <span class="nav-text">Curriculum Learning Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MTCT"><span class="nav-number">2.3.1.</span> <span class="nav-text">MTCT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CILICIA"><span class="nav-number">2.3.2.</span> <span class="nav-text">CILICIA</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Graph-based-Method"><span class="nav-number">2.4.</span> <span class="nav-text">Graph based Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DCSA"><span class="nav-number">2.4.1.</span> <span class="nav-text">DCSA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-AOG"><span class="nav-number">2.4.2.</span> <span class="nav-text">A-AOG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PAR-w-GCN"><span class="nav-number">2.4.3.</span> <span class="nav-text">PAR w&#x2F; GCN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VSGR"><span class="nav-number">2.4.4.</span> <span class="nav-text">VSGR</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Loss-Function-based-Method"><span class="nav-number">2.5.</span> <span class="nav-text">Loss Function based Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#WPAL"><span class="nav-number">2.5.1.</span> <span class="nav-text">WPAL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AWMT"><span class="nav-number">2.5.2.</span> <span class="nav-text">AWMT</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Part-based-Method"><span class="nav-number">2.6.</span> <span class="nav-text">Part-based Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Poselets"><span class="nav-number">2.6.1.</span> <span class="nav-text">Poselets</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RAD"><span class="nav-number">2.6.2.</span> <span class="nav-text">RAD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PANDA"><span class="nav-number">2.6.3.</span> <span class="nav-text">PANDA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MLCNN"><span class="nav-number">2.6.4.</span> <span class="nav-text">MLCNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AAWP"><span class="nav-number">2.6.5.</span> <span class="nav-text">AAWP</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ARAP"><span class="nav-number">2.6.6.</span> <span class="nav-text">ARAP</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DeepCAMP"><span class="nav-number">2.6.7.</span> <span class="nav-text">DeepCAMP</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PGDM"><span class="nav-number">2.6.8.</span> <span class="nav-text">PGDM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DHC"><span class="nav-number">2.6.9.</span> <span class="nav-text">DHC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LGNet"><span class="nav-number">2.6.10.</span> <span class="nav-text">LGNet</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sequential-Prediction-based-Method"><span class="nav-number">2.7.</span> <span class="nav-text">Sequential Prediction based Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CNN-RNN"><span class="nav-number">2.7.1.</span> <span class="nav-text">CNN-RNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#JRL"><span class="nav-number">2.7.2.</span> <span class="nav-text">JRL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GRL"><span class="nav-number">2.7.3.</span> <span class="nav-text">GRL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#JCM"><span class="nav-number">2.7.4.</span> <span class="nav-text">JCM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RCRA"><span class="nav-number">2.7.5.</span> <span class="nav-text">RCRA</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Dataset"><span class="nav-number">3.</span> <span class="nav-text">Dataset</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#overview"><span class="nav-number">3.1.</span> <span class="nav-text">overview</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PETA"><span class="nav-number">3.2.</span> <span class="nav-text">PETA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RAP-Richly-Annotated-Pedestrian"><span class="nav-number">3.3.</span> <span class="nav-text">RAP Richly Annotated Pedestrian</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PA-100K"><span class="nav-number">3.4.</span> <span class="nav-text">PA-100K</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Market-1501-Attribute-amp-DukeMTMC-attribute"><span class="nav-number">3.5.</span> <span class="nav-text">Market-1501 Attribute &amp; DukeMTMC-attribute</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#WIDER"><span class="nav-number">3.6.</span> <span class="nav-text">WIDER</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Bei"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Bei</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Bei</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
