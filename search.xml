<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Boosting</title>
    <url>/2020/02/23/Boosting/</url>
    <content><![CDATA[<p>梳理 Boosting 相关算法原理</p>
<a id="more"></a>
<!-- toc -->
<hr>
<h2 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h2><ul>
<li>参考链接<ul>
<li><a href="https://www.cnblogs.com/ScorpioLu/p/8295990.html" target="_blank" rel="noopener">https://www.cnblogs.com/ScorpioLu/p/8295990.html</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/58052322" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/58052322</a> <a href="https://zhuanlan.zhihu.com/p/26215100" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26215100</a></li>
</ul>
</li>
<li><img src="/images/ml/adaboost1.png" alt="adaboost1.png"></li>
<li>基于这个原理理解，对于adaboost既可以是回归树也可以是分类树，对于每一次迭代产生的loss，都依赖上一轮的迭代结果，从而产生不同划分的tree（弱分类器）</li>
<li><img src="/images/ml/adaboost2.png" alt="adaboost2.png"></li>
<li>这个图很能说明adaboost的想法，通过不断更新样本权重，生成数个弱分类器，ensemble得到结果</li>
<li><img src="/images/ml/adaboost3.png" alt="adaboost3.png"><ol>
<li>step 1<ul>
<li>初始化所有样本的weight为 1/N </li>
</ul>
</li>
<li>step2<ul>
<li>计算 t时刻 error = sum(wt(i) * (yi == h(xi))) / sum(wi)   # sum(wi)=1</li>
<li>alpha(t) = 0.5 * ln(1/error - 1)</li>
<li>update w(t+1)(i) = wt(i) <em> exp(alpha(t) </em> (yi == h(xi))) / Z (其中Z为归一化参数，为sum(w(t+1)))</li>
</ul>
</li>
<li>step3<ul>
<li>FINAL = sgn(accumlatesum(alpha*h(x), 1, T)) </li>
</ul>
</li>
</ol>
</li>
<li><img src="/images/ml/adaboost4.png" alt="adaboost4.png"></li>
<li><img src="/images/ml/adaboost5.png" alt="adaboost5.png"></li>
<li>综上所述，adaboost的想法是在输入上迭代参数，re-weght，迭代多颗树综合出结果，类似 deep learning 中hard mining的操作，ohem focal等</li>
</ul>
<h2 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h2><ul>
<li>paper <a href="http://docs.salford-systems.com/GreedyFuncApproxSS.pdf" target="_blank" rel="noopener">http://docs.salford-systems.com/GreedyFuncApproxSS.pdf</a></li>
<li>参考链接 <a href="https://www.jianshu.com/p/405f233ed04b" target="_blank" rel="noopener">https://www.jianshu.com/p/405f233ed04b</a> <a href="https://www.cnblogs.com/ScorpioLu/p/8296994.html" target="_blank" rel="noopener">https://www.cnblogs.com/ScorpioLu/p/8296994.html</a> </li>
<li>区别于 adaboost， Gradient boosting在原始定义中只会是回归树，如果将分类问题转化为概率问题也可以适用于GBDT</li>
<li>对于adaboost而言，每一迭代的树的输入是re-weight后的原始数据，是可以独立出结果的树，是一个臭皮匠</li>
<li>而对于GBDT，这是一个梯度下降树，最关键的点在于损失函数的数值优化可以看成是在函数空间而不是参数空间，是继先人之志出的结果</li>
<li>简单说，GBDT出发点并非hard mining，而是用输入空间模拟梯度</li>
<li>以下 摘自 李航 机器学习方法<ul>
<li><img src="/images/ml/gbdt1.png" alt="gbdt1.png"></li>
<li><img src="/images/ml/gbdt2.png" alt="gbdt2.png"></li>
<li><img src="/images/ml/gbdt3.png" alt="gbdt3.png"></li>
<li><img src="/images/ml/gbdt4.png" alt="gbdt4.png"></li>
<li><img src="/images/ml/gbdt5.png" alt="gbdt5.png"></li>
<li><img src="/images/ml/gbdt6.png" alt="gbdt6.png"></li>
</ul>
</li>
<li>其中， <img src="/images/ml/gbdt7.png" alt="gbdt7.png"> 为回归树更新方式，对于 最小二乘的MSEloss，rmi = -((y-f(x))^2)’ | f(x) = -(-2y<em>f(x) + f(x)*</em>2)’ = 2y - 2f(x)</li>
<li>其中对Cmj（叶节点区域拟合值）的求解，在很多的地方都没有详细的说明，在GBDT的说明中也只有 <img src="/images/ml/gbdt8.png" alt="gbdt8.png"> 这样一句，意思是说，对于每一个叶节点划分区域，计算其使Loss最小的Cmj值最为拟合值，以MSE为例，求解过程如下<ul>
<li><img src="/images/ml/gbdt9.jpg" alt="gbdt9.png"></li>
</ul>
</li>
<li>那么对于二分类问题如何构建 GBDT 呢</li>
<li>我们知道，对于分类问题，一般使用 熵 来表示，即</li>
<li><img src="/images/ml/gbdt10.png" alt="gbdt10.png"></li>
<li>我们知道对于 LR 而言</li>
<li><img src="/images/ml/gbdt11.png" alt="gbdt11.png"></li>
<li>故 <img src="/images/ml/gbdt12.png" alt="gbdt12.png"> 可改写为 <img src="/images/ml/gbdt13.png" alt="gbdt13.png"></li>
<li><img src="/images/ml/gbdt14.png" alt="gbdt14.png"></li>
<li><img src="/images/ml/gbdt15.png" alt="gbdt15.png"></li>
<li><img src="/images/ml/gbdt16.png" alt="gbdt16.png"></li>
<li>其中 newton-raphson 指的是牛顿迭代</li>
<li><img src="/images/ml/gbdt17.png" alt="gbdt17.png"></li>
</ul>
<h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><ul>
<li>paper <a href="https://arxiv.org/pdf/1603.02754.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.02754.pdf</a></li>
<li>git <a href="https://github.com/dmlc/xgboost" target="_blank" rel="noopener">https://github.com/dmlc/xgboost</a></li>
<li><img src="/images/ml/xgboost1.png" alt="xgboost1.png"></li>
<li><img src="/images/ml/xgboost2.png" alt="xgboost2.png"><ul>
<li>我们设计和构建高度可扩展的端到端提升树系统。 </li>
<li>我们提出了一个理论上合理的加权分位数略图。 这个东西就是推荐分割点的时候用，能不用遍历所有的点，只用部分点就行，近似地表示，省时间。</li>
<li>我们引入了一种新颖的稀疏感知算法用于并行树学习。 令缺失值有默认方向。</li>
<li>我们提出了一个有效的用于核外树形学习的缓存感知块结构。 用缓存加速寻找排序后被打乱的索引的列数据的过程。</li>
</ul>
</li>
<li><img src="/images/ml/xgboost3.png" alt="xgboost3.png"></li>
<li><img src="/images/ml/xgboost4.png" alt="xgboost4.png"></li>
<li>修改了祖传L函数，新增了Ω项用于估计模型复杂度。作者表示这个Ω也是这个 Regularized greedy forest 首先提出的，这里简化了来使用</li>
<li>喜闻乐见的推导环节，祖传展开</li>
<li><img src="/images/ml/xgboost5.png" alt="xgboost5.png"></li>
<li>用gi表示一阶导，用hi表示二阶导，Taylor展开近似，O(f(x)**3)丢掉算了</li>
<li><img src="/images/ml/xgboost6.png" alt="xgboost6.png"></li>
<li>丢掉常数项 L(t-1)</li>
<li><img src="/images/ml/xgboost7.png" alt="xgboost7.png"></li>
<li>代入Ω项展开，合并</li>
<li><img src="/images/ml/xgboost8.png" alt="xgboost8.png"></li>
<li>其中，wj为t时刻新tree上的权值（拟合值）。对  ~L(t) ，对其对 wj 求导 等于0，得到 如下最优解，带入得</li>
<li><img src="/images/ml/xgboost9.png" alt="xgboost9.png"></li>
<li>分裂依据就等于 (分裂后Loss - 分裂前Loss) / 2  - gamma 主要是依据分裂为二叉树所以除2 分裂后复杂度+1 所以-gamma</li>
<li><img src="/images/ml/xgboost10.png" alt="xgboost10.png"></li>
<li>上图简单表示下</li>
<li><img src="/images/ml/xgboost11.png" alt="xgboost11.png"></li>
<li>其中T的深度为3, 总结起来就是如下所示</li>
<li><img src="/images/ml/xgboost12.png" alt="xgboost12.png"></li>
<li>Shrinkage and Column Subsampling<ul>
<li>第一种技术是Friedman引入的收缩。在每一次提升树训练迭代后，在前面乘一个因子η来收缩其权重（也就是我们说的学习率，或者叫步长）。与随机优化中的学习率类似，收缩减少了每棵树的影响，并为将来的树模型留出了改进模型的空间</li>
<li>shrinkage = 0.1，传统的boosting往往没有这玩意，和我们deep learning里的lr基本一致的理解，略有不同</li>
<li>详见99年的paper <a href="https://astro.temple.edu/~msobel/courses_files/StochasticBoosting(gradient).pdf" target="_blank" rel="noopener">https://astro.temple.edu/~msobel/courses_files/StochasticBoosting(gradient).pdf</a></li>
<li>第二种技术上列（特征）子采样。这个技术用于随机森林中，用于梯度增强，但未在现有的开源包中实现。根据用户反馈，使用列子采样可以比传统的行子采样（也支持）更能防止过度采样。列子采样还能加速稍后描述的并行算法。</li>
<li>在boosting中也学习RF的random feature的做法，在输入属性上也随机</li>
</ul>
</li>
<li>Approximate Algorithm for  SPLIT FINDING ALGORITHMS<ul>
<li>上原文简图</li>
<li><img src="/images/ml/xgboost13.png" alt="xgboost13.png"></li>
<li>Sk 是 feature k 的分位点，可以整树做一次或者每次划分都做一次(随着划分，整体划分范围会发生改变)</li>
<li>对每一颗树的每一个叶节点进行一阶导Gi和二阶导Hi的统计</li>
<li>对于每一个叶节点，其一阶导==所有元素一阶导的和，即下图<br><img src="/images/ml/xgboost14.png" alt="xgboost14.png"></li>
<li>其中提到了一点，所谓的Global Split还是Local Split，作者是建议对深的树用local较好，global 和 local 所需的参数有所不同，具体如下图所示<br><img src="/images/ml/xgboost15.png" alt="xgboost15.png"></li>
<li>其中，eps越小划分越细，exact 代表常规的gbdt的全部划分的做法，实验证明，对于Global需要更多的划分点来保证精度，对于Local则可以少一些划分点但是每次分裂后需要重新计算划分点</li>
<li>值得一提的是，常见的GBDT往往是6层，当深度等于6时，Global和Local产生的划分点数量一致，所以用这样的比例来实验</li>
<li>具体来看看划分公式<br><img src="/images/ml/xgboost16.png" alt="xgboost16.png"></li>
<li>计算每一个输入值的二阶导，计算二阶导累加间距约等于eps的点，其中 式8 代表z处rk(z)的计算方式。</li>
<li>式9 看起来复杂，其实就是说了一件事情，获取 sum(h) 的等分点，当大于eps时生成新的区间</li>
<li>作者这里有提及说为什么使用h作为划等分点的依据，有兴趣的朋友可以看看下图</li>
<li><img src="/images/ml/xgboost17.png" alt="xgboost17.png"></li>
</ul>
</li>
<li>Sparsity-aware Split Finding<ul>
<li>对于缺失值处理是本文的一大两点之一，上图看具体怎么做</li>
<li><img src="/images/ml/xgboost18.png" alt="xgboost18.png"></li>
<li>但对于tree的每个节点，xgboost会生成额外的default属性用于缺失值推论，如X1作为输入，Age缺失，Gender male，所以是左左，相应的X2就是左右</li>
<li>那具体应该如何计算default点呢</li>
<li><img src="/images/ml/xgboost19.png" alt="xgboost19.png"></li>
<li>对于node的整体集合 I，提出 x ！= missing 的集合 Ik，计算节点G H</li>
<li>对非空节点集合Ik做划分，假设所有空值节点被划分到右，则左划分的 Gl = sum(G{1 → j})，剩余右节点的Gr = G - Gl，由此得到划分对于空节点的影响</li>
<li>反之亦然，最终得到最大score，得到空值在该node的default选择</li>
<li><img src="/images/ml/xgboost20.png" alt="xgboost20.png"></li>
<li>相较于传统的做法（例如填入均值等），这种sparsity aware algorithm能加速近50倍</li>
</ul>
</li>
<li>SYSTEM DESIGN</li>
<li>在系统层面，xgboost做的也改进很多<ul>
<li>Column Block for Parallel Learning<ul>
<li>In order to reduce the cost of sorting, we propose to store the data in in-memory units, which we called block. Data in each block is stored in the compressed column (CSC) format, with each column sorted by the corresponding feature value.</li>
<li><img src="/images/ml/xgboost21.png" alt="xgboost21.png"></li>
<li>简答说就是，对每个特征（空值过滤），都进行排序，计算g h，存储结果到block，一方面避免了反复计算梯度，另一方面在计算分位点的时候也可以减少很多的计算，此外这样的结构也为多线程和分布式计算提供了可能</li>
<li>为什么先排序的方法会更有效？虽然直观上很容易理解，但是大佬是严谨的，能证明绝对不放过</li>
<li><img src="/images/ml/xgboost22.png" alt="xgboost22.png"></li>
<li>就一个特征为例进行分析， ||x||0log(n)代表n个样本的排序成本（常见的堆排序的时间复杂度，下图演示），K代表tree数量，d代表单树depth</li>
<li>对于常规的xgboost，假形成的树为完全二叉树，每一颗树每一层需要先进行O(||x||0log(n))的排序再进行||x||0次划分，所以时间复杂度是O(Kd||x||0log(n))</li>
<li>block的做法，在初始化的时候就排序O(||x||0log(n))，之后就是正常的CART的时间消耗，即O(Kd||x||0)，所有cost是O(Kd||x||0+||x||0log(n))</li>
<li>使用分位估计点算法后，设划分次数为q，则常规的xgboost的时间消耗为 O(Kd||x||0log(q))</li>
<li>同样的，使用分位估计点算法同时使用block，设B是每个块中的最大行数，cost是O(Kd||x||0+||x||0log(B))</li>
<li><img src="/images/ml/xgboost23.gif" alt="xgboost23.png"></li>
</ul>
</li>
<li>Cache-aware Access<ul>
<li>这里分两部分，一部分是 continuous memory，这个我们在torch上也有continuous的操作，思路是一样的，主要是时查找对象内存在一处从而提速</li>
<li><img src="/images/ml/xgboost24.png" alt="xgboost24.png"></li>
<li>虽然block结构有助于优化分割点查找的时间复杂度，但是算法需要通过行索引间接提取梯度统计量，因为这些值是按特征的顺序访问的，这是一种非连续的内存访问（意思就是按值排序以后指针就乱了）。 分割点枚举的简单实现在累积和非连续内存提取之间引入了即时读/写依赖性（参见图8）。 当梯度统计信息不适合CPU缓存进而发生缓存未命中时，这会减慢分割点查找的速度。</li>
<li>对于贪心算法，我们可以通过缓存感知预取算法来缓解这个问题。 具体来说，我们在每个线程中分配一个内部缓冲区，获取梯度统计信息并存入，然后以小批量方式执行累积。 预取的操作将直接读/写依赖关系更改为更长的依赖关系，有助于数据行数较大时减少运行开销。 图7给出了Higgs和Allstate数据集上缓存感知与非缓存感知算法的比较。 我们发现，当数据集很大时，实现缓存感知的贪婪算法的运行速度是朴素版本的两倍。</li>
<li>对于近似算法，我们通过选择正确的block尺寸来解决问题。 我们将block尺寸定义为block中包含的最大样本数，因为这反映了梯度统计量的高速缓存存储成本。 选择过小的block会导致每个线程的工作量很小，并行计算的效率很低。 另一方面，过大的block会导致高速缓存未命中现象，因为梯度统计信息不适合CPU高速缓存。良好的block尺寸平衡了这两个因素。 我们在两个数据集上比较了block大小的各种选择，结果如图9所示。该结果验证了我们的讨论，并表明每个块选择2**16个样本可以平衡缓存资源利用和并行化效率。</li>
<li>我觉得这个人翻译的不错，虽然翻译了我仍然不是很理解，主要是缓存和计算效率之间的优化，主要是计算在CPU上的优化，具体效果如下</li>
<li>原文链接：<a href="https://blog.csdn.net/zhaojc1995/article/details/89238051" target="_blank" rel="noopener">https://blog.csdn.net/zhaojc1995/article/details/89238051</a></li>
<li><img src="/images/ml/xgboost25.png" alt="xgboost25.png"></li>
<li>在 10M 起的数据集上效果显著，1M的就影响不大</li>
<li><img src="/images/ml/xgboost26.png" alt="xgboost26.png"></li>
<li>对于分块算法，在块大小尽可能大且接近2**16时，计算效率最高</li>
</ul>
</li>
<li>Blocks for Out-of-core Computation<ul>
<li>我们系统的一个目标是充分利用机器的资源来实现可扩展的学习。 除处理器和内存外，利用磁盘空间处理不适合主内存的数据也很重要。为了实现核外计算，我们将数据分成多个块并将每个块存储在磁盘上。在计算过程中，使用独立的线程将块预取到主存储器缓冲区是非常重要的，因为计算可以因此在磁盘读取的情况下进行。但是，这并不能完全解决问题，因为磁盘读取会占用了大量计算时间。减少开销并增加磁盘IO的吞吐量非常重要。 我们主要使用两种技术来改进核外计算。</li>
<li>Block Compression 我们使用的第一种技术是块压缩。该块从列方向压缩，并在加载到主存储器时通过独立的线程进行解压。这可以利用解压过程中的一些计算与磁盘读取成本进行交换。我们使用通用的压缩算法来压缩特征值。对于行索引，我们通过块的起始索引开始减去行索引，并使用16位整型来存储每个偏移量。这要求每个块有2^16个样本，这也被证实是一个好的设置（好的设置指的是2^16这个数字的设置）。在我们测试的大多数数据集中，我们实现了大约26％到29％的压缩率。</li>
<li>Block Sharding第二种技术是以另一种方式将数据分成多个磁盘。为每个磁盘分配一个实现预取的线程，并将数据提取到内存缓冲区中。然后，训练线程交替地从每个缓冲区读取数据。当有多个磁盘可用时，这有助于提高磁盘读取的吞吐量。</li>
<li>此处优化是分布式并行优化，不具体讨论</li>
<li>原文链接：<a href="https://blog.csdn.net/zhaojc1995/article/details/89238051" target="_blank" rel="noopener">https://blog.csdn.net/zhaojc1995/article/details/89238051</a></li>
</ul>
</li>
</ul>
</li>
<li>与主流boosting系统对比<ul>
<li><img src="/images/ml/xgboost27.png" alt="xgboost27.png"></li>
</ul>
</li>
<li>experiment<ul>
<li><img src="/images/ml/xgboost28.png" alt="xgboost28.png"></li>
</ul>
</li>
<li>简单说就是，相较于其他，xgboost速度快效果好，一时无两</li>
</ul>
<h2 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a>LightGBM</h2><ul>
<li>paper <a href="https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf" target="_blank" rel="noopener">LightGBM: A Highly Efficient Gradient Boosting Decision Tree</a></li>
<li>git <a href="https://github.com/Microsoft/LightGBM" target="_blank" rel="noopener">https://github.com/Microsoft/LightGBM</a></li>
<li>参考 <ul>
<li><a href="https://zhuanlan.zhihu.com/p/91167170" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/91167170</a></li>
<li><a href="https://blog.csdn.net/zhaojc1995/article/details/88382424" target="_blank" rel="noopener">https://blog.csdn.net/zhaojc1995/article/details/88382424</a></li>
</ul>
</li>
<li>论文导读<br>-摘要<ul>
<li>梯度提升树（Gradient Boosting Decision Tree，GBDT）是一种非常流行的机器学习算法，并且有很多有效的实现，如XGBoost和pGBT。尽管在这些实现中已经采用了很多工程优化的手段，但是当特征维度高、数据量大时，其效率和可扩展性仍然不能令人满意。主要的原因是，对于每个特征，需要扫描所有数据点，计算所有可能的分割节点的信息增益，这非常耗时。为了解决这个问题，我们提出了两种新技术：基于梯度的单侧采样（Gradient-based One-Side Sampling ，GOSS）和互斥特征捆绑（Exclusive Feature Bundling ，EFB）。在GOSS方法中，我们显著减少了梯度小的数据点的比例，仅使用剩下的数据来计算信息增益。我们证明，具有较大梯度的数据在信息增益的计算中起着更重要的作用，因此利用GOSS方法计算得到的信息增益即使只用了较少数据，精度也非常高 。在EFB方法中，我们捆绑互斥的特征（即，它们很少同时取非零值）去减少特征数量。我们证明找到最理想的特征捆绑的解法是NP难的，但是贪心算法可以达到近似效果（这样可以有效地减少特征的数量又不会大大降低分割节点最终的准确性）。利用GOSS和EFB方法对GBDT的新的优化实现，我们叫它lightGBM。我们针对多个公开数据集做了实验，实验表明LightGBM可以使传统GBDT的训练过程加速20倍以上，同时实现几乎相同的精度。<ul>
<li>main contribution就是 GOSS 和 EFB 方法</li>
</ul>
</li>
<li>基于梯度的单侧采样（GOSS）。 虽然GBDT中的数据样本没有初始权重，但我们注意到不同梯度的数据样本在信息增益的计算中起着不同的作用。根据信息增益的定义，具有较大梯度的那些数据（即训练不足的实例）将对信息增益做出更多贡献。 因此，当对数据样本进行降采样时，为了保持信息增益计算的准确性，我们应该更好地保持那些具有大梯度（例如，大于预定阈值，或者在最高百分位数之间）的样本，仅随机删除那些梯度小的样本。 我们证明，在相同的采样率下，这种处理方法最终计算出的信息增益比均匀随机采样要准确，特别是当信息增益的值具有大范围时。</li>
<li>互斥特征捆绑（EFB）。 在实际应用中，通常是特征维度大但特征空间非常稀疏，这给我们提供了设计出一种几乎无损失的减少有效特征数量的方法的可能性。具体地说，在稀疏特征空间中，许多特征（几乎）是互斥的，即它们很少同时取非零值。 示例包括单热特征（例如，文本挖掘中的独热编码）。 我们可以放心地捆绑这些互斥特征。 为此，我们通过将最佳捆绑问题减少到图着色问题来设计一种有效的算法（通过将特征作为顶点并为每两个特征添加边缘，如果它们不相互排斥），并通过贪婪算法解决它。 恒定近似比。</li>
</ul>
</li>
<li>Histogram-based Algorithm<ul>
<li>在介绍GOSS之前，作者先介绍了 Histogram-based Algorithm 用于减少内存和计算量，原文如下</li>
<li>GBDT的主要成本在于学习决策树，而决策树的学习中最耗时的部分是寻找最佳分割点。最常用的寻找分割点的算法是预排序算法(XGBOOST)，它将特征取值预先排序并枚举出所有可能的分割点。该算法简单易行，能找到最优分割点，但在训练速度和内存消耗方面都是低效的。另一种流行的算法是基于直方图的算法，如Alg.1所示。基于直方图的算法在训练中将连续的特征分箱处理成离散值，利用这些离散的箱子构建特征直方图。由于基于直方图的算法在内存消耗和训练速度上都更有效，因此我们将在其基础上开展我们的工作。</li>
<li>如下图，直方图算法基于特征的直方图找最佳分割点。构建直方图的复杂度为O(#data × #feature)，O(#bin × #feature)。通常箱子长度比数据量小得多，所以直方图的构建导致主要的复杂度。如果我们可以减少数据量或特征维度，我们将能够大大加快GBDT的训练过程。</li>
<li><img src="/images/ml/lightgbm1.png" alt="lightgbm1.png"></li>
<li><img src="/images/ml/lightgbm2.png" alt="lightgbm2.png"></li>
<li><img src="/images/ml/lightgbm3.png" alt="lightgbm3.png"></li>
<li>相较于xgboost的分位数预排序方法，直方图算法分割点总是255个，合并子节点，从而减少运算</li>
<li><img src="/images/ml/lightgbm4.png" alt="lightgbm4.png"><ul>
<li>这里就一颗树对 基于直方图的分割算法进行了说明</li>
<li>I 是train data ； d 是该tree的max depth ； m 是feature 维数</li>
<li>设nodeSet表示某tree level的node集 ； rowSet 是 node上的data划分集</li>
<li>遍历 level i 于 depth 1 → d<ul>
<li>遍历 i 上 node 于 nodeSet[i]<ul>
<li>选取当前 data 集 于 rowSet[node]        </li>
<li>遍历 k 于 所有特征m<ul>
<li>依据data划分出新的Histogram，合并bin，计算每个bin的N G H</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>GOSS<ul>
<li>基于histogram-base algorithm，作者再近一步，对梯度贡献小的data进行有选择的采样。假设 threshold 为 gi &lt; 0.1，采样概率为 1/3，则计算方式如下</li>
<li><img src="/images/ml/lightgbm5.png" alt="lightgbm5.png"></li>
<li>对于大于阈值的条目全部保留，对于小于阈值的条目进行抽样选取； 对于非抽样条目，在统计bin时按原始值加 ； 对于抽样条目，在统计bin时按 原始值 / 抽样概率 累加，包括 N G H</li>
<li>原文中的Algorithm描述，是上述流程的推广</li>
<li><img src="/images/ml/lightgbm6.png" alt="lightgbm6.png"></li>
<li>I 是train data ； d 是该tree的max depth ； a是大梯度数据集的采样比例 ； b是小梯度数据集的采样比例 ； loss 是 loss function ； L 是当前tree，即弱学习器</li>
<li>设 model = {} ； fact = (1-a)/b 用于更新小梯度数据集权重； topN = a <em> len(I) 用于确定大梯度数据集数量 ； randN = b </em> len(I) 用于确定小梯度数据集数量 ； 可以得到 topN + fact <em> randN = a </em> len(I) + b <em> len(I) = (a + b</em>((1-a)/b)) * len(I) = len(I)，通过fact的设置保证len(I)在迭代中不发生改变<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">遍历 level i 于 depth 1 → d</span><br><span class="line">preds &#x3D; modes.predict(I) #上一轮预测结果</span><br><span class="line">g &#x3D; loss(I, pred) ; w &#x3D; [1] * len(I)</span><br><span class="line">sorted &#x3D; sorted(abs(g))</span><br><span class="line">topSet &#x3D; sorted[:TopN]</span><br><span class="line">randSet &#x3D; random.sample(sorted[TopN:], randN)</span><br><span class="line">useSet &#x3D; topSet + randSet</span><br><span class="line">w[randSet] *&#x3D; fact </span><br><span class="line">newModel &#x3D; L(I[usedSet], -g[usedSet], w[usedSet])</span><br><span class="line">model.append(newModel)</span><br></pre></td></tr></table></figure>
通过减少小梯度的的采样从而减少计算量<br>在接下来的该节内容 in paper，作者从理论上讨论了 GOSS 采样的 eps 和 界，论证了GOSS是稳定可靠的优于随机采样的算法。有兴趣的朋友可以查看原文</li>
</ul>
</li>
<li>Exclusive Feature Bundling<ul>
<li>互斥特征绑定，说起来很拗口，上图看看</li>
<li><img src="/images/ml/lightgbm7.png" alt="lightgbm7.png"></li>
<li>从图上我们可以知道，绝大多数feature1在0，绝大多数feature2在0，我们是不是可以认为绝大多数 data 在 feature1== 0 and feature2 == 0</li>
<li>假设我们对feature进行合并，则数量不发生改变，我们将feature2中的1,2向右偏移2，总数仍为100(0项代表同时为0)，就得到了新的变量</li>
<li>这个做法有些玄乎，看作者原生的说法</li>
<li>高维数据通常是稀疏的。特征的稀疏性给我们设计一种近乎无损的降低特征维数的方法提供了可能性。具体来说，在稀疏特征空间中，许多特征是互斥的，即它们从不同时取非零值。我们可以放心的把互斥的特征捆绑成一个特征（这种方法我们叫做互斥特征捆绑）。通过精心设计的特征扫描算法，我们可以从捆绑特征构建出相同的直方图。用这种方法，当$ #bundle &lt;&lt; #feature，直方图构建的复杂度就从，直方图构建的复杂度就从，直方图构建的复杂度就从O(#data × #f eature)变成了变成了变成了O(#data × #bundle)$（这里作者的意思是当远小于的时候效果才能凸显出来）。这样我们可以在不损伤精度的情况下大大加速GBDT的训练速度，下面我们将详细说明如何实现这一点。</li>
<li>这里有两个问题需要解决。第一个问题是如何决定哪些特征需要捆绑在一起。第二个问题是如何构建捆绑后的特征。</li>
<li><img src="/images/ml/lightgbm8.png" alt="lightgbm8.png"></li>
<li>Based on the above discussions, we design an algorithm for exclusive feature bundling as shown in Alg. 3. First, we construct a graph with weighted edges, whose weights correspond to the total conflicts between features. Second, we sort the features by their degrees in the graph in the descending order. Finally, we check each feature in the ordered list, and either assign it to an existing bundle with a small conflict (controlled by γ), or create a new bundle. The time complexity of Alg. 3 is O(#feature2 ) and it is processed only once before training. This complexity is acceptable when the number of features is not very large, but may still suffer if there are millions of features. To further improve the efficiency, we propose a more efficient ordering strategy without building the graph: ordering by the count of nonzero values, which is similar to ordering by degrees since more nonzero values usually leads to higher probability of conflicts. Since we only alter the ordering strategies in Alg. 3, the details of the new algorithm are omitted to avoid duplication.</li>
<li>F是特征们 ； K是最大冲突值，阈值 ； 构建图G(G是一个无向权重图，权重代表冲突率，冲突率 = 同时非0的概率) ；searchOrder 表示 依据G中的边大小进行递减排序图</li>
<li>设 bundles = {} 用于表示合并后的新对象bundle的集合 ； bundlesConflict = {} 用于表示 bundle 对应的conflict值，对于每一个bundle，都规定其conflict值超过K时不再合并，作者在这里是表示F中元素的bundle<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">从有序图 searchOrder 中依次取节点i(F中一个feature) 进行遍历</span><br><span class="line">    needNew &#x3D; True (默认需要重新生成bundle)</span><br><span class="line">    在已有的bundles中取 bundle j 遍历</span><br><span class="line">        cnt &#x3D; 计算在bundle j中加入当前节点i后的的conflict值 （需要bundle j对应数据和i对应数据F[i]）</span><br><span class="line">        如果 cnt + bundleConflict[i] &lt;&#x3D; K</span><br><span class="line">            bundles[j].append(F[i]); needNew &#x3D; False</span><br><span class="line">            break</span><br><span class="line">    if needNew </span><br><span class="line">        bundles.append([F[i])</span><br><span class="line">输出bundles</span><br></pre></td></tr></table></figure>
整体思路就是将conflict大的节点先拎出来自成一派，然后conflict小的节点往上凑，能凑上就不独立成一派</li>
<li><img src="/images/ml/lightgbm9.png" alt="lightgbm9.png"><br>得到bundles后需要对bin进行更新<br>numData: data number ； F: 一个 bundle 包含的features ； binRanges bin的范围，对应每一个包含元素 ； totalBin bin的总数<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">遍历 f 于 F</span><br><span class="line">    total +&#x3D; f所包含的bin数量(通常是255？)</span><br><span class="line">    binRanges.append(totalBin)  # 最终就变成了诸如[10, 20, 30]之类的每个f对应的起始点</span><br><span class="line">newBin &#x3D; Bin(numdata) # 声明初始化</span><br><span class="line">遍历 i 于 numdata</span><br><span class="line">    newbin[i] &#x3D; 0 #数值初始化</span><br><span class="line">    遍历 j 于 len(F)</span><br><span class="line">        若 F[j].bin[i] !&#x3D; 0 （某个bin的某个特征不为0）则 按照是哪个特征进行数值转化 newBin[i] &#x3D; F[j].bin[i] + binRanges[j] # 加上对应的起始点</span><br></pre></td></tr></table></figure>
得到了新的bin和binrange</li>
</ul>
</li>
<li>Experiments<ul>
<li>主要是验证 GOSS 和 EFB 的有效性，可忽略</li>
</ul>
</li>
<li>Extra<ul>
<li><img src="/images/ml/lightgbm10.png" alt="lightgbm10.png"></li>
<li>生成树的方式，LightGBM和XGboost也有所不同，原文中仅提到了一句，引用了 Haijian Shi. Best-first decision tree learning. PhD thesis, The University of Waikato, 2007. 的方法，这也是非常大的不同</li>
</ul>
</li>
<li>XGboost可否一战</li>
</ul>
<h2 id="CatBoost"><a href="#CatBoost" class="headerlink" title="CatBoost"></a>CatBoost</h2><ul>
<li>paper<ul>
<li><a href="http://learningsys.org/nips17/assets/papers/paper_11.pdf" target="_blank" rel="noopener">http://learningsys.org/nips17/assets/papers/paper_11.pdf</a></li>
<li><a href="https://arxiv.org/pdf/1706.09516.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1706.09516.pdf</a></li>
</ul>
</li>
<li>git <a href="https://github.com/catboost/catboost" target="_blank" rel="noopener">https://github.com/catboost/catboost</a>     </li>
<li>参考 <a href="http://datacruiser.io/2019/08/19/DataWhale-Workout-No-8-CatBoost-Summary/" target="_blank" rel="noopener">http://datacruiser.io/2019/08/19/DataWhale-Workout-No-8-CatBoost-Summary/</a></li>
</ul>
<ol>
<li>abstract<br> -In this paper we present CatBoost, a new open-sourced gradient boosting library that successfully handles categorical features and outperforms existing publicly available implementations of gradient boosting in terms of quality on a set of popular publicly available datasets. The library has a GPU implementation of learning algorithm and a CPU implementation of scoring algorithm, which are significantly faster than other gradient boosting libraries on ensembles of similar sizes.<br> -在本文中，我们介绍了CatBoost，这是一个新的开源梯度增强库，该库成功处理了分类功能，并且在一组流行的公共可用数据集上，在质量上均胜过了现有的梯度增强现有实现。该库具有学习算法的GPU实现和评分算法的CPU实现，这比相似大小的集成算法上的其他梯度增强库(gbdts)快得多。</li>
<li>Introduction<ul>
<li>Gradient boosting is a powerful machine-learning technique that achieves state-of-the-art results in a variety of practical tasks. For a number of years, it has remained the primary method for learning problems with heterogeneous features, noisy data, and complex dependencies: web search, recommendation systems, weather forecasting, and many others [2, 15, 17, 18]. It is backed by strong theoretical results that explain how strong predictors can be built by iterative combining weaker models (base predictors) via a greedy procedure that corresponds to gradient descent in a function space. Most popular implementations of gradient boosting use decision trees as base predictors. It is convenient to use decision trees for numerical features, but, in practice, many datasets include categorical features, which are also important for prediction. Categorical feature is a feature having a discrete set of values that are not necessary comparable with each other (e.g., user ID or name of a city). The most commonly used practice for dealing with categorical features in gradient boosting is converting them to numbers before training. In this paper we present a new gradient boosting algorithm that successfully handles categorical features and takes advantage of dealing with them during training as opposed to preprocessing time. Another advantage of the algorithm is that it uses a new schema for calculating leaf values when selecting the tree structure, which helps to reduce overfitting. As a result, the new algorithm outperforms the existing state-of-the-art implementations of gradient boosted decision trees (GBDTs) XGBoost [4], LightGBM1 and H2O2 , on a diverse set of popular tasks (Sec. 6). The algorithm is called CatBoost (for “categorical boosting”) and is released in open source.3 CatBoost has both CPU and GPU implementations. The GPU implementation allows for much faster training and is faster than both state-of-the-art open-source GBDT GPU implementations, XGBoost and LightGBM, on ensembles of similar sizes. The library also has a fast CPU scoring implementation, which outperforms XGBoost and LightGBM implementations on ensembles of similar sizes.</li>
<li>梯度提升是一种功能强大的机器学习技术，可在各种实际任务中达到最新的结果。多年来，它一直是学习异类特征，嘈杂数据和复杂依赖性的问题的主要方法：网络搜索，推荐系统，天气预报等[2、15、17、18]。它以强大的理论结果为后盾，这些理论解释了如何通过对应于函数空间中梯度下降的贪婪过程，通过迭代组合较弱的模型（基本预测变量）来构建强大的预测变量。梯度提升的最流行实现将决策树用作基础预测变量。将决策树用于数字特征很方便，但实际上，许多数据集都包含分类特征，这对于预测也很重要。分类特征是具有一组离散值的特征，这些值不需要彼此可比较（例如，用户ID或城市名称）。处理梯度增强中的分类特征最常用的做法是在训练之前将其转换为数字。在本文中，我们提出了一种新的梯度提升算法，该算法可成功处理分类特征，并在训练过程中利用与预处理相对的优势。该算法的另一个优点是，在选择树结构时，它使用新的模式来计算叶值，这有助于减少过度拟合。结果，新算法在一系列流行任务上胜过了现有的最新技术（梯度提升决策树（GBDT）XGBoost [4]，LightGBM1和H2O2）（第6节）。该算法称为CatBoost（用于“分类提升”），并在开源中发布。3CatBoost同时具有CPU和GPU实现。 GPU的实现允许更快的训练，并且比类似大小的集合中的最新的开源GBDT GPU的实现XGBoost和LightGBM都要快。该库还具有快速的CPU计分实现，在类似大小的集成体上，其性能优于XGBoost和LightGBM。</li>
</ul>
</li>
<li>Categorical features<ul>
<li>Categorical features have a discrete set of values called categories which are not necessary comparable with each other; thus, such features cannot be used in binary decision trees directly. A common practice for dealing with categorical features is converting them to numbers at the preprocessing time, i.e., each category for each example is substituted with one or several numerical values.The most widely used technique which is usually applied to low-cardinality categorical features is one-hot encoding: the original feature is removed and a new binary variable is added for each category [14]. One-hot encoding can be done during the preprocessing phase or during training, the latter can be implemented more efficiently in terms of training time and is implemented in CatBoost.</li>
<li>分类特征具有一组离散的值，称为类别，它们不必相互比较；因此，此类特征无法直接在二进制决策树中使用。处理分类特征的一种常见做法是在预处理时将它们转换为数字，即，每个示例的每个类别都被一个或多个数值替代。通常用于低基数分类特征的最广泛使用的技术是一键编码：删除原始特征，并为每个类别添加新的二进制变量[14]。一键编码可以在预处理阶段或训练期间完成，后来可以在训练时间方面更有效地实现，CatBoost中实现了它。</li>
<li>文章的重头戏，如何变化离散值到数值。所谓分类特征就是类似于gender这样非数值型特征，离散型特征。这里说的是one-hot方法将分类特征转化为数值</li>
<li><img src="/images/ml/catboost1.png" alt="catboost1.png"></li>
<li>处理分类特征的另一种方法是使用示例的标签值来计算一些统计信息。假定有一个D = {(Xi, Yi)}，其中Xi是包含m个features的一个向量，Yi是对应的label值。最简单的方法是用整个训练数据集上的平均标签值替换类别。故Xi,k = <img src="/images/ml/catboost2.png" alt="catboost2.png"> 其中k是分类类别，[ ]表示 Iverson Brackets，就是python中的 == 操作，等于为1 else 0。这样的做法显然会带来过拟合，例如，如果在整个数据集中只有一个类别xi,k的示例，则新的数字特征值将等于此示例上的标签值（期望）。解决该问题的一种直接方法是将数据集分为两部分，仅一部分用于计算统计数据，另一部分仅用于执行训练。这减少了过度拟合，但同时也减少了用于训练模型和计算统计数据的数据量。</li>
<li>有人认为，这是用期望来直接encode输入，使分类特征在进行数值化后有理有据</li>
<li>CatBoost uses a more efficient strategy which reduces overfitting and allows to use the whole dataset for training. Namely, we perform a random permutation of the dataset and for each example we compute average label value for the example with the same category value placed before the given one in the permutation. Let σ = (σ1, . . . , σn) be the permutation, then xσp,k is substituted with</li>
<li><img src="/images/ml/catboost3.png" alt="catboost3.png"></li>
<li>where we also add a prior value P and a parameter a &gt; 0, which is the weight of the prior. Adding prior is a common practice and it helps to reduce the noise obtained from low-frequency categories [3]. For regression tasks standard technique for calculating prior is to take the average label value in the dataset. For binary classification task a prior is usually an a priori probability of encountering a positive class [14]. It is also efficient to use several permutations. However, one can see that a straightforward usage of statistics computed for several permutations would lead to overfitting. As we discuss in the next section, CatBoost uses a novel schema for calculating leaf values which allows to use several permutations without this problem.</li>
<li>CatBoost使用更有效的策略来减少过度拟合，并允许将整个数据集用于训练。即，我们对数据集执行随机排列，对于每个示例，我们计算该示例的平均标签值，该示例的平均标签值在排列中位于给定值之前。设σ=（σ1，…，σn）为置换，则xσp，k替换为</li>
<li>这里我们还添加了一个先验值P和一个参数a&gt; 0，这是先验的权重。先验增加是一种常见的做法，它有助于减少从低频类别获得的噪声[3]。对于回归任务，用于计算先验的标准技术是获取数据集中的平均标签值。对于二元分类任务，先验通常是遇到肯定类别的先验概率[14]。使用多个排列也是有效的。但是，可以看到直接使用为多个排列计算的统计量会导致过度拟合。正如我们在下一节中讨论的那样，CatBoost使用一种新颖的模式来计算叶子值，该模式允许使用多个排列而不会出现此问题。</li>
<li>为了减少过拟合，使用了局部统计量来估计当前数值，在这里是p点以前的统计量代表p点，每次迭代都执行随机排序，从而产生噪声。为了使噪声可控，使用全局期望P来作为基数，并分配权重a。</li>
<li>Feature combinations Note that any combination of several categorical features could be considered as a new one. For example, assume that the task is music recommendation and we have two categorical features: user ID and musical genre. Some user prefers, say, rock music. When we convert user ID and musical genre to numerical features according to (1), we loose this information. A combination of two features solves this problem and gives a new powerful feature. However, the number of combinations grows exponentially with the number of categorical features in dataset and it is not possible to consider all of them in the algorithm. When constructing a new split for the current tree, CatBoost considers combinations in a greedy way. No combinations are considered for the first split in the tree. For the next splits CatBoost combines all combinations and categorical features present in current tree with all categorical features in dataset. Combination values are converted to numbers on the fly. CatBoost also generates combinations of numerical and categorical features in the following way: all the splits selected in the tree are considered as categorical with two values and used in combinations in the same way as categorical ones.</li>
<li>特征组合请注意，几个分类特征的任何组合都可以视为新特征。例如，假设任务是音乐推荐，并且我们具有两个分类功能：用户ID和音乐流派。某些用户喜欢摇滚音乐。当根据（1）将用户ID和音乐流派转换为数字特征时，我们会丢失此信息。两种功能的组合解决了此问题，并提供了一个新的强大功能。但是，组合的数量随数据集中分类特征的数量呈指数增长，并且不可能在算法中考虑所有组合。为当前树构造新的拆分时，CatBoost会以贪婪的方式考虑组合。树中的第一个划分不考虑任何组合。对于下一个拆分，CatBoost将当前树中存在的所有组合和分类特征与数据集中的所有分类特征进行组合。组合值会即时转换为数字。 CatBoost还通过以下方式生成数字和分类特征的组合：在树中选择的所有划分均被视为具有两个值的分类，并以与分类值相同的方式组合使用。</li>
<li>这个做法其实是顺水推舟的，如果是onehot，特征组合将会是相对复杂的事情，而catboost是将feature型抓化为数值型，组合就加起来就行，在树的第一次分裂时不考虑变量的合并，从第二次分裂开始，将所有分类变量及其合并后的特征进入合并变量待选进行分裂点的寻找，从而达到多特征组合的目的。相应的，如果cat_features的数目很多的时候，计算量也会增大许多</li>
<li>Important implementation details Another way of substituting category with a number is calculating number of appearances of this category in the dataset. This is a simple but powerful technique and it is implemented in CatBoost. This type of statistic is also calculated for feature combinations. In order to fit the optimal prior at each step of CatBoost algorithm, we consider several priors and construct a feature for each of them, which is more efficient in terms of quality than standard techniques mentioned above.</li>
<li>重要的实现细节用数字替换类别的另一种方法是计算该类别在数据集中的出现次数。这是一种简单但功能强大的技术，已在CatBoost中实现。还针对要素组合计算此类统计信息。为了使CatBoost算法的每个步骤都适合最优先验，我们考虑了几个先验并为每个先验构造一个特征，就质量而言，它比上述标准技术更有效。</li>
<li>直接用频率来embedding category feature，catboost也做了实现</li>
</ul>
</li>
</ol>
<ul>
<li>Fighting Gradient Bias<ul>
<li>CatBoost, as well as all standard gradient boosting implementations, builds each new tree to approximate the gradients of the current model. However, all classical boosting algorithms suffer from overfitting caused by the problem of biased pointwise gradient estimates. Gradients used at each step are estimated using the same data points the current model was built on. This leads to a shift of the distribution of estimated gradients in any domain of feature space in comparison with the true distribution of gradients in this domain, which leads to overfitting. The idea of biased gradients was discussed in previous literature [1] [9]. We have provided a formal analysis of this problem in the paper [5]. The paper also contains modifications of classical gradient boosting algorithm that try to solve this problem. CatBoost implements one of those modifications, briefly described below.</li>
<li>CatBoost以及所有标准的梯度增强实现，都将构建每棵新树以近似当前模型的梯度。然而，所有经典的boosting算法都因有有偏差的逐点梯度估计问题而导致过拟合。使用当前模型所基于的相同数据点来估算在每个步骤中使用的渐变。与该区域中梯度的真实分布相比，这导致在特征空间的任何域中估计梯度的分布发生偏移，从而导致过度拟合。以前的文献[1] [9]中讨论了偏斜的想法。我们在论文[5]中提供了对此问题的正式分析。本文还包含尝试解决此问题的经典梯度提升算法的修改。 CatBoost实现了其中一种修改，下面将对其进行简要介绍。</li>
<li>这里需要介绍为什么boosting问题是有偏差的估计，catboost具体改进了什么为什么能改进</li>
<li>在paper <a href="https://arxiv.org/pdf/1706.09516.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1706.09516.pdf</a> 的第4章节有详细描述，有兴趣的朋友可以一试</li>
<li>In many GBDTs (e.g., XGBoost, LightGBM) building next tree comprises two steps: choosing the tree structure and setting values in leafs after the tree structure is fixed. To choose the best tree structure, the algorithm enumerates through different splits, builds trees with these splits, sets values in the obtained leafs, scores the trees and selects the best split. Leaf values in both phases are calculated as approximations for gradients [8] or for Newton steps. In CatBoost the second phase is performed using traditional GBDT scheme and for the first phase we use the modified version.</li>
<li>在许多GBDT（例如XGBoost，LightGBM）中，构建下一棵树包括两个步骤：选择树结构，并在树结构固定后在叶子中设置值。为了选择最佳的树结构，该算法将枚举不同的分割，用这些分割构建树，在获得的叶子中设置值，对树评分并选择最佳的分割。两个阶段的叶值均以梯度[8]或牛顿阶跃的近似值计算。在CatBoost中，第二阶段使用传统的GBDT方案执行，而第一阶段则使用修改后的版本。</li>
<li>According to intuition obtained from our empirical results and our theoretical analysis in [5], it is highly desirable to use unbiased estimates of the gradient step. Let F i be the model constructed after building first i trees, g i (Xk, Yk) be the gradient value on k-th training sample after building i trees. To make the gradient g i (Xk, Yk) unbiased w.r.t. the model F i , we need to have F i trained without the observation Xk. Since we need unbiased gradients for all training examples, no observations may be used for training F i , which at first glance makes the training process impossible. We consider the following trick to deal with this problem: for each example Xk, we train a separate model Mk that is never updated using a gradient estimate for this example. With Mk, we estimate the gradient on Xk and use this estimate to score the resulting tree. Let us present the pseudo-code that explains how this trick can be performed. Let Loss(y, a) be the optimizing loss function, where y is the label value and a is the formula value.</li>
<li>根据从我们的经验结果和我们在[5]中的理论分析获得的直觉，非常希望使用梯度步的无偏估计。设Fi是构建第i棵树后构建的模型，gi(Xk，Yk)是构建第i棵树后第k个训练样本上的梯度值。为了使梯度gi(Xk，Yk)无偏w.r.t.在模型Fi上，我们需要在没有观测值Xk的情况下对Fi进行训练。由于我们需要所有训练示例的无偏梯度，因此不可将任何观测值用于训练Fi，乍一看这使得训练过程成为不可能。我们考虑以下技巧来解决此问题：对于每个示例Xk，我们训练一个单独的模型Mk，对于该示例，该模型永远不会使用梯度估计进行更新。使用Mk，我们估计Xk上的梯度，并使用此估计值对结果树进行评分。让我们给出解释该技巧如何执行的伪代码。设Loss(y，a)为优化损失函数，其中y为标签值，a为公式值。</li>
<li><img src="/images/ml/catboost4.png" alt="catboost4.png"></li>
<li>Note that Mi is trained without using the example Xi . CatBoost implementation uses the following relaxation of this idea: all Mi share the same tree structures.</li>
<li>请注意，对Mi进行了训练但不使用示例Xi。 CatBoost实现使用了以下这种想法的拓展: 所有Mi共享相同的树结构。</li>
<li>对每一次iter，都会依据random.randn(1,s)进行n次训练，并将相应序列与上一次的结果相加</li>
<li>这里使用另一篇paper的描述会更清晰</li>
<li><img src="/images/ml/catboost5.png" alt="catboost5.png"></li>
<li>对于总数为n的数据集D，catboost会生成n颗gbdt，对于任意的Mi i∈ [0,n]，训练时仅使用部分D[0 : i]，也对应这上面所说的无偏估计</li>
<li>对于具体位置j，在t时刻，residualj = yj - Mj-1t-1 (xj)，其中category_feature也会被对应的使用2中描述的方法 Ordered TS 执行，相辅相成。作者称该方法为 Ordered boosting，具体如下，和上面的Algorithm 1是同一个意思。此处将gradient简化为residual，更简单明晰</li>
<li><img src="/images/ml/catboost6.png" alt="catboost6.png"></li>
<li><img src="/images/ml/catboost7.png" alt="catboost7.png"></li>
<li>在CatBoost中，我们生成训练数据集的s个随机排列。我们使用几种置换来增强算法的鲁棒性：我们对随机置换进行采样并在其基础上获得梯度。这些排列与用于计算分类特征统计量的排列相同。我们使用不同的排列来训练不同的模型，因此使用多个排列不会导致过度拟合。对于每个排列σ，我们训练n个不同的模型Mi，如上所示。这意味着，要构建一棵树，我们需要针对每个排列σ存储并重新计算O（n 2）近似值：对于每个模型Mi，我们必须更新Mi（X1），。 。 。 ，Mi（Xi）。因此，该操作的结果复杂度为O（s n2）。在我们的实际实现中，我们使用了一个重要的技巧来将一棵树的构造的复杂度降低为O（sn）：对于每个排列，我们不保存和更新O（n 2）值Mi（Xj），而是保持值M0 i（ Xj），i = 1，，。 。 。 ，[log2（n）]，j &lt;2 i + 1，其中M0 i（Xj）是基于前两个i样本的样本j的近似值。然后，预测数M0 i（Xj）不大于P0≤i≤log2（n）2 i + 1 &lt;4n。根据近似值M0 i（Xk）估算用于选择树形结构的示例Xk的梯度，其中i = [log2（k）]。</li>
<li>简单来说就是通过近似采样来代替全部计算</li>
</ul>
</li>
<li>Fast scorer<ul>
<li>CatBoost uses oblivious trees as base predictors. In such trees the same splitting criterion is used across an entire level of the tree [12, 13]. Such trees are balanced and less prone to overfitting. Gradient boosted oblivious trees were successfully used in various learning tasks [7, 10]. In oblivious trees each leaf index can be encoded as a binary vector with length equal to the depth of the tree. This fact is widely used in CatBoost model evaluator: we first binarize all used float features, statistics and one-hot encoded features and then use binary features to calculate model predictions. All binary feature values for all examples are stored in a continuous vector B. Leaf values are stored in a float vectors of size 2 d , where d is the tree depth. To calculate the leaf index for the t-th tree and for an example x we build a binary vector Pd−1 i=0 2 i · B(x, f(t, i)), where B(x, f) is the value of the binary feature f on the example x that we read from the vector B and f(t, i) is the number of the binary feature from t-th tree on depth i. That vectors can be built in a data parallel manner which gives up to 3x speedup. This results in a much faster scorer than all existing ones as shown in our experiments.</li>
<li>CatBoost使用完全对称树作为基础预测变量。在这样的树中，在树的整个级别上使用相同的分割标准[12、13]。这样的树木很平衡，不太容易过拟合。梯度增强的遗忘树已成功用于各种学习任务中[7，10]。在遗忘树中，每个叶子索引都可以被编码为二进制矢量，其长度等于树的深度。这个事实在CatBoost模型评估器中得到了广泛使用：我们首先对所有使用的浮点特征，统计信息和一次性编码特征进行二值化，然后使用二进制特征来计算模型预测。所有示例的所有二进制特征值都存储在连续向量B中。叶值存储在大小为2 d的浮点向量中，其中d是树的深度。为了计算第t棵树的叶子索引，对于x示例，我们建立了二进制矢量Pd-1 i = 0 2 i·B（x，f（t，i）），其中B（x，f）为从向量B读取的示例x上的二值特征f的值，f（t，i）是深度t上来自第t树的二值特征的数目。可以以并行数据的方式构建矢量，从而使速度提高3倍。如我们的实验所示，这会导致打分器比所有现有打分器快得多。</li>
<li>使用了完全对称树，就是快</li>
<li>那么这个完全对称树是个啥玩意呢？</li>
<li>参考 <a href="https://www.zhihu.com/question/311641149/answer/593286799" target="_blank" rel="noopener">https://www.zhihu.com/question/311641149/answer/593286799</a></li>
<li>我们常见的普通决策树都是如下所示的树，每一个node都会有独立的最大gain割</li>
<li><img src="/images/ml/catboost8.png" alt="catboost8.png"></li>
<li>但是完全对称树不同，在同一level，完全对称树的split的条件是一致的，如下所示（颜色一致表示同一割条件）</li>
<li><img src="/images/ml/catboost9.png" alt="catboost9.png"></li>
<li>对称树中，每一层的每一个节点判断条件都是一样的。假如如果我们只训练一棵树，那么显然对称树的overfit能力会比普通的决策树弱；但是在GBM中，我们通常训练很多的树，所以overfit的能力不必担心。那么对称树在GBM中有什么优势呢？下面列出三点：</li>
<li>拟合模式相对简单，因为每一层都是一个判断条件</li>
<li>可以提高预测速度</li>
<li>对称树的结构本身比普通决策树自由度小，可以看作是加入了penalty，或者看作regularization</li>
<li>为什么会更快呢？</li>
<li>上面这棵树最底层有四个节点，我们可以把这四个节点从0到3编号（index），每一个节点有一个对应值（value）。比如一个样本最终进入了第一个节点，那么这棵树对这个样本的预测值就是1.6。我们可以把value存放在一个数组里面，这样给定index就可以立刻得到value。那么对一个样本用一棵树进行预测时，我们只需要找到这个样本对应的index，因为在对称树中每一层的判断条件都是一样的，所以每一层都可以用0或者1来表示，比如是学生，用1表示，不是用0；有关注机器会学习用1表示，没有用0，那么每一个样本进入这棵树，都可以用两个bits来表示，也就是四种可能结果：00，01，10，和11。这两个bits代表的数字就是index。这意味着什么呢？我们在做预测的时候，只要对每一层的条件进行判断，然后就可以找到index。因为每一个树的结构（判断条件）是已知的，我们甚至可以对这个过程进行并行计算。</li>
<li>举个栗子</li>
<li>以常见的单棵树为例，depth=6</li>
<li>那么对于二叉树而言最少有 5个判断节点，最多有 2**5+1 个判断节点</li>
<li>对于完全对称二叉树而言，总是5个判断节点，故在infer时速度是明显有提升的</li>
<li>训练时，每个level的node不再各自查找各自的最大gain割，而是直接查找整个level的最大gain割</li>
<li>以常见的单棵树为例，depth=6，feature都是数值型，dataset.<strong>len</strong>() = N</li>
<li>对于完全树而言，对于每一个level每一个data in dataset都需要对每一个feature的候选split点进行计算，所以无论是否是对称，计算量是相差无几的</li>
<li>而常规的cart往往会设置停止分裂条件，不会总是完全树，所以在训练上，完全树是要慢于普通树的</li>
</ul>
</li>
<li>后面的基本不用看了，GPU支持和实验</li>
<li>总体来说，文章主要两个内容</li>
<li>Ordered TS</li>
<li>Ordered Boosting</li>
</ul>
<h2 id="Install-Guide"><a href="#Install-Guide" class="headerlink" title="Install Guide"></a>Install Guide</h2><ul>
<li>xgboost cpu&amp;gpu<ul>
<li>pip install xgboost</li>
</ul>
</li>
<li>catboost cpu&amp;gpu<ul>
<li>pip install catboost</li>
<li>extra 可视化<ul>
<li>pip install ipywidgets ; jupyter nbextension enable —py widgetsnbextension</li>
</ul>
</li>
</ul>
</li>
<li>lightgbm cpu<ul>
<li>pip install lightgbm</li>
<li>mac 安装需要 先 brew install lightgbm or build</li>
</ul>
</li>
<li>lightgbm-gpu<ul>
<li>在Linux上安装需要先安装依赖<ul>
<li>sudo apt install cmake ocl-icd-libopencl1 ocl-icd-opencl-dev libboost-dev libboost-system-dev libboost-filesystem-dev</li>
</ul>
</li>
<li>pip install lightgbm —install-option=–gpu<ul>
<li>cuda得这样<ul>
<li>pip install lightgbm —install-option=—gpu —install-option=”—opencl-include-dir=/usr/local/cuda/include/“ —install-option=”—opencl-library=/usr/local/cuda/lib64/libOpenCL.so”</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>tree</tag>
        <tag>boosting</tag>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>Decision Tree</title>
    <url>/2020/02/23/Decision_Tree/</url>
    <content><![CDATA[<p>梳理 Decision Tree(决策树) 算法原理<br><a id="more"></a><br><!-- toc --></p>
<h2 id="foreword"><a href="#foreword" class="headerlink" title="foreword"></a>foreword</h2><ul>
<li>相信所有学习过数据结构这门课的朋友们都学习过树。我们从ID3开始复习下这玩意</li>
</ul>
<h2 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h2><ul>
<li><img src="/images/ml/ID3_1.png" alt="ID3_1.png"></li>
<li>H(C) 代表C的熵，H(C|D) 代表C在D下的熵， 所以 gain(D) D对于C的信息增益就是 C的信息增益 - C在D发生时的熵</li>
<li>结合例子说明</li>
<li><img src="/images/ml/ID3_2.jpg" alt="ID3_2.png"></li>
<li>以play为root，构建DT，其中 play 的gain(play) 就是如下所示</li>
<li><img src="/images/ml/ID3_3.png" alt="ID3_3.png"></li>
<li><img src="/images/ml/ID3_4.jpg" alt="ID3_4.png"></li>
<li>我们单列所有变量，对其进行分析</li>
<li><img src="/images/ml/ID3_5.png" alt="ID3_5.png"></li>
<li>非常简单明了，其中 gain(天气) = gain(play) - H(天气)，其他也同理。这说明在 outlook temperature humidity windy 四者中，outlook最重要，选取为第一位的节点</li>
<li>由此生成第一节点，分裂出三个分叉，其中overcast的gain为0，低于随意一个阈值，停止分裂，其他两项继续分裂</li>
<li>ID3伪代码：</li>
<li><img src="/images/ml/ID3_6.png" alt="ID3_6.png"></li>
<li>ID3 缺点<ul>
<li>ID3 没有剪枝策略，容易过拟合；</li>
<li>信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于 1；</li>
<li>只能用于处理离散分布的特征；</li>
<li>没有考虑缺失值。</li>
</ul>
</li>
</ul>
<h2 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h2><ol>
<li>可以处理连续数值型属性 <ul>
<li>对于离散值，C4.5和ID3的处理方法相同，对于某个属性的值连续时，假设这这个节点上的数据集合样本为total，C4.5算法进行如下处理： <ul>
<li>将样本数据该属性A上的具体数值按照升序排列，得到属性序列值：{A1,A2,A3,…,Atotal}</li>
<li>在上一步生成的序列值中生成total-1个分割点。第i个分割点的取值为Ai和Ai+1的均值，每个分割点都将属性序列划分为两个子集。</li>
<li>计算每个分割点的信息增益(Information Gain),得到total-1个信息增益。</li>
<li>对分裂点的信息增益进行修正：减去log2(N-1)/|D|，其中N为可能的分裂点个数，D为数据集合大小。</li>
<li>选择修正后的信息增益值最大的分类点作为该属性的最佳分类点</li>
<li>计算最佳分裂点的信息增益率(Gain Ratio)作为该属性的Gain Ratio</li>
<li>选择Gain Ratio最大的属性作为分类属性。</li>
</ul>
</li>
<li>其实就是把连续值问题转化为多分类问题</li>
</ul>
</li>
<li>信息增益率<ul>
<li>C4.5对信息增益添加了罚项，改为了信息增益比。并且能够处理连续特征了。以及相比于ID3，能够处理缺失值了。但是C4.5仍然只能用于分类。C4.5也可以是多叉树。</li>
<li>gain 函数被重新定义为</li>
<li><img src="/images/ml/C4.5.png" alt="C4.5.png"></li>
<li>以 outlook 为例</li>
<li>H(outlook) = -(5/14 <em> log2(5/14) + 4/14 </em> log2(4/14) + 5/14 * log2(5/14)) = 1.5774062828523454</li>
<li>C(outlook) = gain(outlook) / H(outlook) = 0.247 / 1.5774 = 0.156586788</li>
<li>在计算信息增益时，C4.5考虑了变量本身的熵，熵越大的变量得到惩罚越高，从而综合考虑得到了新的选择</li>
<li>同样的可以得到</li>
<li>H(temperature) = -(4/14 <em> log2(4/14) + 6/14 </em> log2(6/14) + 4/14 * log2(4/14)) = 1.5566567074628228</li>
<li>C(temperature) = 0.029 / 1.5567 = 0.018629151</li>
<li>H(humidity) = -(7/14 <em> log2(7/14) + 7/14 </em> log2(7/14)) = 1</li>
<li>C(humidity) = 0.152 / 1 = 0.152</li>
<li>H(windy) = -(8/14 <em> log2(8/14) + 6/14 </em> log2(6/14))  = 0.9852281360342515</li>
<li>C(windy) = 0.048 / 0.9852 = 0.048721072</li>
<li>虽然结果还是选择outlook，但是outlook 和 humidity的差距已经被拉得很近</li>
</ul>
</li>
<li>剪枝<ul>
<li>预剪枝：在节点划分前来确定是否继续增长，及早停止增长的主要方法有：<ul>
<li>节点内数据样本低于某一阈值；</li>
<li>所有节点特征都已分裂；</li>
<li>节点划分前准确率比划分后准确率高。</li>
</ul>
</li>
<li>后剪枝：在已经生成的决策树上进行剪枝，从而得到简化版的剪枝决策树，C4.5是采用后剪枝的方法<ul>
<li>采用的悲观剪枝方法，用递归的方式从低往上针对每一个非叶子节点，评估用一个最佳叶子节点去代替这课子树是否有益</li>
<li>如果剪枝后与剪枝前相比其错误率是保持或者下降，则这棵子树就可以被替换掉</li>
<li>C4.5 通过训练数据集上的错误分类数量来估算未知样本上的错误率。</li>
</ul>
</li>
</ul>
</li>
<li>缺失值处理 <ul>
<li>对于某些采样数据，可能会缺少属性值</li>
<li>在这种情况下，处理缺少属性值的通常做法是赋予该属性的常见值，或者属性均值</li>
<li>另外一种比较好的方法是为该属性的每个可能值赋予一个概率，即将该属性以概率形式赋值<ul>
<li>例如给定Boolean属性B，已知采样数据有12个B=0和88个B=1实例，那么在赋值过程中，B属性的缺失值被赋值为B(0)=0.12、B(1)=0.88；所以属性B的缺失值以12%概率被分到False的分支，以88%概率被分到True的分支。这种处理的目的是计算信息增益，使得这种属性值缺失的样本也能处理。</li>
</ul>
</li>
</ul>
</li>
<li>缺点<ul>
<li>剪枝策略可以再优化；</li>
<li>C4.5 用的是多叉树，用二叉树效率更高；</li>
<li>C4.5 使用的熵模型拥有大量耗时的对数运算，连续值还有排序运算；</li>
<li>C4.5 在构造树的过程中，对数值属性值需要按照其大小进行排序，从中选择一个分割点，所以只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行。</li>
</ul>
</li>
</ol>
<ul>
<li>伪代码<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Function C4.5(R:包含连续属性的无类别属性集合,C:类别属性,S:训练集)    </span><br><span class="line">Begin    </span><br><span class="line">   If S为空,返回一个值为Failure的单个节点;    </span><br><span class="line">   If S是由相同类别属性值的记录组成,    </span><br><span class="line">      返回一个带有该值的单个节点;    </span><br><span class="line">   If R为空,则返回一个单节点,其值为在S的记录中找出的频率最高的类别属性值;    </span><br><span class="line">   [注意未出现错误则意味着是不适合分类的记录]；    </span><br><span class="line">  For 所有的属性R(Ri) Do    </span><br><span class="line">        If 属性Ri为连续属性，则    </span><br><span class="line">        Begin    </span><br><span class="line">           sort(Ri属性值)  </span><br><span class="line">           将Ri的最小值赋给A1：    </span><br><span class="line">             将Ri的最大值赋给Am；    </span><br><span class="line">           For j From 1 To m-1 Do Aj&#x3D;(A1+Aj+1)&#x2F;2;    </span><br><span class="line">           将Ri点的基于Aj(1&lt;&#x3D;j&lt;&#x3D;m-1划分的最大信息增益属性(Ri,S)赋给A；    </span><br><span class="line">        End；    </span><br><span class="line">  将R中属性之间具有最大信息增益的属性(D,S)赋给D;    </span><br><span class="line">  将属性D的值赋给&#123;dj&#x2F;j&#x3D;1,2...m&#125;；    </span><br><span class="line">  将分别由对应于D的值为dj的记录组成的S的子集赋给&#123;sj&#x2F;j&#x3D;1,2...m&#125;;    </span><br><span class="line">  返回一棵树，其根标记为D;树枝标记为d1,d2...dm;    </span><br><span class="line">  再分别构造以下树:    </span><br><span class="line">  C4.5(R-&#123;D&#125;,C,S1),C4.5(R-&#123;D&#125;,C,S2)...C4.5(R-&#123;D&#125;,C,Sm);    </span><br><span class="line">End C4.5</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h2><ul>
<li>以基尼系数为准则选择最优划分属性，可以应用于分类和回归</li>
<li>CART是一棵二叉树，采用二元切分法，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1</li>
<li>相比ID3和C4.5，CART应用要多一些，既可以用于分类也可以用于回归</li>
<li>CART 包含的基本过程有分裂，剪枝和树选择。<ul>
<li>分裂：分裂过程是一个二叉递归划分过程，其输入和预测特征既可以是连续型的也可以是离散型的，CART 没有停止准则，会一直生长下去；</li>
<li>剪枝：采用代价复杂度剪枝，从最大树开始，每次选择训练数据熵对整体性能贡献最小的那个分裂节点作为下一个剪枝对象，直到只剩下根节点。CART 会产生一系列嵌套的剪枝树，需要从中选出一颗最优的决策树；</li>
<li>树选择：用单独的测试集评估每棵剪枝树的预测性能（也可以用交叉验证）。</li>
</ul>
</li>
<li>CART 在 C4.5 的基础上进行了很多提升。<ul>
<li>C4.5 为多叉树，运算速度慢，CART 为二叉树，运算速度快；</li>
<li>C4.5 只能分类，CART 既可以分类也可以回归；</li>
<li>CART 使用 Gini 系数作为变量的不纯度量，减少了大量的对数运算；</li>
<li>CART 采用代理测试来估计缺失值，而 C4.5 以不同概率划分到不同节点中；</li>
<li>CART 采用“基于代价复杂度剪枝”方法进行剪枝，而 C4.5 采用悲观剪枝方法。</li>
</ul>
</li>
</ul>
<ol>
<li>分类树<ul>
<li>CART作为分类树时，特征属性可以是连续类型也可以是离散类型，但观察属性(即标签属性或者分类属性)必须是离散类型。</li>
<li>类似于C4.5，对于连续值，先把连续属性转换为离散属性再进行处理。</li>
<li>另外，对于连续属性先进行排序（升序），只有在决策属性（即分类发生了变化）发生改变的地方才需要切开，这可以显著减少运算量。<ol>
<li>对特征的取值进行升序排序</li>
<li>两个特征取值之间的中点作为可能的分裂点，将数据集分成两部分，计算每个可能的分裂点的GiniGain。优化算法就是只计算分类属性发生改变的那些特征取值</li>
<li>选择GiniGain最小的分裂点作为该特征的最佳分裂点（注意，若修正则此处需对最佳分裂点的Gini Gain减去log2(N-1)/|D|（N是连续特征的取值个数，D是训练数据数目） </li>
</ol>
</li>
<li>必须注意的是：根据离散特征分支划分数据集时，子数据集中不再包含该特征（因为每个分支下的子数据集该特征的取值就会是一样的，信息增益或者Gini Gain将不再变化，这也是C4.5等决策树离散型特征不会被重复选择为节点分裂的属性）；而根据连续特征分支时，各分支下的子数据集必须依旧包含该特征（当然，左右分支各包含的分别是取值小于、大于等于分裂值的子数据集），因为该连续特征再接下来的树分支过程中可能依旧起着决定性作用。</li>
<li>CART分类时，使用基尼指数（Gini）来选择最好的数据分割的特征，gini描述的是纯度，与信息熵的含义相似。CART中每一次迭代都会降低GINI系数。</li>
<li><img src="/images/ml/cart1.png" alt="cart1.png"></li>
<li>对于上题<ul>
<li>G(oulook) = 5/14 <em> (1 - (2/5)<strong>2 - (3/5)</strong>2) + 4/14 </em> (1 - (4/4)<strong>2 - 0</strong>2) + 5/14 <em>(1 - (3/5)<em>*2 - (2/5)</em></em>2) = 0.34285714285714286</li>
<li>G(temperature) = 4/14 <em> (1 - (2/4)<strong>2 - (2/4)</strong>2) + 6/14 </em> (1 - (4/6)<strong>2 - (2/6)</strong>2) + 4/14 <em>(1 - (3/4)<em>*2 - (1/4)</em></em>2) = 0.44047619047619047</li>
<li>G(humidity) = 7/14 <em> (1 - (3/7)<strong>2 - (4/7)</strong>2) + 7/14 </em> (1 - (6/7)<strong>2 - (1/7)</strong>2) = 0.3673469387755103</li>
<li>G(windy) = 8/14 <em> (1 - (6/8)<strong>2 - (2/8)</strong>2) + 6/14 </em> (1 - (3/6)<strong>2 - (3/6)</strong>2) = 0.42857142857142855</li>
<li>仍然选 outlook，但是变量间差异变化较大</li>
</ul>
</li>
</ul>
<ol>
<li>缺失值<ul>
<li>对于缺失值，cart和c4.5的处理方式类似，缺失特征的gini是在非缺失数据上先划分然后根据非缺失值的比例前面乘上相应系数，降低存在缺失值特征的gini然后和其它特征分裂之后的gini增益进行比较。</li>
<li>如果缺失特征恰好是gini增益最大的特征，那么要在有缺失值的特征上分裂就比较麻烦了：</li>
<li>cart使用的方式是：surrogate splits</li>
<li>翻译为中文是代理特征分裂，有两种情况：<ol>
<li>首先，如果某个存在缺失值的特征恰好是当前的分裂增益最大的特征，那么我们需要遍历剩余的特征，剩余的特征中如果有也存在缺失值的特征，那么这些特征忽略，仅仅在完全没有缺失值的特征上进行选择，我们选择其中能够与最佳增益的缺失特征分裂之后增益最接近的特征进行分裂。</li>
<li>如果我们事先设置了一定的标准仅仅选择仅仅选择差异性在一定范围内的特征作为代理特征进行分裂而导致了没有特征和最佳缺失特征的差异性满足要求，或者所有特征都存在缺失值的情况下，缺失样本默认进入个数最大的叶子节点。</li>
</ol>
</li>
</ul>
</li>
<li>剪枝<ul>
<li>CART 剪枝算法从完全生长的决策树的底端剪去一些子树，使决策树变小（模型简单），从而能够对未知数据有更准确的预测。CART剪枝算法由两步组成：首先从生成算法产生的决策树T0T0底端开始不断剪枝，直到T0T0的根节点，形成一个子序列T0,T1,T3,…..TnT0,T1,T3,…..Tn,然后通过交叉验证在独立的验证集上对子树序列进行测试，从中选择最优子树。</li>
<li><img src="/images/ml/cart2.png" alt="cart2.png"></li>
<li>其中，alpha最大只会是T0的depth，对于分类树交叉验证gini，对于回归树一般交叉验证lsd</li>
</ul>
</li>
</ol>
</li>
<li>回归树<ul>
<li>回归树要求观察属性是连续类型，由于节点分裂选择特征属性时通常使用最小绝对偏差（LAD）或者最小二乘偏差（LSD）法，因此通常特征属性也是连续类型。</li>
<li>也就是 mae mse</li>
<li><img src="/images/ml/cart3.png" alt="cart3.png"></li>
<li>以最小绝对偏差（LAD）为例 <ol>
<li>先令最佳方差为无限大bestVar=inf</li>
<li>依次计算根据某特征（FeatureCount次迭代）划分数据后的总方差currentVar（，计算方法为：划分后左右子数据集的总方差之和），如果currentVar</li>
<li>返回最佳分支特征、分支特征值（离散特征则为二分序列、连续特征则为分裂点的值），左右分支子数据集。</li>
</ol>
</li>
<li>举个栗子</li>
<li><img src="/images/ml/cart4.png" alt="cart4.png"></li>
<li>这里是使用了 LSD，也就是MSE作为评估，取代GINI作为分裂标准</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>tree</tag>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>Random Forest</title>
    <url>/2020/02/23/Random_Forest/</url>
    <content><![CDATA[<p>梳理 Random Forest(随机森林) 算法原理<br><a id="more"></a><br><!-- toc --></p>
<h2 id="foreword"><a href="#foreword" class="headerlink" title="foreword"></a>foreword</h2><ul>
<li>随机森林就是通过集成学习的思想将多棵树集成的一种算法，它的基本单元是决策树，而它的本质属于机器学习的一大分支——集成学习（Ensemble Learning）方法。通过组合多棵独立的决策树后根据投票或取均值的方式得到最终预测结果的机器学习方法，往往比单棵树具有更高的准确率和更强的稳定性。随机森林相比于决策树拥有出色的性能主要取决于随机抽取样本和特征和集成算法，前者让它具有更稳定的抗过拟合能力，后者让它有更高的准确率。</li>
</ul>
<h2 id="Bagging思想"><a href="#Bagging思想" class="headerlink" title="Bagging思想"></a>Bagging思想</h2><ul>
<li>Bagging是bootstrap aggregating。思想就是从总体样本当中随机取一部分样本进行训练，通过多次这样的结果，进行投票获取平均值作为结果输出，这就极大可能的避免了不好的样本数据，从而提高准确度。因为有些是不好的样本，相当于噪声，模型学入噪声后会使准确度不高。</li>
<li>bagging有以下几种方式增强鲁棒性<ol>
<li>数据样本扰动 （RF采用）<ul>
<li>给定初始数据集，可从中产生生不同的数据子集，再利用不同的数据子集训练出不同的个体学习器.数据样本扰动是基于采样法，例如Bagging采用自助法采样，，对很多的常见基学习器，例如决策树，神经网络等，训练样本稍加变动就会导致学习器有显著变化，这些学习器称为‘不稳定基学习器’，但是也有稳定学习器，对于样本扰动，这样的学习器往往不会出现太显著的变化，例如线性学习器，除非特别离群的点，一般的话对拟合直线影响不大；支持向量机，与支持向量关系比较大，其他地方的点对学习器影响不大；朴素贝叶斯，在样本分布相对固定时，样本足够多时样本扰动不足以大幅度改变概率；K近邻学习器，主要与K个邻居有关，而与其他位置的点关系不大，这类学习器称为稳定学习器.对于这类学习器集成，往往需要属性扰动等机制.但是值得一提的是，数据样本扰动会改变样本初始的分布，会人为的引入偏差，也会影响基学习器的性能.</li>
</ul>
</li>
<li>输入属性扰动 （RF采用）<ul>
<li>训练集的输入向量X通常不是一维的，这里我们假设输入数据维度是K维，从而选择不同的子属性集提供了观察数据的不同视角，显然从不同属性子空间训练出来的学习器会有所不同，著名的随机子空算法就依赖于属性扰动，该算法从初始属性集中抽出若干个子集，再基于每个属性子集训练一个基学习器.在包含大量属性的情况下，在属性子空间训练个体学习器不仅能产生多样性大的个体，还会因为属性维数减少提高训练效率，但当数据维数较小或数据的属性都有较大重要性时，不宜使用属性扰动.</li>
</ul>
</li>
<li>输出表示扰动<ul>
<li>此类做法的基本思想是对输出表示进行操纵以增强多样性，可对训练样本的类别标记稍作变动，但要注意尺度，如果变动较大，则会人为引入较大偏差，反而得不偿失.例如二分类到多分类问题，MvM中使用的ECOC法，就是通    过改变输出表示，最后采用投票法决定样本类别.</li>
</ul>
</li>
<li>算法参数扰动<ul>
<li>这个和我们常说的调参比较相似，一个基学习器算法都对应着或多或少的超参数，通过对算法参数的调整，往往能够得到不同性能的学习器，例如设置神经网络的隐层神经元数，决策树的深度，支持向量机的带宽width等等. </li>
</ul>
</li>
</ol>
</li>
</ul>
<h2 id="随机森林的基本结构"><a href="#随机森林的基本结构" class="headerlink" title="随机森林的基本结构"></a>随机森林的基本结构</h2><ul>
<li><img src="/images/ml/rf1.jpeg" alt="rf1.jpeg"></li>
<li>常见使用cart作为构建单棵树，对于每一颗子树，输入的数据集和属性都是random的，通过最后的ensemble对结果进行汇总</li>
</ul>
<h2 id="随机森林如何工作"><a href="#随机森林如何工作" class="headerlink" title="随机森林如何工作"></a>随机森林如何工作</h2><ul>
<li><p>随机森林的是通过一个个弱的分类器（决策树）最终组成一个强分类器，那么森林中的每棵树是如何生成的呢？</p>
<ol>
<li><p>假设训练集大小为N，采用Bootstrap sample的方法，对每棵树，随机有放回地从训练集中抽取N个训练样本，作为该棵树的训练集；</p>
<ul>
<li>每棵树的训练集都是不同的，而且里面可能包含重复的训练样本</li>
<li>当通过替换采样绘制当前树的训练集时，大约三分之一的情况被遗漏在样本之外。 随着树木被添加到森林中，该oob（out-of-bag）数据用于获得对分类错误的运行无偏估计。 它还用于获得变量重要性的估计。<ul>
<li>oob并没有用于构建第k棵树。将每个案例放在第k树的第k树的构造中，以获得分类。 以这种方式，在大约三分之一的树中为每种情况获得测试集分类。 在运行结束时，将j作为每次案例n为oob时获得大多数选票的类。 在所有情况下，j不等于n的真实等级的次数的比例是oob误差估计。 事实证明，这在许多测试中都是公正的。</li>
</ul>
</li>
</ul>
</li>
<li><p>假设每个样本的特征维度为M，制定一个常熟m&lt;&lt;M，随机地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的</p>
</li>
<li>每棵树都尽最大程度地生长，不进行剪枝（因为每棵树本身虽然可能拟合很好，但是对整体数据集是几乎不可能过拟合的，不需要剪枝提高鲁棒性）</li>
<li>预测新的样本时，只需要将N棵决策树的分类结果合并输出</li>
</ol>
</li>
</ul>
<h2 id="错误率"><a href="#错误率" class="headerlink" title="错误率"></a>错误率</h2><ul>
<li>在关于随机森林的原始论文中，显示森林错误率取决于两件事：<ul>
<li>森林中任何两棵树之间的相关性。 增加相关性会增加森林错误率。</li>
<li>森林中每棵树的力量(具有低错误率的树是强分类器)。 增加单个树木的强度(分类更精确)会降低森林错误率。</li>
</ul>
</li>
</ul>
<h2 id="重要性评估"><a href="#重要性评估" class="headerlink" title="重要性评估"></a>重要性评估</h2><ul>
<li><p>判断每个特征在随机森林中的每颗树上做了多大的贡献，然后取个平均值，最后比一比特征之间的贡献大小。其中关于贡献的计算方式可以是基尼指数或袋外数据错误率。</p>
</li>
<li><p><img src="/images/ml/rf2.png" alt="rf2.jpeg"></p>
</li>
<li><p><img src="/images/ml/rf3.png" alt="rf3.jpeg"></p>
</li>
<li><p>附sklearn代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line">x, y = df.iloc[:, <span class="number">1</span>:].values, df.iloc[:, <span class="number">0</span>].values</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = <span class="number">0.3</span>, random_state = <span class="number">0</span>)</span><br><span class="line">feat_labels = df.columns[<span class="number">1</span>:]</span><br><span class="line">forest = RandomForestClassifier(n_estimators=<span class="number">10000</span>, random_state=<span class="number">0</span>, n_jobs=<span class="number">-1</span>)</span><br><span class="line">forest.fit(x_train, y_train)</span><br><span class="line">importances = forest.feature_importances_</span><br><span class="line">indices = np.argsort(importances)[::<span class="number">-1</span>]</span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> range(x_train.shape[<span class="number">1</span>]):</span><br><span class="line">    print(<span class="string">"%2d) %-*s %f"</span> % (f + <span class="number">1</span>, <span class="number">30</span>, feat_labels[indices[f]], importances[indices[f]]))</span><br></pre></td></tr></table></figure>
</li>
</ul>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
        <tag>rf</tag>
      </tags>
  </entry>
  <entry>
    <title>Anchor Free Development History</title>
    <url>/2020/02/23/Anchor_Free_Development_History/</url>
    <content><![CDATA[<p>梳理 Anchor Free 的发展史<br><a id="more"></a><br><!-- toc --></p>
<hr>
<h2 id="YOLO-v1"><a href="#YOLO-v1" class="headerlink" title="YOLO v1"></a>YOLO v1</h2><ul>
<li>paper <a href="http://arxiv.org/abs/1506.02640" target="_blank" rel="noopener">You Only Look Once: Unified, Real-Time Object Detection</a> </li>
<li>git <a href="https://github.com/pjreddie/darknet" target="_blank" rel="noopener">https://github.com/pjreddie/darknet</a></li>
<li><img src="/images/detection/yolo1_1.png" alt="yolo1_1.png"></li>
<li><img src="/images/detection/yolo1_2.png" alt="yolo1_2.png"></li>
<li>Each grid cell predicts B bounding boxes and confidence scores for those boxes.</li>
<li>Our system models detection as a regression problem. It divides the image into an S × S grid and for each grid cell predicts B bounding boxes, confidence for those boxes, and C class probabilities. These predictions are encoded as an S × S × (B ∗ 5 + C) tensor.<ul>
<li>For evaluating YOLO on PASCAL VOC, we use S = 7, B = 2. PASCAL VOC has 20 labelled classes so C = 20. Our final prediction is a 7 × 7 × 30 tensor.</li>
</ul>
</li>
<li>loss<ul>
<li><img src="/images/detection/yolo1_3.png" alt="yolo1_3.png"></li>
</ul>
</li>
<li>exp<ul>
<li><img src="/images/detection/yolo1_4.png" alt="yolo1_4.png"></li>
</ul>
</li>
<li>现在看来简单粗暴，在当时可谓是轰动一时，方法是极具开创性的。</li>
<li>FRRCNN的速度不敢恭维，而YOLO v1 几乎是率先达到了DEEP learning 的实时水平，虽然anchor free只是无心插柳，但也确实是anchor free。</li>
<li>但是没有anchor — 召回低，没有mu-level-feature 精度有限，backbone是darknet19，精度低</li>
<li>在这篇文章之后，这个彩虹小马哥吸取 ssd 的anchor及mul-level做法，使得yolo2 与 ssd 一时瑜亮</li>
</ul>
<h2 id="DenseBox"><a href="#DenseBox" class="headerlink" title="DenseBox"></a>DenseBox</h2><ul>
<li>paper <a href="https://arxiv.org/pdf/1509.04874.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1509.04874.pdf</a></li>
<li><img src="/images/detection/densebox1.png" alt="densebox1.png"></li>
<li><img src="/images/detection/densebox2.png" alt="densebox2.png"></li>
<li><img src="/images/detection/densebox3.png" alt="densebox3.png"></li>
<li>这是一篇思路很清晰的文章，看流程图就能明白他做了些什么</li>
<li>在那个VGG还盛行的年代，率先使用了高低level通过upsample方式融合特征出结果，还是用了landmark信息refine检测框</li>
<li>可惜后续没能深挖下去，不然现有anchor free的结构会提前好些时间出来，可能与 FPN 类似的高低层特征融合 及 ssd 这样多level 出框的思路出来有关</li>
</ul>
<h2 id="CornerNet"><a href="#CornerNet" class="headerlink" title="CornerNet"></a>CornerNet</h2><ul>
<li>paper <a href="https://arxiv.org/pdf/1808.01244.pdf" target="_blank" rel="noopener">CornerNet: Detecting Objects as Paired Keypoints</a></li>
<li>git <a href="https://github.com/princeton-vl/CornerNet" target="_blank" rel="noopener">https://github.com/princeton-vl/CornerNet</a></li>
<li><img src="/images/detection/cornernet1.png" alt="cornernet1.png"></li>
<li><img src="/images/detection/cornernet2.png" alt="cornernet2.png"></li>
<li><img src="/images/detection/cornernet3.png" alt="cornernet3.png"></li>
<li>这是一篇用人体关键点类似的思路去做检测的文章</li>
<li>heatmaps: 这个grid是不是要找的 左上|右下 点</li>
<li>embedings: 向量，用于左上右下点匹配</li>
<li>offset: 用于关键点微调</li>
<li>corner pooling: 特殊的maxpooling 用于 进一步凸显 左上|右下 的信息</li>
<li><img src="/images/detection/cornernet4.png" alt="cornernet4.png"></li>
<li>值得一提的文章中类比了两种loss，pull loss用于使CornerPair向量接近更匹配，push loss用于区分不同的CornerPair</li>
<li><img src="/images/detection/cornernet5.png" alt="cornernet5.png"></li>
<li>Anchor Free reborn! 原来没有anchor也可以SOTA! </li>
<li>文章的创新点太多太多，以至于让人感觉非常的繁复，于是后面的人满脑子想的就是，哎？能不能简单点</li>
<li>Hourglass-104 太强了！不过真的慢还很大啊</li>
<li>heatmap 的做法使得 当 classes 很多时计算量很大 又慢了些</li>
<li>还得 左上|右下 各算一个heatmap 又慢了些</li>
<li>还得匹配 左上|右下 点？ 又慢了些</li>
<li>还有额外的为了提升这个设计思路的 pooling？ 又慢了些</li>
<li>总的来说，该文章首次让 anchor free 站上 coco map 40+，researchers 纷纷开始尝试，是不是可以干掉这个该死的anchor</li>
</ul>
<h2 id="ExtremeNet"><a href="#ExtremeNet" class="headerlink" title="ExtremeNet"></a>ExtremeNet</h2><ul>
<li>paper <a href="https://arxiv.org/pdf/1901.08043.pdf" target="_blank" rel="noopener">Bottom-up Object Detection by Grouping Extreme and Center Points</a></li>
<li>git <a href="https://github.com/xingyizhou/ExtremeNet" target="_blank" rel="noopener">https://github.com/xingyizhou/ExtremeNet</a></li>
<li><img src="/images/detection/extremenet1.png" alt="extremenet1.png"></li>
<li>看效果牛逼哄哄，究竟是怎么实现的呢？</li>
<li>We detect four extreme points (top-most, leftmost, bottom-most, right-most) and one center point of objects using a standard keypoint estimation network.</li>
<li>There are no direct extreme point annotation in the COCO. However, there are complete annotations for object segmentation masks. We thus find extreme points as extrema in the polygonal mask annotations. In cases where an edge is parallel to an axis or within a 3 ◦ angle, we place the extreme point at the center of the edge. Although our training data is derived from the more expensive segmentation annotation, the extreme point data itself is 4× cheaper to collect than the standard bounding box.</li>
<li>o(╯□╰)o  竟然是从mask里转换出来的，然后还说我这个标注啊比标准的bbox标注便宜4倍  （黑人问号？？？）</li>
<li>文章的启迪来自于 cvpr 2018 的 <a href="https://arxiv.org/pdf/1711.09081.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1711.09081.pdf</a> Deep Extreme Cut: From Extreme Points to Object Segmentation</li>
<li><img src="/images/detection/extremenet2.png" alt="extremenet2.png"></li>
<li>ExtremeNet的主framework， heatmap 对应各个 classification，offset不对类别分别处置，4 x (x,y) x H x w</li>
<li>那怎么把这些点组装起来呢</li>
<li><img src="/images/detection/extremenet3.png" alt="extremenet3.png"></li>
<li><img src="/images/detection/extremenet4.png" alt="extremenet4.png"></li>
<li>第一步，ExtrectPeak。 即提取heatmap中所有的极值点，极值点定义为在3x3滑动窗口中的极大值。</li>
<li>第二步，暴力枚举。对于每一种极值点组合（进行适当的剪枝以减小遍历规模），计算它们的中心点，如果center map对应位置上的响应超过预设阈值，则将这一组5个点作为一个备选，该备选组合的score为5个对应点的score平均值。</li>
<li>做着做着发现有 ghost box，啥意思呢。</li>
<li>Center grouping may give a high-confidence falsepositive detection for three equally spaced colinear objects of the same size. The center object has two choices here, commit to the correct small box, or predict a much larger box containing the extreme points of its neighbors. We call these false-positive detections “ghost” boxes.</li>
<li>。。。这个不就是你的算法缺陷吗？咋解决呢？</li>
<li>To discourage ghost boxes, we use a form of soft non-maxima suppression [1]. If the sum of scores of all boxes contained in a certain bounding box exceeds 3 times of the score of itself, we divide its score by 2.</li>
<li>堵上。。。真的醉了。</li>
<li>然后回归出来的极值点数值不够大，咋整呢？cornernet不是有个corner pooling 使方向上max一致吗，那搞个edge aggregation吧，把极值点加强一下，尽量就一个极值点</li>
<li>解决办法是，对每一个极值点，向它的两个方向进行聚集。具体做法是，沿着X/Y轴方向，将第一个单调下降区间内的点的score按一定权重累加到原极值点上。效果如下图所示，可以看出，红圈部分的响应明显增强了。</li>
<li><img src="/images/detection/extremenet5.png" alt="extremenet5.png"></li>
<li><img src="/images/detection/extremenet6.png" alt="extremenet6.png"></li>
<li>方法太过繁琐，有没有简单方便的方法呢</li>
</ul>
<h2 id="FSAF"><a href="#FSAF" class="headerlink" title="FSAF"></a>FSAF</h2><ul>
<li>paper <a href="https://arxiv.org/pdf/1903.00621.pdf" target="_blank" rel="noopener">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></li>
<li>Feature Selective Anchor-Free Module for Single-Shot Object Detection 文章title就说明了文章的主体思想，下面两张图也很清晰</li>
<li><img src="/images/detection/fsaf1.png" alt="fsaf1.png"></li>
<li><img src="/images/detection/fsaf2.png" alt="fsaf2.png"></li>
<li>主要的motivation是，不同尺寸的物体依据其与FPN每一层 Anchor 的适配程度，分配到不同分辨率的层上进行学习</li>
<li>two limitations:<br>  1) heuristicguided feature selection;<br>  2) overlap-based anchor sampling. During training, each instance is always matched to the closest anchor box(es) according to IoU overlap. And anchor boxes are associated with a certain level of feature map by human-defined rules, such as box size. Therefore, the selected feature level for each instance is purely based on adhoc heuristics.</li>
<li>这个想法其实是很对的，anchor能iou匹配去搞这些事情，在没有anchor的情况下，我们该如何去分配每一层的目标呢？</li>
<li>那就把anchor堆起来，看看那个匹配了形成真值，然后anchor free的分支依据这个真值去选择用那一层的特征去做最后的检测，原文</li>
<li>Our motivation is to let each instance select the best level of feature freely to optimize the network, so there should be no anchor boxes to constrain the feature selection in our module. Instead, we encode the instances in an anchor-free manner to learn the parameters for classification and regression. The general concept is presented in Figure 3. An anchor-free branch is built per level of feature pyramid, independent to the anchor-based branch. Similar to the anchor-based branch, it consists of a classification subnet and a regression subnet (not shown in figure). An instance can be assigned to arbitrary level of the anchor-free branch. During training, we dynamically select the most suitable level of feature for each instance based on the instance content instead of just the size of instance box. The selected level of feature then learns to detect the assigned instances. At inference, the FSAF module can run independently or jointly with anchorbased branches. Our FSAF module is agnostic to the backbone network and can be applied to single-shot detectors with a structure of feature pyramid. Additionally, the instantiation of anchor-free branches and online feature selection can be various. In this work, we keep the implementation of our FSAF module simple so that its computational cost is marginal compared to the whole network</li>
<li><img src="/images/detection/fsaf3.png" alt="fsaf3.png"></li>
<li><img src="/images/detection/fsaf4.png" alt="fsaf4.png"></li>
<li>那anchor-free的分支是不是在更新参数的时候只更新匹配上的那个level的feature呢？ （Online feature selection mechanism）</li>
<li><img src="/images/detection/fsaf5.png" alt="fsaf5.png"></li>
<li><img src="/images/detection/fsaf6.png" alt="fsaf6.png"></li>
<li>关于 inference，作者说anchor-base分支其实在inference的时候是可以不用的，但是不用的话。其实如table2所示，精度上是和原来的retina差不多的</li>
<li>Inference: The FSAF module just adds a few convolution layers to the fully-convolutional RetinaNet, so the inference is still as simple as forwarding an image through the network. For anchor-free branches, we only decode box predictions from at most 1k top-scoring locations in each pyramid level, after thresholding the confidence scores by 0.05. These top predictions from all levels are merged with the box predictions from anchor-based branches, followed by non-maximum suppression with a threshold of 0.5, yielding the final detections.</li>
<li>文章目前没有官方的开源代码，总体来说结论偏向于，训练时候 anchor-base 和 anchor-free 共存，在inference阶段可以单独使用anchor-free分支或者联合使用</li>
<li>这是首个在inference阶段的anchor-free能到达anchor-base精度的文章，最最重要的一点是，如何让anchor-free能work well，不同level的feature会成为关键</li>
</ul>
<h2 id="FCOS"><a href="#FCOS" class="headerlink" title="FCOS"></a>FCOS</h2><ul>
<li>paper <a href="https://arxiv.org/pdf/1904.01355.pdf" target="_blank" rel="noopener">FCOS: Fully Convolutional One-Stage Object Detection</a></li>
<li>git <a href="https://github.com/tianzhi0549/FCOS" target="_blank" rel="noopener">https://github.com/tianzhi0549/FCOS</a></li>
<li><img src="/images/detection/fcos1.png" alt="fcos1.png"></li>
<li>文章里有提到一个例子，对于anchor-free而言，对于一个grid，很有可能会作为多个 obj 的center，那如何解决这个问题呢？使用mul-level feature</li>
<li><img src="/images/detection/fcos2.png" alt="fcos2.png"></li>
<li>蓝色 + 绿色 是标准的 ssd-fpn 的结构，head 里也是常规的 tower， 额外引入了一个center-loss作为辅助</li>
<li>那如何在无anchor的情况下去正确匹配ground truth呢？如何解决前言的问题呢？</li>
<li>估计是受 FSAF 和 trident RCNN，SNIPER 启发，作者让每一层仅关注对应大小的bbox，让不同 level 的 feature 出 不同大小的 bbox，从而避免了前言里的问题（当然这有一个前提：同样大小的同一类别obj不会拥有同一个center）</li>
<li><img src="/images/detection/fcos3.png" alt="fcos3.png"></li>
<li><img src="/images/detection/fcos4.png" alt="fcos4.png"></li>
<li><img src="/images/detection/fcos5.png" alt="fcos5.png"></li>
<li>这个center-ness的思路是很好的。对于一个ground truth，可能grid不足以完全匹配其center，周边会有好几个近邻的grid都匹配上了。那如何去筛选呢？当然是选最近的。</li>
<li>作者在插入 center-ness 分支的时候也做了实验，这是一个由中心距算出来的loss是不是应该插在 regression 分支呢？ 作者做了实验，发现插 regression 分支还不如不插。</li>
<li>我个人的理解是，虽然这是一个和距离相关的loss，但其实仍然是一个分类为目的的loss，用于区分其是否为中心点，与最后分类的分值相辅相成，所以还是加在 分类 的分支里</li>
<li><img src="/images/detection/fcos6.png" alt="fcos6.png"></li>
<li><img src="/images/detection/fcos7.png" alt="fcos7.png"></li>
<li>最后，作者还就FCOS作为RPN网络进行了分析，实验证明，FCOS不仅recall更高而且在高IOU threshold下效果依然好过retinanet</li>
<li>值得推荐的好方法</li>
<li>值得一提的是社区给作者提供了许多改进点，速度不变的情况下提升了1.*%的COCO map，具体可以看git repo</li>
</ul>
<h2 id="CenterNet-op"><a href="#CenterNet-op" class="headerlink" title="CenterNet-op"></a>CenterNet-op</h2><ul>
<li>paper <a href="https://arxiv.org/abs/1904.07850" target="_blank" rel="noopener">Objects as Points</a></li>
<li>git <a href="https://github.com/xingyizhou/CenterNet" target="_blank" rel="noopener">https://github.com/xingyizhou/CenterNet</a></li>
<li><img src="/images/detection/centernetop1.png" alt="centernetop1.png"></li>
<li>上来就是拳打RetinaNet, 脚踢YOLO3</li>
<li>CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1% AP at 142 FPS, 37.4% AP at 52 FPS, and 45.1% AP with multi-scale testing at 1.4 FPS. </li>
<li>看看他究竟是怎么做的</li>
<li>首先假设输入图像为I（W <em> H </em> 3），其中W和H分别为图像的宽和高，然后在预测的时候，我们要产生出关键点的热点图(keypoint heatmap)：Y（W/R <em> H/R </em> C）<ul>
<li>其中R为输出对应原图的步长，而C是在目标检测中对应着检测点的数量，如在COCO目标检测任务中，这个C的值为80，代表当前有80个类别。</li>
</ul>
</li>
<li>We use the default output stride of R = 4 in literature [4,40,42]. The output stride downsamples the output prediction by a factor R. A prediction Yˆ x,y,c = 1 corresponds to a detected keypoint, while Yˆ x,y,c = 0 is background.</li>
<li><img src="/images/detection/centernetop2.png" alt="centernetop2.png"></li>
<li><img src="/images/detection/centernetop3.png" alt="centernetop3.png"></li>
<li><img src="/images/detection/centernetop4.png" alt="centernetop4.png"></li>
<li>说的很明白。</li>
<li>h = H // 4  w = W // 4</li>
<li>hw: n<em>c</em>h<em>w  wh: n</em>2<em>h</em>w  reg: n<em>2</em>h*w</li>
<li>对于每一个类分别生成heatmap，将gt keypoints以高斯核的形式结合进去形成easy-hard mining类似的heatmap</li>
<li>使用 l1 loss 作为regression的loss</li>
<li>后处理 对hw使用 sigmoid + maxpooling 进行nms，取代了传统的nms模块，说是很efficient 在我看来也只是对于 anchor很多情况下 nms慢的问题，anchor少的时候优势不大</li>
<li>附源代码<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_nms</span><span class="params">(heat, kernel=<span class="number">3</span>)</span>:</span></span><br><span class="line">    pad = (kernel - <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line"> hmax = nn.functional.max_pool2d(</span><br><span class="line">        heat, (kernel, kernel), stride=<span class="number">1</span>, padding=pad)</span><br><span class="line">    keep = (hmax == heat).float()</span><br><span class="line">    x = heat * keep</span><br><span class="line"> <span class="keyword">return</span> x</span><br><span class="line">hm = output[<span class="string">'hm'</span>].sigmoid_()</span><br><span class="line">heat = _nms(heat)</span><br></pre></td></tr></table></figure></li>
<li><img src="/images/detection/centernetop5.png" alt="centernetop5.png"></li>
<li>同样是简单有效的方案</li>
<li>对backbone有很大的依赖，当backbone能力下降时（不适用dcn）效果下降迅速，不适合端化</li>
<li>当分辨率提高时计算量增幅大</li>
<li>当中心点重合时，由于单grid只回归一个框，故会出现意料之外的问题</li>
</ul>
<h2 id="CenterNet-kt"><a href="#CenterNet-kt" class="headerlink" title="CenterNet-kt"></a>CenterNet-kt</h2><ul>
<li>paper <a href="https://arxiv.org/abs/1904.08189" target="_blank" rel="noopener">CenterNet: Keypoint Triplets for Object Detection</a></li>
<li>git <a href="https://github.com/Duankaiwen/CenterNet" target="_blank" rel="noopener">https://github.com/Duankaiwen/CenterNet</a></li>
<li><img src="/images/detection/centernetkt1.png" alt="centernetkt1.png"></li>
<li>CornerNet 换个思路版本</li>
<li>本文利用关键点三元组即中心点、左上角点和右下角点三个关键点而不是两个点来确定一个目标，使网络花费了很小的代价便具备了感知物体内部信息的能力，从而能有效抑制误检。另外，为了更好的检测中心点和角点，我们分别提出了 center pooling 和 cascade corner pooling 来提取中心点和角点的特征。 啥意思呢？CornerPair对当且仅当其中心位置附近包含Center时才算是真的是个bbox</li>
<li>先从特征对齐角度分析一波motivation</li>
<li>为啥2阶段好呢？有roi pooling，分类的结果和bbox的信息是对齐的；但凡是anchor-based的one-stage det方案，anchor和特征就是对不齐的，CornerNet虽然没有anchor也有存在这样的问题</li>
<li>咋整呢？那我是不是可以验证一下这个框到底对不对</li>
<li><img src="/images/detection/centernetkt2.png" alt="centernetkt2.png"></li>
<li><img src="/images/detection/centernetkt3.png" alt="centernetkt3.png"></li>
<li>作者把自己的方法叫做 Object Detection as Keypoint Triplets</li>
<li>通过 center pooling 和 cascade corner pooling 分别得到 center heatmap 和 corner heatmaps，用来预测关键点的位置。得到角点的位置和类别后，通过 offsets 将角点的位置映射到输入图片的对应位置，然后通过 embedings 判断哪两个角点属于同一个物体，以便组成一个检测框。相似于CornerNet的Pooling，本文也有一个CenterPooling</li>
<li><img src="/images/detection/centernetkt4.png" alt="centernetkt4.png"></li>
<li><img src="/images/detection/centernetkt5.png" alt="centernetkt5.png"></li>
<li>其他的就和CornerNet一样了，就不重复叙述了，上结果</li>
<li><img src="/images/detection/centernetkt6.png" alt="centernetkt6.png"></li>
<li>结果还是很强势的，吊打了在座的各位 One-Stage Det，但是由于是CornerNet加重版，inference的速度还是有点慢的 （且仅仅511*511 分辨率）</li>
<li>With an average inference time of 270ms using a 52-layer hourglass backbone [29] and 340ms using a 104-layer hourglass backbone [29] per image, CenterNet is quite efficient yet closely matches the state-of-the-art performance of the other twostage detectors.</li>
<li>很强大，相当于CornerNet上再加了一层是否包含Center的判断，就是太慢了</li>
</ul>
<h2 id="CornerNet-Lite"><a href="#CornerNet-Lite" class="headerlink" title="CornerNet-Lite"></a>CornerNet-Lite</h2><ul>
<li>paper <a href="https://arxiv.org/abs/1904.08900" target="_blank" rel="noopener">CornerNet-Lite: Efficient Keypoint Based Object Detection</a></li>
<li>git <a href="https://github.com/princeton-vl/CornerNet-Lite" target="_blank" rel="noopener">https://github.com/princeton-vl/CornerNet-Lite</a></li>
<li><img src="/images/detection/cornernetlite1.png" alt="cornernetlite1.png"></li>
<li>上来就是拳打前作cornernet，脚踢正在风头的YOLO3，那么问题来了。这上面的inference time竟然是在7700k+1080ti上的时间。。。（令人害怕）</li>
<li>恩，我觉得我们这个conrnet是有点慢啊，那要不减少点计算的像素量不改变结构或者是干脆减轻结构？</li>
<li><img src="/images/detection/cornernetlite2.png" alt="cornernetlite2.png"></li>
<li>saccade的overview。全图直接检测好烦，计算量太大了。我先down size，然后生成类似与trident的三个level的attention maps，映射到原图剪出来，然后走正常的cornernet流程。。。</li>
<li>值得一提的是。。作者说这样做相较于CornerNet原生减少了6x的时间而且仅提升了1%的accuracy。</li>
<li>这一整套的出发点和处理技巧和SPIPER和相似。说实在的。。这个结构说他 two-stage 一点也不冤枉啊。。</li>
<li>接下来看 squeeze，核心思路是 well-known 的depth wise conv，和预期的相似，cornernet-squeeze 变快了，精度也下降了</li>
<li><img src="/images/detection/cornernetlite3.png" alt="cornernetlite3.png"></li>
<li><img src="/images/detection/cornernetlite4.png" alt="cornernetlite4.png"></li>
<li>最后，作者提出了 cornernet-saccade的几个亮点：1-训练占用内存少2-attention效果拔群3-用了Hourglass-54精度还上升了！</li>
<li><img src="/images/detection/cornernetlite5.png" alt="cornernetlite5.png"></li>
<li><img src="/images/detection/cornernetlite6.png" alt="cornernetlite6.png"></li>
<li><img src="/images/detection/cornernetlite7.png" alt="cornernetlite7.png"></li>
<li>实验做的好，说什么都对。不过话说回来，对于尺寸较小的图片，深的网络确实不一定比稍微浅一些的网络强。经过gt-attention裁剪原图后图片基本变小了</li>
<li>值得一提的是，corner-saccade 的 APs 高的吓人，但此处并非coco-test的结果，下面的才是。 24.4也是当前one-stage中 APs 非常高的存在了</li>
<li><img src="/images/detection/cornernetlite8.png" alt="cornernetlite8.png"></li>
<li>（伪）one-stage 当前最高精度可能性的存在</li>
</ul>
<h2 id="RepPoints"><a href="#RepPoints" class="headerlink" title="RepPoints"></a>RepPoints</h2><ul>
<li>paper <a href="https://arxiv.org/abs/1904.11490" target="_blank" rel="noopener">RepPoints: Point Set Representation for Object Detection</a></li>
<li>git <a href="https://github.com/microsoft/RepPoints" target="_blank" rel="noopener">https://github.com/microsoft/RepPoints</a></li>
<li><img src="/images/detection/reppoints1.png" alt="reppoints1.png"></li>
<li>干净利落的 refineDet 加强版？</li>
<li>any FPN branch feature – 1st deconv (point loss 1) – 2st deconv (point loss 2) + normal class pred</li>
<li>这个deconv怎么用来算bbox呢</li>
<li><img src="/images/detection/reppoints2.png" alt="reppoints2.png"></li>
<li><img src="/images/detection/reppoints3.png" alt="reppoints3.png"></li>
<li><img src="/images/detection/reppoints4.png" alt="reppoints4.png"></li>
<li>使用 1st DCONV  的 OFFSET_MAP 作为回归的 pseudo BOX（生成‘anchor’），然后对其进行分类，微调</li>
<li><img src="/images/detection/reppoints5.png" alt="reppoints5.png"></li>
<li>文章思路新颖，颇有开辟新流派的趋势</li>
</ul>
]]></content>
      <categories>
        <category>cv</category>
      </categories>
      <tags>
        <tag>anchor_free</tag>
        <tag>detection</tag>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>Segmentation Survey</title>
    <url>/2020/02/23/Segmentation_Survey/</url>
    <content><![CDATA[<p>梳理 Segmentation(分割) 相关介绍，数据集，算法<br><a id="more"></a><br><!-- toc --></p>
<hr>
<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><hr>
<h2 id="Foreword"><a href="#Foreword" class="headerlink" title="Foreword"></a>Foreword</h2><ul>
<li>相信时至今日，分割作为三大基础任务，是被广为熟知的</li>
<li>值得一提的是，分割任务往往被认为有两类<ul>
<li>语义分割 semantic segmentation<ul>
<li>语义分割只区分类别不区分个体</li>
<li>代表作的有 FCN Deeplab 系列</li>
</ul>
</li>
<li>实例分割 instance segmentation<ul>
<li>实例分割不仅区分类别，对同类别的不同个体也需要区分</li>
<li>代表作有 Mask-RCNN </li>
</ul>
</li>
</ul>
</li>
<li>（借用 gluoncv的一张图来说明）</li>
<li><img src="/images/segmentation/Intro1.png" alt="Intro1.png"></li>
</ul>
<hr>
<h1 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h1><hr>
<h2 id="Semantic-Segmentation"><a href="#Semantic-Segmentation" class="headerlink" title="Semantic Segmentation"></a>Semantic Segmentation</h2><h3 id="Unet"><a href="#Unet" class="headerlink" title="Unet"></a>Unet</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1505.04597.pdf" target="_blank" rel="noopener">U-Net: Convolutional Networks for Biomedical Image Segmentation</a></li>
<li>git <a href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/" target="_blank" rel="noopener">https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/</a><ul>
<li>当然 git 上非官方的也是一抓一大把</li>
</ul>
</li>
<li><img src="/images/segmentation/UNET1.png" alt="UNET1.png"></li>
<li>非常经典的encode-decode结构，其特点是在decode时高级特征会不断与低级特征融合，对图像本身的结构和语义保护地很好，有效地使局部特征得到充分表现</li>
<li><img src="/images/segmentation/UNET2.png" alt="UNET2.png"></li>
</ul>
<h3 id="Segnet"><a href="#Segnet" class="headerlink" title="Segnet"></a>Segnet</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1511.00561.pdf" target="_blank" rel="noopener">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</a></li>
<li>code <a href="http://mi.eng.cam.ac.uk/projects/segnet/" target="_blank" rel="noopener">http://mi.eng.cam.ac.uk/projects/segnet/</a></li>
<li><img src="/images/segmentation/segnet1.png" alt="segnet1.png"></li>
<li>和Unet很相似，文章里提到的不同点也是有点牵强。。<ul>
<li>As compared to SegNet, U-Net [16] (proposed for the medical imaging community) does not reuse pooling indices but instead transfers the entire feature map (at the cost of more memory) to the corresponding decoders and concatenates them to upsampled (via deconvolution) decoder feature maps. There is no conv5 and max-pool 5 block in U-Net as in the VGG net architecture. SegNet, on the other hand, uses all of the pre-trained convolutional layer weights from VGG net as pre-trained weights.</li>
</ul>
</li>
</ul>
<h3 id="Deeplab-v1-amp-v2"><a href="#Deeplab-v1-amp-v2" class="headerlink" title="Deeplab v1 &amp; v2"></a>Deeplab v1 &amp; v2</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1606.00915.pdf" target="_blank" rel="noopener">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a></li>
<li>home <a href="http://liangchiehchen.com/projects/DeepLab.html" target="_blank" rel="noopener">http://liangchiehchen.com/projects/DeepLab.html</a></li>
<li><img src="/images/segmentation/deeplabv1&amp;2_1.png" alt="deeplabv1&amp;2_1.png"></li>
<li>在那个年代多阶段还被普遍认为优于end2end，DCNN extract Feature， upsample， FC CRF得到结果。这是deeplab v1的主体思想<ul>
<li>CRFs have been broadly used in semantic segmentation to combine class scores computed by multi-way classifiers with the lowlevel information captured by the local interactions of pixels and edges [23], [24] or superpixels [25]. Even though works of increased sophistication have been proposed to model the hierarchical dependency [26], [27], [28] and/or highorder dependencies of segments [29], [30], [31], [32], [33], we use the fully connected pairwise CRF proposed by [22] for its efficient computation, and ability to capture fine edge details while also catering for long range dependencies.</li>
</ul>
</li>
<li><img src="/images/segmentation/deeplabv1&amp;2_2.png" alt="deeplabv1&amp;2_2.png"></li>
<li>本文最大的贡献之一是带动了Astro conv在seg任务上的发展</li>
<li><img src="/images/segmentation/deeplabv1&amp;2_3.png" alt="deeplabv1&amp;2_3.png"></li>
<li>有ASPP的版本成为deeplab v2也是同样的出色，多rate的astro conv拼凑而成</li>
<li><img src="/images/segmentation/deeplabv1&amp;2_4.png" alt="deeplabv1&amp;2_4.png"></li>
<li>通过CRF的持续迭代，边缘信息变得充分，分割结果更加完美</li>
<li><img src="/images/segmentation/deeplabv1&amp;2_5.png" alt="deeplabv1&amp;2_5.png"></li>
<li>FOV相当于是单通道的ASPP，实验证明，large rate的aspp效果有显著提高</li>
<li><img src="/images/segmentation/deeplabv1&amp;2_6.png" alt="deeplabv1&amp;2_6.png"></li>
<li>说什么也要sota一下</li>
<li><img src="/images/segmentation/deeplabv1&amp;2_7.png" alt="deeplabv1&amp;2_7.png"></li>
<li>有趣的是，ASPP在 PASCAL-Person-Part 上表现不佳，不如不用</li>
<li><img src="/images/segmentation/deeplabv1&amp;2_8.png" alt="deeplabv1&amp;2_8.png"></li>
<li>Cityscapes 上ASPP管用</li>
</ul>
<h3 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1605.06211.pdf" target="_blank" rel="noopener">Fully Convolutional Networks for Semantic Segmentation</a></li>
<li>git <a href="https://github.com/shelhamer/fcn.berkeleyvision.org" target="_blank" rel="noopener">https://github.com/shelhamer/fcn.berkeleyvision.org</a></li>
<li><img src="/images/segmentation/FCN1.png" alt="FCN1.png"></li>
<li>非常简洁直接，打开seg新世界大门</li>
<li><img src="/images/segmentation/FCN2.png" alt="FCN2.png"></li>
<li>同时，FCN也开启了多level特征融合的大门</li>
<li><img src="/images/segmentation/FCN3.png" alt="FCN3.png"></li>
<li>32s 16s 8s 表示 downsample 2**（5 4 3） 的结果，肉眼可见的更好了</li>
<li><img src="/images/segmentation/FCN4.png" alt="FCN4.png"></li>
<li>结构简洁 结果不错 速度快 还要啥自行车</li>
</ul>
<h3 id="Enet"><a href="#Enet" class="headerlink" title="Enet"></a>Enet</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1606.02147.pdf" target="_blank" rel="noopener">ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation</a></li>
<li><img src="/images/segmentation/enet1.png" alt="enet1.png"></li>
<li>开始意识到maxpooling在初级阶段对input的伤害，使用一个3x3 stride2缓解信息丢失；bottleneck 的设计估计是参考了 res，略有一些不同，使用了prelu</li>
<li><img src="/images/segmentation/enet2.png" alt="enet2.png"></li>
<li>主体结构，在stage2 stage3 都有递增的dilated conv<ul>
<li>asymmetric <ul>
<li>Sometimes we replace it with asymmetric convolution i.e. a sequence of 5 × 1 and 1 × 5 convolutions isntead of 5 x 5. </li>
</ul>
</li>
</ul>
</li>
<li><img src="/images/segmentation/enet3.png" alt="enet3.png"></li>
<li>展示了相对segnet的速度优势</li>
<li><img src="/images/segmentation/enet4.png" alt="enet4.png"></li>
<li>cityscapes 略逊 segnet</li>
</ul>
<h3 id="PSP"><a href="#PSP" class="headerlink" title="PSP"></a>PSP</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1612.01105.pdf,https://hszhao.github.io/projects/pspnet/" target="_blank" rel="noopener">Pyramid Scene Parsing Network</a></li>
<li>semantic seg 扛把子</li>
<li><img src="/images/segmentation/psp1.png" alt="psp1.png"></li>
<li>直接上代码，非常容易理解<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_PSP1x1Conv</span><span class="params">(in_channels, out_channels, norm_layer, norm_kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">        norm_layer(out_channels, **(&#123;&#125; <span class="keyword">if</span> norm_kwargs <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> norm_kwargs)),</span><br><span class="line">        nn.ReLU(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_PyramidPooling</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, **kwargs)</span>:</span></span><br><span class="line">        super(_PyramidPooling, self).__init__()</span><br><span class="line">        out_channels = int(in_channels / <span class="number">4</span>)</span><br><span class="line">        self.avgpool1 = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.avgpool2 = nn.AdaptiveAvgPool2d(<span class="number">2</span>)</span><br><span class="line">        self.avgpool3 = nn.AdaptiveAvgPool2d(<span class="number">3</span>)</span><br><span class="line">        self.avgpool4 = nn.AdaptiveAvgPool2d(<span class="number">6</span>)</span><br><span class="line">        self.conv1 = _PSP1x1Conv(in_channels, out_channels, **kwargs)</span><br><span class="line">        self.conv2 = _PSP1x1Conv(in_channels, out_channels, **kwargs)</span><br><span class="line">        self.conv3 = _PSP1x1Conv(in_channels, out_channels, **kwargs)</span><br><span class="line">        self.conv4 = _PSP1x1Conv(in_channels, out_channels, **kwargs)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        size = x.size()[<span class="number">2</span>:]</span><br><span class="line">        feat1 = F.interpolate(self.conv1(self.avgpool1(x)), size, mode=<span class="string">'bilinear'</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">        feat2 = F.interpolate(self.conv2(self.avgpool2(x)), size, mode=<span class="string">'bilinear'</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">        feat3 = F.interpolate(self.conv3(self.avgpool3(x)), size, mode=<span class="string">'bilinear'</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">        feat4 = F.interpolate(self.conv4(self.avgpool4(x)), size, mode=<span class="string">'bilinear'</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.cat([x, feat1, feat2, feat3, feat4], dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li>
<li>就是avgpooling到不同的尺度获取不同粒度的特征信息</li>
<li><img src="/images/segmentation/psp2.png" alt="psp2.png"></li>
<li>aux loss 是一个由stage4出来的辅助loss，接的haed是FCN的head（没有psp模块）</li>
<li><img src="/images/segmentation/psp3.png" alt="psp3.png"></li>
<li>作者在Ablation Study中对这个aux loss进行了分析，这玩意就是好使啊怎么都加点</li>
<li><img src="/images/segmentation/psp4.png" alt="psp4.png"></li>
<li>展示了PSP的渐进加点方法在ImageNet scene parsing challenge 2016上</li>
<li><img src="/images/segmentation/psp5.png" alt="psp5.png"></li>
<li>PSP效果堪称惊艳</li>
<li><img src="/images/segmentation/psp6.png" alt="psp6.png"></li>
<li>Cityscapes 当然也不会放过</li>
</ul>
<h3 id="ICNet"><a href="#ICNet" class="headerlink" title="ICNet"></a>ICNet</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1704.08545.pdf" target="_blank" rel="noopener">ICNet for Real-Time Semantic Segmentation on High-Resolution Images</a></li>
<li>git <a href="https://github.com/hszhao/ICNet" target="_blank" rel="noopener">https://github.com/hszhao/ICNet</a></li>
<li><img src="/images/segmentation/icnet1.png" alt="icnet1.png"></li>
<li>虽然PSP很不错，但是不够快，ICNET致力于 acc 和 speed的均衡</li>
<li><img src="/images/segmentation/icnet2.png" alt="icnet2.png"></li>
<li>通过三个不同大小的input三个不同的子网络的特征进行融合，得到最终结果</li>
<li><img src="/images/segmentation/icnet3.png" alt="icnet3.png"></li>
<li>CFF的细节</li>
<li><img src="/images/segmentation/icnet4.png" alt="icnet4.png"></li>
<li>列出了几种 semantic segmentation 的常见的structure。我对他说的 ours(d) 是有疑义的，他其实每一个子网络都有output</li>
<li><img src="/images/segmentation/icnet5.png" alt="icnet5.png"></li>
<li>CityScapes 结果，yolo策略，快的没我好，好的没我快</li>
<li><img src="/images/segmentation/icnet6.png" alt="icnet6.png"></li>
<li>在CamVid和CoCoStuff上也是一样的不错</li>
</ul>
<h3 id="DenseASPP"><a href="#DenseASPP" class="headerlink" title="DenseASPP"></a>DenseASPP</h3><ul>
<li>paper <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_DenseASPP_for_Semantic_CVPR_2018_paper.pdf" target="_blank" rel="noopener">DenseASPP for Semantic Segmentation in Street Scenes</a></li>
<li><img src="/images/segmentation/denseaspp1.png" alt="denseaspp1.png"></li>
<li>受启发于 densenet，给aspp也dense上</li>
<li><img src="/images/segmentation/denseaspp2.png" alt="denseaspp2.png"></li>
</ul>
<h3 id="Deeplab-v3"><a href="#Deeplab-v3" class="headerlink" title="Deeplab v3"></a>Deeplab v3</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1706.05587.pdf" target="_blank" rel="noopener">Rethinking Atrous Convolution for Semantic Image Segmentation</a></li>
<li>主要就是改进了ASPP</li>
<li><img src="/images/segmentation/deeplabv3_1.png" alt="deeplabv3_1.png"></li>
<li>在ASPP上加了一个image pooling，并在block4上使用了rate=2的dilate conv</li>
<li><img src="/images/segmentation/deeplabv3_2.png" alt="deeplabv3_2.png"></li>
<li>相较PSP有少量提升，JFT指pretrained on JFT dataset</li>
<li><img src="/images/segmentation/deeplabv3_3.png" alt="deeplabv3_3.png"></li>
<li>CityScapes上也是如此</li>
</ul>
<h3 id="Deeplab-v3-1"><a href="#Deeplab-v3-1" class="headerlink" title="Deeplab v3+"></a>Deeplab v3+</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1802.02611.pdf" target="_blank" rel="noopener">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</a></li>
<li>git <a href="https://github.com/tensorflow/models/tree/master/research/deeplab" target="_blank" rel="noopener">https://github.com/tensorflow/models/tree/master/research/deeplab</a></li>
<li><img src="/images/segmentation/deeplabv3+_1.png" alt="deeplabv3+_1.png"></li>
<li>a是deeplab v3的结构，b是常见的encode-decode结构，相结合成c成为Deeplab v3</li>
<li><img src="/images/segmentation/deeplabv3+_2.png" alt="deeplabv3+_2.png"></li>
<li>结构简单清晰</li>
<li><img src="/images/segmentation/deeplabv3+_3.png" alt="deeplabv3+_3.png"></li>
<li>顺路玄学设计一把 Xception</li>
<li><img src="/images/segmentation/deeplabv3+_4.png" alt="deeplabv3+_4.png"></li>
<li>得益于backbone和高低阶特征融合，进步很大</li>
</ul>
<h3 id="EncNet"><a href="#EncNet" class="headerlink" title="EncNet"></a>EncNet</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1803.08904v1.pdf" target="_blank" rel="noopener">Context Encoding for Semantic Segmentation</a></li>
<li><img src="/images/segmentation/encnet1.png" alt="encnet1.png"></li>
<li>对最后的结果重新分配权重，并使用一个新的se loss用来监督监督在这张图中某一个类存在的概率</li>
<li><img src="/images/segmentation/encnet2.png" alt="encnet2.png"></li>
<li>相较于早它一个月发布的 deeplab v3+，这个结果实在不够看了</li>
</ul>
<h3 id="BiSeNet"><a href="#BiSeNet" class="headerlink" title="BiSeNet"></a>BiSeNet</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1808.00897.pdf" target="_blank" rel="noopener">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li>
<li>Megvii出品</li>
<li><img src="/images/segmentation/bisenet1.png" alt="bisenet1.png"></li>
<li>这个arm就是去除了缩放channel俩conv的se，ffm是一个经典的attention结构，这里成为是特征融合模块</li>
<li><img src="/images/segmentation/bisenet2.png" alt="bisenet2.png"></li>
<li>对比之前的性价比之王 ICNet，同精度下速度提升3倍；大幅提升精度下速度也翻倍</li>
<li><img src="/images/segmentation/bisenet3.png" alt="bisenet3.png"></li>
<li>上大模型精度也是顶尖的</li>
</ul>
<h3 id="DANet"><a href="#DANet" class="headerlink" title="DANet"></a>DANet</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1809.02983.pdf" target="_blank" rel="noopener">Dual Attention Network for Scene Segmentation</a></li>
<li>git <a href="https://github.com/junfu1115/DANet/" target="_blank" rel="noopener">https://github.com/junfu1115/DANet/</a></li>
<li><img src="/images/segmentation/danet1.png" alt="danet1.png"></li>
<li>空间的attention 和 channel的attention 融合</li>
<li><img src="/images/segmentation/danet2.png" alt="danet2.png"></li>
<li>具体attention的做法，spatial的做法还算正常，channel的做法略有些诡异。。</li>
<li>直接上代码<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_PositionAttentionModule</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">""" Position attention module"""</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, **kwargs)</span>:</span></span><br><span class="line">        super(_PositionAttentionModule, self).__init__()</span><br><span class="line">        self.conv_b = nn.Conv2d(in_channels, in_channels // <span class="number">8</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv_c = nn.Conv2d(in_channels, in_channels // <span class="number">8</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv_d = nn.Conv2d(in_channels, in_channels, <span class="number">1</span>)</span><br><span class="line">        self.alpha = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line">        self.softmax = nn.Softmax(dim=<span class="number">-1</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        batch_size, _, height, width = x.size()</span><br><span class="line">        feat_b = self.conv_b(x).view(batch_size, <span class="number">-1</span>, height * width).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        feat_c = self.conv_c(x).view(batch_size, <span class="number">-1</span>, height * width)</span><br><span class="line">        attention_s = self.softmax(torch.bmm(feat_b, feat_c))  <span class="comment"># conv transpose .* conv</span></span><br><span class="line">        feat_d = self.conv_d(x).view(batch_size, <span class="number">-1</span>, height * width)</span><br><span class="line">        feat_e = torch.bmm(feat_d, attention_s.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)).view(batch_size, <span class="number">-1</span>, height, width)  <span class="comment"># conv .* conv transpose )reshape</span></span><br><span class="line">        out = self.alpha * feat_e + x</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_ChannelAttentionModule</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Channel attention module"""</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(_ChannelAttentionModule, self).__init__()</span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line">        self.softmax = nn.Softmax(dim=<span class="number">-1</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        batch_size, _, height, width = x.size()</span><br><span class="line">        feat_a = x.view(batch_size, <span class="number">-1</span>, height * width)</span><br><span class="line">        feat_a_transpose = x.view(batch_size, <span class="number">-1</span>, height * width).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        attention = torch.bmm(feat_a, feat_a_transpose) <span class="comment"># x = x * x transpose</span></span><br><span class="line">        attention_new = torch.max(attention, dim=<span class="number">-1</span>, keepdim=<span class="literal">True</span>)[<span class="number">0</span>].expand_as(attention) - attention <span class="comment"># x.max - x</span></span><br><span class="line">        attention = self.softmax(attention_new)</span><br><span class="line"> </span><br><span class="line">        feat_e = torch.bmm(attention, feat_a).view(batch_size, <span class="number">-1</span>, height, width)</span><br><span class="line">        out = self.beta * feat_e + x</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure></li>
<li><img src="/images/segmentation/danet3.png" alt="danet3.png"></li>
<li>cityscapes略强于deeplab v3的水平</li>
<li><img src="/images/segmentation/danet4.png" alt="danet4.png"></li>
<li>VOC也是PSP差不多水平</li>
</ul>
<h3 id="CGNet"><a href="#CGNet" class="headerlink" title="CGNet"></a>CGNet</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1811.08201.pdf" target="_blank" rel="noopener">CGNet: A Light-weight Context Guided Network for Semantic Segmentation</a></li>
<li>git <a href="https://github.com/wutianyiRosun/CGNet" target="_blank" rel="noopener">https://github.com/wutianyiRosun/CGNet</a></li>
<li><img src="/images/segmentation/cgnet1.png" alt="cgnet1.png"></li>
<li>展示了文章的主思路。a是FCN，b是FC+context module，例如psp aspp，咱这个每个阶段都给上context Feature</li>
<li><img src="/images/segmentation/cgnet2.png" alt="cgnet2.png"></li>
<li>名字花里胡哨取一堆，就是3x3 conv和3x3 dilate=3 conv concat，然后se</li>
<li><img src="/images/segmentation/cgnet3.png" alt="cgnet3.png"></li>
<li>对module中的residual的位置也做了区分</li>
<li><img src="/images/segmentation/cgnet4.png" alt="cgnet4.png"></li>
<li>对标对象是 ENet ESPNet，同样参数下实现了精度 5% - 10%的进步</li>
</ul>
<h3 id="OCNet"><a href="#OCNet" class="headerlink" title="OCNet"></a>OCNet</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1809.00916.pdf" target="_blank" rel="noopener">OCNet: Object Context Network for Scene Parsing</a></li>
<li>git <a href="https://github.com/PkuRainBow/OCNet.pytorch" target="_blank" rel="noopener">https://github.com/PkuRainBow/OCNet.pytorch</a></li>
<li>微软出品</li>
<li><img src="/images/segmentation/ocnet1.png" alt="ocnet1.png"></li>
<li>展示了几种OC架构，paper中并没有画出OC的基本结构，直接上代码 </li>
<li>OCM → BaseOC_Module             OCP → BaseOC_Context_Module<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_SelfAttentionBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    The basic implementation for self-attention block/non-local block</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        N X C X H X W</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        in_channels       : the dimension of the input feature map</span></span><br><span class="line"><span class="string">        key_channels      : the dimension after the key/query transform</span></span><br><span class="line"><span class="string">        value_channels    : the dimension after the value transform</span></span><br><span class="line"><span class="string">        scale             : choose the scale to downsample the input feature maps (save memory cost)</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        N X C X H X W</span></span><br><span class="line"><span class="string">        position-aware context features.(w/o concate or add with the input)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, key_channels, value_channels, out_channels=None, scale=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(_SelfAttentionBlock, self).__init__()</span><br><span class="line">        self.scale = scale</span><br><span class="line">        self.in_channels = in_channels</span><br><span class="line">        self.out_channels = out_channels</span><br><span class="line">        self.key_channels = key_channels</span><br><span class="line">        self.value_channels = value_channels</span><br><span class="line">        <span class="keyword">if</span> out_channels == <span class="literal">None</span>:</span><br><span class="line">            self.out_channels = in_channels</span><br><span class="line">        self.pool = nn.MaxPool2d(kernel_size=(scale, scale))</span><br><span class="line">        self.f_key = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,</span><br><span class="line">                kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</span><br><span class="line">            InPlaceABNSync(self.key_channels),</span><br><span class="line">        )</span><br><span class="line">        self.f_query = self.f_key</span><br><span class="line">        self.f_value = nn.Conv2d(in_channels=self.in_channels, out_channels=self.value_channels,</span><br><span class="line">            kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.W = nn.Conv2d(in_channels=self.value_channels, out_channels=self.out_channels,</span><br><span class="line">            kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        nn.init.constant(self.W.weight, <span class="number">0</span>)</span><br><span class="line">        nn.init.constant(self.W.bias, <span class="number">0</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        batch_size, h, w = x.size(<span class="number">0</span>), x.size(<span class="number">2</span>), x.size(<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">if</span> self.scale &gt; <span class="number">1</span>:</span><br><span class="line">            x = self.pool(x)</span><br><span class="line"> </span><br><span class="line">        value = self.f_value(x).view(batch_size, self.value_channels, <span class="number">-1</span>)</span><br><span class="line">        value = value.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        query = self.f_query(x).view(batch_size, self.key_channels, <span class="number">-1</span>)</span><br><span class="line">        query = query.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        key = self.f_key(x).view(batch_size, self.key_channels, <span class="number">-1</span>)</span><br><span class="line"> </span><br><span class="line">        sim_map = torch.matmul(query, key)</span><br><span class="line">        sim_map = (self.key_channels**<span class="number">-.5</span>) * sim_map</span><br><span class="line">        sim_map = F.softmax(sim_map, dim=<span class="number">-1</span>)</span><br><span class="line"> </span><br><span class="line">        context = torch.matmul(sim_map, value)</span><br><span class="line">        context = context.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">        context = context.view(batch_size, self.value_channels, *x.size()[<span class="number">2</span>:])</span><br><span class="line">        context = self.W(context)</span><br><span class="line">        <span class="keyword">if</span> self.scale &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">if</span> torch_ver == <span class="string">'0.4'</span>:</span><br><span class="line">                context = F.upsample(input=context, size=(h, w), mode=<span class="string">'bilinear'</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">elif</span> torch_ver == <span class="string">'0.3'</span>:</span><br><span class="line">                context = F.upsample(input=context, size=(h, w), mode=<span class="string">'bilinear'</span>)</span><br><span class="line">        <span class="keyword">return</span> context</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SelfAttentionBlock2D</span><span class="params">(_SelfAttentionBlock)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, key_channels, value_channels, out_channels=None, scale=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(SelfAttentionBlock2D, self).__init__(in_channels,</span><br><span class="line">                                                    key_channels,</span><br><span class="line">                                                    value_channels,</span><br><span class="line">                                                    out_channels,</span><br><span class="line">                                                    scale)</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseOC_Module</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the BaseOC module</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        in_features / out_features: the channels of the input / output feature maps.</span></span><br><span class="line"><span class="string">        dropout: we choose 0.05 as the default value.</span></span><br><span class="line"><span class="string">        size: you can apply multiple sizes. Here we only use one size.</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        features fused with Object context information.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels, key_channels, value_channels, dropout, sizes=<span class="params">([<span class="number">1</span>])</span>)</span>:</span></span><br><span class="line">        super(BaseOC_Module, self).__init__()</span><br><span class="line">        self.stages = []</span><br><span class="line">        self.stages = nn.ModuleList([self._make_stage(in_channels, out_channels, key_channels, value_channels, size) <span class="keyword">for</span> size <span class="keyword">in</span> sizes])       </span><br><span class="line">        self.conv_bn_dropout = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">2</span>*in_channels, out_channels, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>),</span><br><span class="line">            InPlaceABNSync(out_channels),</span><br><span class="line">            nn.Dropout2d(dropout)</span><br><span class="line">            )</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_stage</span><span class="params">(self, in_channels, output_channels, key_channels, value_channels, size)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> SelfAttentionBlock2D(in_channels,</span><br><span class="line">                                    key_channels,</span><br><span class="line">                                    value_channels,</span><br><span class="line">                                    output_channels,</span><br><span class="line">                                    size)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, feats)</span>:</span></span><br><span class="line">        priors = [stage(feats) <span class="keyword">for</span> stage <span class="keyword">in</span> self.stages]</span><br><span class="line">        context = priors[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(priors)):</span><br><span class="line">            context += priors[i]</span><br><span class="line">        output = self.conv_bn_dropout(torch.cat([context, feats], <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseOC_Context_Module</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Output only the context features.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        in_features / out_features: the channels of the input / output feature maps.</span></span><br><span class="line"><span class="string">        dropout: specify the dropout ratio</span></span><br><span class="line"><span class="string">        fusion: We provide two different fusion method, "concat" or "add"</span></span><br><span class="line"><span class="string">        size: we find that directly learn the attention weights on even 1/8 feature maps is hard.</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        features after "concat" or "add"</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels, key_channels, value_channels, dropout, sizes=<span class="params">([<span class="number">1</span>])</span>)</span>:</span></span><br><span class="line">        super(BaseOC_Context_Module, self).__init__()</span><br><span class="line">        self.stages = []</span><br><span class="line">        self.stages = nn.ModuleList([self._make_stage(in_channels, out_channels, key_channels, value_channels, size) <span class="keyword">for</span> size <span class="keyword">in</span> sizes])</span><br><span class="line">        self.conv_bn_dropout = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>),</span><br><span class="line">            InPlaceABNSync(out_channels),</span><br><span class="line">            )</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_stage</span><span class="params">(self, in_channels, output_channels, key_channels, value_channels, size)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> SelfAttentionBlock2D(in_channels,</span><br><span class="line">                                    key_channels,</span><br><span class="line">                                    value_channels,</span><br><span class="line">                                    output_channels,</span><br><span class="line">                                    size)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, feats)</span>:</span></span><br><span class="line">        priors = [stage(feats) <span class="keyword">for</span> stage <span class="keyword">in</span> self.stages]</span><br><span class="line">        context = priors[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(priors)):</span><br><span class="line">            context += priors[i]</span><br><span class="line">        output = self.conv_bn_dropout(context)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure></li>
<li>可以得出结论，所谓OC module就是常见的spatial attention的改装</li>
<li><img src="/images/segmentation/ocnet2.png" alt="ocnet2.png"></li>
<li>可以看出ASPP的结构加上OC还是有不少提升的</li>
<li><img src="/images/segmentation/ocnet3.png" alt="ocnet3.png"></li>
<li>尽管改动简单，但是效果拔群</li>
<li><img src="/images/segmentation/ocnet4.png" alt="ocnet4.png"></li>
<li>在LIP上也做了实验，效果也很好</li>
</ul>
<h3 id="DUNet"><a href="#DUNet" class="headerlink" title="DUNet"></a>DUNet</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1903.02120.pdf" target="_blank" rel="noopener">Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li>
<li><img src="/images/segmentation/dunet1.png" alt="dunet1.png"></li>
<li>整体架构是个常规操作，看看DUpsample怎么玩</li>
<li><img src="/images/segmentation/dunet2.png" alt="dunet2.png"></li>
<li>我觉得这个图不是很直白，直接上代码</li>
<li>整体就是 conv  c → c<em>factor</em>factor，然后reshape reshape<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DUpsampling</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""DUsampling module"""</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels, scale_factor=<span class="number">2</span>, **kwargs)</span>:</span></span><br><span class="line">        super(DUpsampling, self).__init__()</span><br><span class="line">        self.scale_factor = scale_factor</span><br><span class="line">        self.conv_w = nn.Conv2d(in_channels, out_channels * scale_factor * scale_factor, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv_w(x)</span><br><span class="line">        n, c, h, w = x.size()</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># N, C, H, W --&gt; N, W, H, C</span></span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>).contiguous()</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># N, W, H, C --&gt; N, W, H * scale, C // scale</span></span><br><span class="line">        x = x.view(n, w, h * self.scale_factor, c // self.scale_factor)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># N, W, H * scale, C // scale --&gt; N, H * scale, W, C // scale</span></span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># N, H * scale, W, C // scale --&gt; N, H * scale, W * scale, C // (scale ** 2)</span></span><br><span class="line">        x = x.view(n, h * self.scale_factor, w * self.scale_factor, c // (self.scale_factor * self.scale_factor))</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># N, H * scale, W * scale, C // (scale ** 2) -- &gt; N, C // (scale ** 2), H * scale, W * scale</span></span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></li>
<li><img src="/images/segmentation/dunet3.png" alt="dunet3.png"></li>
<li>在voc 上进行对比，dusample相对bilinear upsample确有其优势</li>
<li><img src="/images/segmentation/dunet4.png" alt="dunet4.png"></li>
<li>这个方法在deeplab v3+上一样有效，提升了0.3%</li>
</ul>
<h3 id="fastFCN"><a href="#fastFCN" class="headerlink" title="fastFCN"></a>fastFCN</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1903.11816.pdf" target="_blank" rel="noopener">FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation</a></li>
<li>git <a href="https://github.come/wuhuikai/FastFCN" target="_blank" rel="noopener">https://github.come/wuhuikai/FastFCN</a></li>
<li><img src="/images/segmentation/fastfcn1.png" alt="fastfcn1.png"></li>
<li>标准的结构，主要在JPU</li>
<li><img src="/images/segmentation/fastfcn2.png" alt="fastfcn2.png"></li>
<li>上采样到8X，使用多dilate进行conv，cat conv 得到结果。。。岂不是要在8x上aspp？？？那fast在哪呢</li>
<li><img src="/images/segmentation/fastfcn3.png" alt="fastfcn3.png"></li>
<li>效果略有提升</li>
<li><img src="/images/segmentation/fastfcn4.png" alt="fastfcn4.png"></li>
<li>配合EncNet效果不错</li>
</ul>
<h3 id="LEDNET"><a href="#LEDNET" class="headerlink" title="LEDNET"></a>LEDNET</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1905.02423.pdf" target="_blank" rel="noopener">LEDNET: A LIGHTWEIGHT ENCODER-DECODER NETWORK FOR REAL-TIME SEMANTIC SEGMENTATION</a></li>
<li><img src="/images/segmentation/lednet1.png" alt="lednet1.png"></li>
<li>主要的骚操作在decode部分</li>
</ul>
<h3 id="Fast-SCNN"><a href="#Fast-SCNN" class="headerlink" title="Fast-SCNN"></a>Fast-SCNN</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1902.04502.pdf" target="_blank" rel="noopener">Fast-SCNN: Fast Semantic Segmentation Network</a></li>
<li><img src="/images/segmentation/fastscnn1.png" alt="fastscnn1.png"></li>
<li>感觉就是deeplab v3+去掉aspp</li>
</ul>
<h3 id="HRNet"><a href="#HRNet" class="headerlink" title="HRNet"></a>HRNet</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1904.04514.pdf" target="_blank" rel="noopener">High-Resolution Representations for Labeling Pixels and Regions</a></li>
<li>git <a href="https://github.com/HRNet" target="_blank" rel="noopener">https://github.com/HRNet</a></li>
<li><img src="/images/segmentation/hrnet1.png" alt="hrnet1.png"></li>
<li><img src="/images/segmentation/hrnet2.png" alt="hrnet2.png"></li>
<li>HRNet在seg上确实是unstoppable，结构大家也都是很了解了</li>
<li><img src="/images/segmentation/hrnet3.png" alt="hrnet3.png"></li>
<li><img src="/images/segmentation/hrnet4.png" alt="hrnet4.png"></li>
<li>在LIP上，没有extra监督信息的情况下达到了55.9，相当高</li>
</ul>
<h3 id="DFANet"><a href="#DFANet" class="headerlink" title="DFANet"></a>DFANet</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1904.02216.pdf" target="_blank" rel="noopener">DFANet: Deep Feature Aggregation for Real-Time Semantic Segmentation</a></li>
<li><img src="/images/segmentation/dfanet1.png" alt="dfanet1.png"></li>
<li>列举了不同的structure</li>
<li><img src="/images/segmentation/dfanet2.png" alt="dfanet2.png"></li>
<li>看起来就像是三个重复的网络在concat，和HRNet神似</li>
<li><img src="/images/segmentation/dfanet3.png" alt="dfanet3.png"></li>
<li>优势在于网络架整体深度的降低带来的计算的快速，在100fps的场景是最好的选择</li>
</ul>
<h3 id="OCRNet"><a href="#OCRNet" class="headerlink" title="OCRNet"></a>OCRNet</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1909.11065.pdf" target="_blank" rel="noopener">Object-Contextual Representations for Semantic Segmentation</a></li>
<li><img src="/images/segmentation/ocrnet1.png" alt="ocrnet1.png"></li>
<li>目前没有源代码，对其中的具体操作还有待商榷</li>
<li><img src="/images/segmentation/ocrnet2.png" alt="ocrnet2.png"></li>
<li><img src="/images/segmentation/ocrnet3.png" alt="ocrnet3.png"></li>
<li>效果令人震惊，就坐等爸爸开源了</li>
<li>It can be seen that our approach (HRNetV2 + OCR) achieves very competitive performance w/o using the video information or depth information. We then combine our OCR with ASPP [4] by replacing the global average pooling with our OCR, which (HRNetV2 + OCR (w/ ASP)) achieves 1st on 1 metric and 2nd on 3 of the 4 metrics with only a single model.</li>
<li>还说到了，如果和ASPP一起使用，将GAP换成OCR即可效果拔群</li>
</ul>
<h2 id="Instance-Segmentation"><a href="#Instance-Segmentation" class="headerlink" title="Instance Segmentation"></a>Instance Segmentation</h2><p>相信说，instance seg 的deep风潮是从mask-rcnn开始的</p>
<h3 id="Mask-RCNN"><a href="#Mask-RCNN" class="headerlink" title="Mask-RCNN"></a>Mask-RCNN</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1703.06870.pdf" target="_blank" rel="noopener">Mask R-CNN</a></li>
<li>git <ul>
<li><a href="https://github.com/facebookresearch/Detectron" target="_blank" rel="noopener">https://github.com/facebookresearch/Detectron</a></li>
<li><a href="https://github.com/facebookresearch/maskrcnn-benchmark" target="_blank" rel="noopener">https://github.com/facebookresearch/maskrcnn-benchmark</a></li>
<li><a href="https://github.com/facebookresearch/Detectron2" target="_blank" rel="noopener">https://github.com/facebookresearch/Detectron2</a></li>
<li>facebook出了3个repo主打都是maskrcnn，真-大哥</li>
</ul>
</li>
<li><img src="/images/segmentation/maskrcnn1.png" alt="maskrcnn1.png"></li>
<li>主体架构图</li>
<li><img src="/images/segmentation/maskrcnn2.png" alt="maskrcnn2.png"></li>
<li>将RoiPooling改进为RoiAlign，从单纯的max到bilinear interpolation</li>
<li><img src="/images/segmentation/maskrcnn3.png" alt="maskrcnn3.png"></li>
<li>展示了面对FRRCNN和FRRCNN w/FPN时略有不同的head设计</li>
<li><img src="/images/segmentation/maskrcnn4.png" alt="maskrcnn4.png"></li>
<li>相较之前的结果进步巨大</li>
<li><img src="/images/segmentation/maskrcnn5.png" alt="maskrcnn5.png"></li>
<li>与此同时，就单纯拿检测效果对比，mask-RCNN的效果也明显的好过FRRCNN在使用同一个backbone的情况下</li>
</ul>
<h3 id="PANet"><a href="#PANet" class="headerlink" title="PANet"></a>PANet</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1803.01534.pdf" target="_blank" rel="noopener">Path Aggregation Network for Instance Segmentation</a></li>
<li>git <a href="https://github.com/ShuLiu1993/PANet" target="_blank" rel="noopener">https://github.com/ShuLiu1993/PANet</a></li>
<li>panet 也是重量级的，荣誉很多<ul>
<li>CVPR 2018 Spotlight paper</li>
<li>1st place of <a href="http://cocodataset.org/#detections-leaderboard" target="_blank" rel="noopener">COCO Instance Segmentation Challenge 2017</a> </li>
<li>2nd place of <a href="http://cocodataset.org/#detections-leaderboard" target="_blank" rel="noopener">COCO Detection Challenge 2017</a> </li>
<li>1st place of 2018 <a href="http://cvit.iiit.ac.in/scene-understanding-challenge-2018/benchmarks.php#instance" target="_blank" rel="noopener">Scene Understanding Challenge for Autonomous Navigation in Unstructured Environments</a></li>
<li>相信只需要有其中一个荣誉就是重量级的了</li>
</ul>
</li>
<li><img src="/images/segmentation/panet1.png" alt="panet1.png"></li>
<li>a阶段是FPN加上一条红线，代表底层特征与高层特征的pw add；b阶段将特征再次组装，绿线作用和a阶段红线相似；c阶段进行roi pooling，将结果fuse得到结果</li>
<li><img src="/images/segmentation/panet2.png" alt="panet2.png"></li>
<li>分拆来看，这是b阶段的细节，基本就是FPN中的upsample改为stride 2 kernel 3x3的conv</li>
<li>Note that N2 is simply P2, without any processing.</li>
<li><img src="/images/segmentation/panet3.png" alt="panet3.png"></li>
<li>值得一提的是，对于多阶段的检测算法而言，各自level的roi pooling是独立进行的，但是这个图上ROI proposal都是对齐的，其实中间还有一步对齐的op，对齐后将对应的特征融合</li>
<li><a href="https://github.com/ShuLiu1993/PANet/blob/master/lib/modeling/collect_and_distribute_fpn_rpn_proposals.py" target="_blank" rel="noopener">https://github.com/ShuLiu1993/PANet/blob/master/lib/modeling/collect_and_distribute_fpn_rpn_proposals.py</a></li>
<li>“””Merge RPN proposals generated at multiple FPN levels and then distribute those proposals to their appropriate FPN levels. An anchor at one FPN level may predict an RoI that will map to another level, hence the need to redistribute the proposals.“”“</li>
<li>源代码中有这样一段专门用来做这个事情，简单说就是将所有的proposal给其他level都复制一份，来达到对齐</li>
<li><img src="/images/segmentation/panet4.png" alt="panet4.png"></li>
<li>在mask分支作者也是使用了 conv 与 fc 结合的策略提高seg的精度</li>
<li><img src="/images/segmentation/panet5.png" alt="panet5.png"></li>
<li>吊打了Mask-RCNN w/FPN，但其实也伴随着肉眼可见的计算量增加</li>
<li><img src="/images/segmentation/panet6.png" alt="panet6.png"></li>
<li>在detection上也不遑多让，也是吊打</li>
<li><img src="/images/segmentation/panet7.png" alt="panet7.png"></li>
<li>同时，作者在 Ablation Studies 中也复现了mask rcnn，并使用训练技巧使其涨点4.4</li>
<li><img src="/images/segmentation/panet8.png" alt="panet8.png"></li>
<li>分享了COCO第一的方法，看得出来，这几个方法都涨点很猛</li>
<li>总结：这是一篇干货满满的文章，值得一看</li>
</ul>
<h3 id="MS-R-CNN"><a href="#MS-R-CNN" class="headerlink" title="MS R-CNN"></a>MS R-CNN</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1903.00241.pdf" target="_blank" rel="noopener">Mask Scoring R-CNN</a></li>
<li>git <a href="https://github.com/zjhuang22/maskscoring_rcnn" target="_blank" rel="noopener">https://github.com/zjhuang22/maskscoring_rcnn</a></li>
<li><img src="/images/segmentation/msrcnn1.png" alt="msrcnn1.png"></li>
<li>The RCNN head and Mask head are standard components of Mask R-CNN. 新的mask iou分支用于预测各个类别的iou分值</li>
<li><img src="/images/segmentation/msrcnn2.png" alt="msrcnn2.png"></li>
<li><img src="/images/segmentation/msrcnn3.png" alt="msrcnn3.png"></li>
<li>这样简单的操作就开始快乐涨点了，在几乎所有情况下都有效，还要啥自行车</li>
</ul>
<h3 id="yolact"><a href="#yolact" class="headerlink" title="yolact"></a>yolact</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1904.02689.pdf" target="_blank" rel="noopener">YOLACT Real-time Instance Segmentation</a></li>
<li>git <a href="https://github.com/dbolya/yolact" target="_blank" rel="noopener">https://github.com/dbolya/yolact</a></li>
<li><img src="/images/segmentation/yolact1.png" alt="yolact1.png"></li>
<li>based on retinanet. predict出bbox cls mask，经过nms，然后和protonet的结果结合得到instance的mask</li>
<li><img src="/images/segmentation/yolact2.png" alt="yolact2.png"></li>
<li>haed略有不同，share了tower减少计算和参数，多计算了一份mask分支，mask分支的dim k是由config设置的<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> cfg.mask_type == mask_type.direct:</span><br><span class="line">    cfg.mask_dim = cfg.mask_size（<span class="number">16</span>）**<span class="number">2</span></span><br><span class="line"><span class="keyword">elif</span> cfg.mask_type == mask_type.lincomb:</span><br><span class="line">    cfg.mask_dim = num_grids + num_features</span><br></pre></td></tr></table></figure></li>
<li>loss: Since each pixel can be assigned to more than one class, we use sigmoid and c channels instead of softmax and c + 1. This loss is given a weight of 1 and results in a +0.4 mAP boost.</li>
<li><img src="/images/segmentation/yolact3.png" alt="yolact3.png"></li>
<li>整体上看，精度相同的情况下速度上完全吊打了FCIS，是个realtime 不错的选择</li>
</ul>
<h3 id="PolarMask"><a href="#PolarMask" class="headerlink" title="PolarMask"></a>PolarMask</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1909.13226.pdf" target="_blank" rel="noopener">PolarMask: Single Shot Instance Segmentation with Polar Representation</a></li>
<li>git <a href="https://github.com/xieenze/PolarMask" target="_blank" rel="noopener">https://github.com/xieenze/PolarMask</a></li>
<li><img src="/images/segmentation/polarmask1.png" alt="polarmask1.png"></li>
<li>展示了笛卡尔系建模和极坐标系建模的detail</li>
<li><img src="/images/segmentation/polarmask2.png" alt="polarmask2.png"></li>
<li>文章将FCOS进行了拓展，将bbox视为polar系下的4等分角度多边形，将mask视为polar系下的无线等分的多边形</li>
<li><img src="/images/segmentation/polarmask3.png" alt="polarmask3.png"></li>
<li>换个方式算centerness，后面有实验证明这个centerness的优势</li>
<li><img src="/images/segmentation/polarmask4.png" alt="polarmask4.png"></li>
<li>既然采用了polar系，iou的计算方式也要有所改变，虽然这个式是积分式，其实现实里是离散化成n等分的</li>
<li><img src="/images/segmentation/polarmask5.png" alt="polarmask5.png"></li>
<li>(左 → 右 上 → 下)<ul>
<li>rays 代表切割的份数，实验证明切36份就差不多了</li>
<li>对比了smooth-l1 和 polar iou loss，不用iou loss真滴不行</li>
<li>使用polar centerness比卡迪尔中心更好</li>
<li>box branch 有没有无所谓</li>
<li>backbone还是越牛逼越好</li>
<li>scale 当然也是越大越好</li>
</ul>
</li>
<li><img src="/images/segmentation/polarmask6.png" alt="polarmask6.png"></li>
<li>在仅训练12epochs w/o aug的情况下达到了30+的coco mask map</li>
</ul>
<h3 id="CenterMask"><a href="#CenterMask" class="headerlink" title="CenterMask"></a>CenterMask</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1911.06667.pdf" target="_blank" rel="noopener">CenterMask:Real-Time Anchor-Free Instance Segmentation</a></li>
<li><img src="/images/segmentation/centermask1.png" alt="centermask1.png"></li>
<li>amazing！在速度和精度上均超过了mask rcnn，而且在实时模型的pk中也大幅胜过yolact，来看看他是怎么做的</li>
<li><img src="/images/segmentation/centermask2.png" alt="centermask2.png"></li>
<li>整体架构：使用FCOS作为类似RPN网络用于出bbox和cls，然后对每一个Bbox经过sag mask来抑制pixel层面的noise来完成mask<ul>
<li>图上没有，但是作者提到了 Adaptive RoI Assignment Function 用于自适应多level， 将不同大小的box标准化到一个大小</li>
<li>sam是一个 pooling + sig + elewise-mul 的 spatial attention guided mask（就是个空间attention）</li>
</ul>
</li>
<li><img src="/images/segmentation/centermask3.png" alt="centermask3.png"></li>
<li>文中一大亮点是，提出了 VoVNet v2，提高了vovnet的性能，主要的改动在 OSA module 上</li>
<li>Residual connection： 上图b</li>
<li>eSE： 上图c 主要就是gap然后fc，elewise-mul （ECANet也是如此，叫法不同）</li>
<li><img src="/images/segmentation/centermask4.png" alt="centermask4.png"></li>
<li>作者以FCOS0-R50为例，展示了将其改造为centermask的过程和时间消耗增加其中 mask scoring就是前面提到亮点ms rcnn的miou loss分支，时间消耗每图多15ms，是可接受范围</li>
<li><img src="/images/segmentation/centermask5.png" alt="centermask5.png"></li>
<li>展示了VoVNetV2中改进的两点的效果，在时间消耗小幅增加的的前提下，精度得到了很不错的trade off</li>
<li><img src="/images/segmentation/centermask6.png" alt="centermask6.png"></li>
<li>与现在主流的backbone: resnet resnext hrnet 做了对比，在同等精度下（或相对高一些的精度），VoVNet在CenterMask上都能在GPU上跑得更快</li>
<li><img src="/images/segmentation/centermask7.png" alt="centermask7.png"></li>
<li>对比现在realtime的instance mask架构，CenterMask在同等速度下精度都能有较大提升</li>
<li>非常值得推荐的方法</li>
</ul>
<hr>
<h1 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h1><hr>
<h2 id="Semantic-Segmentation-1"><a href="#Semantic-Segmentation-1" class="headerlink" title="Semantic Segmentation"></a>Semantic Segmentation</h2><h3 id="Pascal-VOC-2012"><a href="#Pascal-VOC-2012" class="headerlink" title="Pascal VOC 2012"></a>Pascal VOC 2012</h3><ul>
<li>home <a href="http://host.robots.ox.ac.uk:8080/pascal/VOC/voc2012/index.html" target="_blank" rel="noopener">http://host.robots.ox.ac.uk:8080/pascal/VOC/voc2012/index.html</a></li>
<li>download <a href="http://host.robots.ox.ac.uk:8080/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar" target="_blank" rel="noopener">http://host.robots.ox.ac.uk:8080/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar</a></li>
<li>20 classes. The 2012 dataset contains images from 2008-2011 for which additional segmentations have been prepared. As in previous years the assignment to training/test sets has been maintained. The total number of images with segmentation has been increased from 7,062 to 9,993.</li>
<li>VOCAug <ul>
<li>VOCAug 也是非常常用的数据集，由VOC变种而来</li>
<li>11355 train 2857 val</li>
</ul>
</li>
</ul>
<h3 id="Cityscapes"><a href="#Cityscapes" class="headerlink" title="Cityscapes"></a>Cityscapes</h3><ul>
<li>home <a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener">https://www.cityscapes-dataset.com/</a></li>
<li>download home上可以找到，需要注册账号下载</li>
<li>baiduyun <a href="https://pan.baidu.com/s/1w3W_dQBUiHcwkLOtbSJ1Tg" target="_blank" rel="noopener">https://pan.baidu.com/s/1w3W_dQBUiHcwkLOtbSJ1Tg</a>   1bln</li>
<li>30 classes. We present a new large-scale dataset that contains a diverse set of stereo video sequences recorded in street scenes from 50 different cities, with high quality pixel-level annotations of 5 000 frames in addition to a larger set of 20 000 weakly annotated frames. The dataset is thus an order of magnitude larger than similar previous attempts. Details on annotated classes and examples of our annotations are available at this webpage.</li>
</ul>
<h3 id="ADE20K"><a href="#ADE20K" class="headerlink" title="ADE20K"></a>ADE20K</h3><ul>
<li>home <a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/" target="_blank" rel="noopener">http://groups.csail.mit.edu/vision/datasets/ADE20K/</a></li>
<li>download <a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/ADE20K_2016_07_26.zip" target="_blank" rel="noopener">http://groups.csail.mit.edu/vision/datasets/ADE20K/ADE20K_2016_07_26.zip</a></li>
<li>train 20210 val 2000 test. 类别是开放的，目前至少有250个类</li>
</ul>
<h2 id="Instance-Segmentation-1"><a href="#Instance-Segmentation-1" class="headerlink" title="Instance Segmentation"></a>Instance Segmentation</h2><h3 id="COCO-17"><a href="#COCO-17" class="headerlink" title="COCO 17"></a>COCO 17</h3><ul>
<li>home <a href="http://cocodataset.org/#download" target="_blank" rel="noopener">http://cocodataset.org/#download</a> | <a href="http://cocodataset.org/#stuff-2017" target="_blank" rel="noopener">http://cocodataset.org/#stuff-2017</a></li>
<li>download <a href="http://cocodataset.org/#download" target="_blank" rel="noopener">http://cocodataset.org/#download</a><ul>
<li>需要下载的文件较多，看到的都下载就行</li>
</ul>
</li>
<li>The task includes 55K COCO images (train 40K, val 5K, test-dev 5K, test-challenge 5K) with annotations for 91 stuff classes and 1 ‘other’ class. The stuff annotations cover 38M superpixels (10B pixels) with 296K stuff regions (5.4 stuff labels per image). Annotations for train and val are now available for download, while test set annotations will remain private.</li>
</ul>
]]></content>
      <categories>
        <category>cv</category>
      </categories>
      <tags>
        <tag>cv</tag>
        <tag>seg</tag>
        <tag>semantic_seg</tag>
        <tag>instance_seg</tag>
      </tags>
  </entry>
  <entry>
    <title>Pedestrian Detection Survey</title>
    <url>/2020/02/22/Pedestrian_Detection_Survey/</url>
    <content><![CDATA[<p>梳理 Pedestrian Detection(行人检测) 相关介绍，数据集，算法<br><a id="more"></a><br><!-- toc --></p>
<hr>
<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><hr>
<h2 id="Foreword"><a href="#Foreword" class="headerlink" title="Foreword"></a>Foreword</h2><ul>
<li>和通用检测不同，行人检测顾名思义，是只有行人一类的，特殊的前景类检测，其难点在于：<ul>
<li>occlusion<ul>
<li>行人检测往往是安防视角或者自动驾驶视角，行人的完整度往往难以保证，容易被遮挡</li>
</ul>
</li>
<li>crowd<ul>
<li>现有方法的detection往往伴随着nms，在过于密集的场景下，紧挨着的行人往往容易被过滤掉</li>
</ul>
</li>
</ul>
</li>
<li>本文档着重于介绍行人检测专用数据集，算法，及与常规检测方法的对比实验</li>
</ul>
<hr>
<h1 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h1><hr>
<p>此处的讨论仅限于在 caltech or cityperson 上做过实验的文章</p>
<h2 id="MS-CNN"><a href="#MS-CNN" class="headerlink" title="MS-CNN"></a>MS-CNN</h2><ul>
<li>paper <a href="https://arxiv.org/pdf/1607.07155.pdf" target="_blank" rel="noopener">A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection</a></li>
<li><img src="/images/pedestrian_detection/mscnn1.png" alt="mscnn1.png"></li>
<li>全文只要思想就是解决目标scale变化的问题，在16年时这样的想法还比较先进，初始化了cityperson baseline 13.32%</li>
</ul>
<h2 id="Repultion-Loss"><a href="#Repultion-Loss" class="headerlink" title="Repultion Loss"></a>Repultion Loss</h2><ul>
<li>paper <a href="https://arxiv.org/pdf/1910.06160.pdf" target="_blank" rel="noopener">Mask-Guided Attention Network for Occluded Pedestrian Detection</a></li>
<li><img src="/images/pedestrian_detection/RepultionLoss1.png" alt="RepultionLoss1.png"></li>
<li>主要思想就是解决一个预测框在两个框中间重叠的情况，也就是所谓的crowd的情况</li>
</ul>
<h2 id="OR-CNN"><a href="#OR-CNN" class="headerlink" title="OR-CNN"></a>OR-CNN</h2><ul>
<li>paper <a href="https://arxiv.org/pdf/1807.08407.pdf" target="_blank" rel="noopener">Occlusion-aware R-CNN: Detecting Pedestrians in a Crowd</a></li>
<li>main contribution</li>
<li>We propose a new occlusion-aware R-CNN method, which uses a new designed AggLoss to enforce proposals to be close to the corresponding objects, as well as minimize the internal region distances of proposals associated with the same objects.</li>
<li>我们提出了一种新的关注遮挡的RCNN，使用了全新的AggLoss</li>
<li>We design a new PORoI pooling unit to replace the RoI pooling layer in the second Fast R-CNN module to integrate the prior structure information of human body with visibility prediction into the network.</li>
<li>我们设计了一种新的POROI pooling取代了原先的ROI pooling</li>
<li><img src="/images/pedestrian_detection/ORCNN1.png" alt="ORCNN1.png"><ol>
<li>agg loss加在rpn上，是由 smooth-L1 和 compactness loss 组成</li>
<li><img src="/images/pedestrian_detection/ORCNN1.1.png" alt="ORCNN1.1.png"></li>
</ol>
</li>
<li>com loss 主要用来约束能匹配gt的anchor refine后能尽可能紧致（框的均值离真值小）</li>
<li><img src="/images/pedestrian_detection/ORCNN2.png" alt="ORCNN2.png"><ol>
<li>OROI pooling</li>
<li>对一个proposal Q，我们将其分为5部分，使用roi pooling得到这五个部分的特征，然后将其喂入OP unit</li>
<li>op unit主要就是一个前后景的attention</li>
<li>最后elewise sum</li>
</ol>
</li>
<li><img src="/images/pedestrian_detection/ORCNN3.png" alt="ORCNN3.png"></li>
<li>cityperson 有可见区域的标签，将5个部分于课件区域IOU进行计算，得到 occlusion loss</li>
<li><img src="/images/pedestrian_detection/ORCNN4.png" alt="ORCNN4.png"></li>
<li>1.3X的VGG16对应val集的 R 11 H 51.3 是非常强的结果了，也是之后工作经常拿ORCNN作为baseline的原因之一</li>
<li><img src="/images/pedestrian_detection/ORCNN5.png" alt="ORCNN5.png"></li>
<li>POROI pooling结果示意图，理想状态下，被遮挡的部位分值低，但不影响其他部分分值</li>
<li><img src="/images/pedestrian_detection/ORCNN6.png" alt="ORCNN6.png"></li>
<li>在test集上表现为 R 11.32，非常不错的成绩</li>
</ul>
<h2 id="CSP"><a href="#CSP" class="headerlink" title="CSP"></a>CSP</h2><ul>
<li>paper <a href="https://arxiv.org/pdf/1904.02948.pdf" target="_blank" rel="noopener">Center and Scale Prediction: A Box-free Approach for Object Detection</a></li>
<li>git <a href="https://github.com/liuwei16/CSP" target="_blank" rel="noopener">https://github.com/liuwei16/CSP</a></li>
<li><img src="/images/pedestrian_detection/CSP1.png" alt="CSP1.png"></li>
<li>convNet的整体框架图，和anchor free的算法们惊人相似，出的时间也相似，都是受CornerNet启发</li>
<li><img src="/images/pedestrian_detection/CSP2.png" alt="CSP2.png"></li>
<li>具体框架图，concat mul-level feature，整体conv出heatmap和scale map，和centernet如出一辙</li>
<li><img src="/images/pedestrian_detection/CSP3.png" alt="CSP3.png"></li>
<li>ussion mask 来做heatmap，也是关键点任务中常见的转换</li>
<li>Scale can be defined as the height and/or width of objects. Towards high-quality ground truth for pedestrian detection, line annotation is first proposed in [54, 55], where tight bounding boxes are automatically generated with a uniform aspect ratio of 0.41. </li>
<li>相当于是固定了anchor ratio去预测scale。由于pedestrian往往是站立的人，长宽比被预设为0.41</li>
<li><img src="/images/pedestrian_detection/CSP4.png" alt="CSP4.png"></li>
<li>在1x上跑出了OC RCNN 1.3x的结果，非常的不错</li>
<li>作者顺路还在widerface上测试了下，效果还不错</li>
<li>CSP其实和常规的检测算法没有太大区别了 </li>
</ul>
<h2 id="MGAN"><a href="#MGAN" class="headerlink" title="MGAN"></a>MGAN</h2><ul>
<li>paper <a href="https://arxiv.org/pdf/1910.06160.pdf" target="_blank" rel="noopener">Mask-Guided Attention Network for Occluded Pedestrian Detection</a></li>
<li>git <a href="https://github.com/Leotju/MGAN" target="_blank" rel="noopener">https://github.com/Leotju/MGAN</a></li>
<li><img src="/images/pedestrian_detection/MGAN1.png" alt="MGAN1.png"></li>
<li>相较于baseline，MGAN能检测出更多的pedestrian</li>
<li><img src="/images/pedestrian_detection/MGAN2.png" alt="MGAN2.png"></li>
<li>相较于FRRCNN，仅新增了MGA模块</li>
<li><img src="/images/pedestrian_detection/MGAN3.png" alt="MGAN3.png"></li>
<li>MGA模块详解，主要作用是生成attention mask重权重feature map</li>
<li>We therefore adapt visible-region bounding box annotation as an approximate alternative.<ul>
<li>使用visible region生成粗粒度的mask用于监督attention</li>
</ul>
</li>
<li><img src="/images/pedestrian_detection/MGAN4.png" alt="MGAN4.png"></li>
<li><img src="/images/pedestrian_detection/MGAN5.png" alt="MGAN5.png"></li>
<li>相较于其他算法，MGAN的干净简单的做法赢得了更好的R与HO</li>
<li><img src="/images/pedestrian_detection/MGAN6.png" alt="MGAN6.png"></li>
<li>在caltech上效果也非常好<ul>
<li>这里不得不提这个Bi-Box</li>
<li><img src="/images/pedestrian_detection/MGAN7.png" alt="MGAN7.png"></li>
<li>也是希望通过利用visible part的信息，改善recall，但是bi-box是两个分支，没有相互间的特征融合，而且对于行人对象，将可视部分视为全身的一部分（前置）而非另一个类，是一个更好的选择</li>
</ul>
</li>
</ul>
<hr>
<h1 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h1><hr>
<p>一般来说，CityPersons数据集较小也有代表性，适合实验</p>
<h2 id="CityPersons"><a href="#CityPersons" class="headerlink" title="CityPersons"></a>CityPersons</h2><ul>
<li>a benchmark for CityPersons, which is a subset of the Cityscapes dataset.</li>
<li>项目库 <a href="https://bitbucket.org/shanshanzhang/citypersons/src/default/" target="_blank" rel="noopener">https://bitbucket.org/shanshanzhang/citypersons/src/default/</a></li>
<li>Benchmark</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Method</th>
<th style="text-align:center">MR (Reasonable)</th>
<th style="text-align:center">MR (Reasonable_small)</th>
<th style="text-align:center">MR (Reasonable_occ=heavy)</th>
<th style="text-align:center">MR (All)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><a href="https://arxiv.org/abs/1910.09188" target="_blank" rel="noopener">APD</a>*</td>
<td style="text-align:center">8.27%</td>
<td style="text-align:center">11.03%</td>
<td style="text-align:center">35.45%</td>
<td style="text-align:center">35.65%</td>
</tr>
<tr>
<td style="text-align:center">YT-PedDet*</td>
<td style="text-align:center">8.41%</td>
<td style="text-align:center">10.60%</td>
<td style="text-align:center">37.88%</td>
<td style="text-align:center">37.22%</td>
</tr>
<tr>
<td style="text-align:center">STNet*</td>
<td style="text-align:center">8.92%</td>
<td style="text-align:center">11.13%</td>
<td style="text-align:center">34.31%</td>
<td style="text-align:center">29.54%</td>
</tr>
<tr>
<td style="text-align:center"><a href="https://arxiv.org/abs/1910.06160" target="_blank" rel="noopener">MGAN</a></td>
<td style="text-align:center">9.29%</td>
<td style="text-align:center">11.38%</td>
<td style="text-align:center">40.97%</td>
<td style="text-align:center">38.86%</td>
</tr>
<tr>
<td style="text-align:center">DVRNet*</td>
<td style="text-align:center">10.99%</td>
<td style="text-align:center">15.68%</td>
<td style="text-align:center">43.77%</td>
<td style="text-align:center">41.48%</td>
</tr>
<tr>
<td style="text-align:center"><a href="https://arxiv.org/abs/1911.11985" target="_blank" rel="noopener">HBA-RCNN</a>*</td>
<td style="text-align:center">11.26%</td>
<td style="text-align:center">15.68%</td>
<td style="text-align:center">39.54%</td>
<td style="text-align:center">38.77%</td>
</tr>
<tr>
<td style="text-align:center"><a href="https://arxiv.org/abs/1807.08407" target="_blank" rel="noopener">OR-CNN</a></td>
<td style="text-align:center">11.32%</td>
<td style="text-align:center">14.19%</td>
<td style="text-align:center">51.43%</td>
<td style="text-align:center">40.19%</td>
</tr>
<tr>
<td style="text-align:center"><a href="http://arxiv.org/abs/1711.07752" target="_blank" rel="noopener">Repultion Loss</a></td>
<td style="text-align:center">11.48%</td>
<td style="text-align:center">15.67%</td>
<td style="text-align:center">52.59%</td>
<td style="text-align:center">39.17%</td>
</tr>
<tr>
<td style="text-align:center"><a href="https://arxiv.org/abs/1906.09756" target="_blank" rel="noopener">Cascade MS-CNN</a></td>
<td style="text-align:center">11.62%</td>
<td style="text-align:center">13.64%</td>
<td style="text-align:center">47.14%</td>
<td style="text-align:center">37.63%</td>
</tr>
<tr>
<td style="text-align:center"><a href="http://202.119.95.70/cache/12/03/openaccess.thecvf.com/f36bf52f1783160552c75ae3cd300e84/Zhang_CityPersons_A_Diverse_CVPR_2017_paper.pdf" target="_blank" rel="noopener">Adapted FasterRCNN</a></td>
<td style="text-align:center">12.97%</td>
<td style="text-align:center">37.24%</td>
<td style="text-align:center">50.47%</td>
<td style="text-align:center">43.86%</td>
</tr>
<tr>
<td style="text-align:center"><a href="https://arxiv.org/abs/1607.07155" target="_blank" rel="noopener">MS-CNN</a></td>
<td style="text-align:center">13.32%</td>
<td style="text-align:center">15.86%</td>
<td style="text-align:center">51.88%</td>
<td style="text-align:center">39.94%</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Caltech"><a href="#Caltech" class="headerlink" title="Caltech"></a>Caltech</h2><ul>
<li>home <a href="http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/" target="_blank" rel="noopener">http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/</a></li>
<li>The Caltech Pedestrian Dataset consists of approximately 10 hours of 640x480 30Hz video taken from a vehicle driving through regular traffic in an urban environment. About 250,000 frames (in 137 approximately minute long segments) with a total of 350,000 bounding boxes and 2300 unique pedestrians were annotated. The annotation includes temporal correspondence between bounding boxes and detailed occlusion labels. More information can be found in our PAMI 2012 and CVPR 2009 benchmarking papers.</li>
<li><img src="/images/pedestrian_detection/Caltech.png" alt="Caltech.png"><ul>
<li>没错，就是fb三剑(ji)客(lao)之一的 Piotr Dollár 主导的项目。coco他也是主导者之一</li>
</ul>
</li>
</ul>
<h2 id="crowd-human"><a href="#crowd-human" class="headerlink" title="crowd human"></a>crowd human</h2><ul>
<li>home <a href="http://www.crowdhuman.org/" target="_blank" rel="noopener">http://www.crowdhuman.org/</a></li>
<li>face ++ 出品</li>
<li><img src="/images/pedestrian_detection/crowd_human1.png" alt="crowd_human1.png"></li>
<li>这是一个新的数据集，相较之前的数据集，单图人数更多，不同的person更多，也更加密集</li>
<li><img src="/images/pedestrian_detection/crowd_human2.png" alt="crowd_human2.png"></li>
<li>在标注上，crowd human也是独具特色<ul>
<li>有人头和全身的标注</li>
<li>有实框（明显的未被遮挡的对象）和虚框（有明显遮挡的对象）的区别</li>
<li>X 代表ignore</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>cv</category>
      </categories>
      <tags>
        <tag>cv</tag>
        <tag>pedestrian_detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Pedestrian Attribute Recognition Survey</title>
    <url>/2020/02/22/Pedestrian_Attribute_Recognition_Survey/</url>
    <content><![CDATA[<p>梳理 Pedestrian Attribute Recognition(行人属性识别) 相关介绍，数据集，算法<br><a id="more"></a><br><!-- toc --></p>
<hr>
<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><hr>
<h2 id="Foreword"><a href="#Foreword" class="headerlink" title="Foreword"></a>Foreword</h2><ul>
<li>（reference <a href="https://arxiv.org/pdf/1901.07474.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1901.07474.pdf</a> <a href="https://github.com/wangxiao5791509/Pedestrian-Attribute-Recognition-Paper-List）" target="_blank" rel="noopener">https://github.com/wangxiao5791509/Pedestrian-Attribute-Recognition-Paper-List）</a></li>
<li><img src="/images/PAR/foreword.png" alt="foreword.png"></li>
</ul>
<h2 id="What-is-Pedestrian-Attribute-Recognition"><a href="#What-is-Pedestrian-Attribute-Recognition" class="headerlink" title="What is Pedestrian Attribute Recognition"></a>What is Pedestrian Attribute Recognition</h2><ul>
<li>Pedestrian attributes, are humanly searchable semantic descriptions and can be used as soft-biometrics in visual surveillance, with applications in person re-identification, face verification, and human identification. Pedestrian attributes recognition (PAR) aims at mining the attributes of target people when given person image, as shown in follow.</li>
<li>行人属性是人为的可搜索的拥有语义信息的描述，可以用来作为一种软生物识别技术在视频监控领域，在re-id，人脸识别，人体识别上都有应用。下面是行人属性识别的展示</li>
<li><img src="/images/PAR/Intro2.png" alt="Intro2.png"></li>
</ul>
<h2 id="Traditional-practice-and-more"><a href="#Traditional-practice-and-more" class="headerlink" title="Traditional practice and more"></a>Traditional practice and more</h2><ul>
<li>Traditional pedestrian attributes recognition methods usually focus on developing robust feature representation from the perspectives of hand-crafted features, powerful classifiers or attributes relations. Some milestones including HOG [1], SIFT [2], SVM [3] or CRF model [4]. However, the reports on large-scale benchmark evaluations suggest that the performance of these traditional algorithms is far from the requirement of realistic applications. Over the past several years, deep learning have achieved an impressive performance due to their success on automatic feature extraction using multi-layer nonlinear transformation, especially in computer vision, speech recognition and natural language processing. Several deep learning based attribute recognition algorithms has been proposed based on these breakthroughs.</li>
<li>传统的做法基本就是 手工设计的特征 + 强力的分类器/属性关联 ，如 HOG SIFT SVM CRF，但是这些在大规模的benchmark上就没那么好使了。在过去的这些年里，深度学习方法因其自动强大的特征提取器带来了令人印象深刻的表现，在行人属性识别上也是如此。</li>
</ul>
<h2 id="PROBLEM-FORMULATION-AND-CHALLENGES"><a href="#PROBLEM-FORMULATION-AND-CHALLENGES" class="headerlink" title="PROBLEM FORMULATION AND CHALLENGES"></a>PROBLEM FORMULATION AND CHALLENGES</h2><ul>
<li>Multi-views 对同一对象，不同视角观察带来的差异</li>
<li>Occlusion 遮挡</li>
<li>Unbalanced Data Distribution 不平衡的 数据|lable 分布</li>
<li>Low Resolution 低分辨率</li>
<li>Illumination 亮暗在一日内变化大</li>
<li>Blur 模糊</li>
</ul>
<h2 id="Evaluation-Criteria"><a href="#Evaluation-Criteria" class="headerlink" title="Evaluation Criteria"></a>Evaluation Criteria</h2><ul>
<li><img src="/images/PAR/Intro3.png" alt="Intro3.png"></li>
</ul>
<h2 id="REGULAR-PIPELINE-FOR-PAR"><a href="#REGULAR-PIPELINE-FOR-PAR" class="headerlink" title="REGULAR PIPELINE FOR PAR"></a>REGULAR PIPELINE FOR PAR</h2><ol>
<li>Multi-task Learning<ul>
<li><img src="/images/PAR/Intro4.png" alt="Intro4.png"></li>
<li>deep learning based multi-task learning, i.e. the hard and soft parameter sharing. The hard parameter sharing usually take the shallow layers as shared layers to learn the common feature representations of multiple tasks, and treat the high-level layers as taskspecific layers to learn more discriminative patterns. This mode is the most popular framework in the deep learning community. The illustration of hard parameter sharing can be found in Figure 4 (left sub-figure). For the soft parameter sharing multi-task learning (as shown in Figure 4 (right sub-figure)), they train each task independently, but make the parameters between different tasks similar via the introduced regularization constrains, such as L2 distance [53] and trace norm [54].</li>
<li>深度学习里多任务学习往往分 硬|软 参数共享<ul>
<li>硬参数共享共享一个backbone，后面接多任务</li>
<li>软参数共享几乎完全独立，通过使不同任务中的参数相似（例如 L2-Distance, trace norm）来建立任务间的联系</li>
</ul>
</li>
</ul>
</li>
<li>Multi-label Learning<ul>
<li><img src="/images/PAR/Intro5.png" alt="Intro5.png"></li>
<li>problem transformation<ul>
<li>binary relevance algorithm<ul>
<li>将问题转化为 后接多个二分类 的分类问题</li>
<li>简单，符合直觉</li>
<li>忽略了标签间的关联关系</li>
</ul>
</li>
<li>classifier chain algorithm<ul>
<li>将问题转化为 二分类链 问题，每一个二分类结果以来之前的结果</li>
</ul>
</li>
<li>calibrated label ranking algorithm.<ul>
<li>将问题转化为 标签排序 问题</li>
</ul>
</li>
<li>random k-Labelsets algorithm<ul>
<li>将问题转化为 多组标签 分类问题 </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>algorithm adaptation <ol>
<li>multi-label k-nearest neighbour</li>
<li>multi-label decision tree</li>
<li>ranking support vector machine  Rank-SVM</li>
<li>collective multilabel classifier</li>
</ol>
</li>
</ol>
<h2 id="APPLICATIONS"><a href="#APPLICATIONS" class="headerlink" title="APPLICATIONS"></a>APPLICATIONS</h2><ul>
<li>Visual attributes can be seen as a kind of mid-level feature representation which may provide important information for high-level human related tasks, such as person re-identification [128], [129], [130], [131], [132], pedestrian detection [133], person tracking [134], person retrieval [135], [136], human action recognition[137], scene understanding [138]. Due to the limited space of this paper, we only review some works in the rest of this subsections.</li>
<li>视觉属性可以视作一种中间等级的特征，可以用来协助高级的与人相关的任务，例如 re-id 行人检测 行人跟踪 行人回复 行人动作识别 场景理解等。此外，行人属性也作为一种标签用于统计单体和群体的画像，在商业场景下用途广泛</li>
</ul>
<hr>
<h1 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h1><hr>
<h2 id="Global-Image-based-Method"><a href="#Global-Image-based-Method" class="headerlink" title="Global Image-based Method"></a>Global Image-based Method</h2><h3 id="ACN"><a href="#ACN" class="headerlink" title="ACN"></a>ACN</h3><ul>
<li>paper <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015_workshops/w11/papers/Sudowe_Person_Attribute_Recognition_ICCV_2015_paper.pdf" target="_blank" rel="noopener">Person Attribute Recognition with a Jointly-trained Holistic CNN Model</a></li>
<li><img src="/images/PAR/ACN1.png" alt="ACN1.png"></li>
<li>they adopt a pre-trained AlexNet as basic feature extraction sub-network, and replace the last fully connected layer with one loss per attribute using the KL-loss. </li>
<li>In addition, they also propose a new dataset named as PARSE-27k to support their evaluation. This dataset contains 27000 pedestrians and annotated with 10 attributes. Different from regular person attribute dataset, they propose a new category annotation, i.e., not decidable (N/A). Because for most input images, some attributes are not decidable due to occlusion, image boundaries, or any other reason.</li>
<li>最早期的文章之一，想法简单直接，推出了新的数据集PARSE-27k用做评估</li>
</ul>
<h3 id="DeepSAR-DeepMAR"><a href="#DeepSAR-DeepMAR" class="headerlink" title="DeepSAR/DeepMAR"></a>DeepSAR/DeepMAR</h3><ul>
<li>paper <a href="http://doc.startdt.net/download/attachments/37640056/Multi-attributeLearningforPedestrianAttributeRecognitioninSurveillanceScenarios.pdf?version=1&amp;modificationDate=1571305310000&amp;api=v2" target="_blank" rel="noopener">Multi-attribute Learning for Pedestrian Attribute Recognition in Surveillance Scenarios</a></li>
<li>git <a href="https://github.com/dangweili/pedestrian-attribute-recognition-pytorch" target="_blank" rel="noopener">https://github.com/dangweili/pedestrian-attribute-recognition-pytorch</a></li>
<li><img src="/images/PAR/DEEPSAR1.png" alt="DEEPSAR1.png"></li>
<li>Single Attribute Recognition (SAR)<ul>
<li>DeepSAR model is proposed to recognize each attribute one by one.Treating each attribute as an independent component, the DeepSAR method is proposed to predict each attribute.</li>
<li><img src="/images/PAR/DEEPSAR2.png" alt="DEEPSAR2.png"></li>
</ul>
</li>
<li>Multi-attribute Recognition (MAR)<ul>
<li>To better utilize the relationship among attributes, the unified multi-attribute jointly learning model (DeepMAR) is proposed to learn all the attributes at the same time.</li>
<li>Different from DeepSAR, the input of the DeepMAR is an image with its attribute label vector and the loss function considers all the attributes jointly.</li>
<li><img src="/images/PAR/DEEPSAR3.png" alt="DEEPSAR3.png"></li>
</ul>
</li>
<li>Experiment<ul>
<li><img src="/images/PAR/DEEPSAR4.png" alt="DEEPSAR4.png"></li>
<li>属性间的关联关系有助于提升平均准确率，但在具体各项上，有提升的有下降的</li>
</ul>
</li>
</ul>
<h3 id="MTCNN"><a href="#MTCNN" class="headerlink" title="MTCNN"></a>MTCNN</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1601.00400.pdf" target="_blank" rel="noopener"> Multi-task CNN Model for Attribute Prediction</a></li>
<li><img src="/images/PAR/MTCNN1.png" alt="MTCNN1.png"></li>
<li>Because our model requires more than one CNN model, we remove the last fully connected layers, as we substitute these layers with our own joint MTL objective loss, depending on the weight parameter matrix learned within.<ul>
<li>移除原始网络最后一层的fc，使用了都有独特的 joint MTL objective loss 来联合训练</li>
<li>Feature Sharing and Competition in MTL<ul>
<li><img src="/images/PAR/MTCNN2.png" alt="MTCNN2.png"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Attention-based-Method"><a href="#Attention-based-Method" class="headerlink" title="Attention-based Method"></a>Attention-based Method</h2><h3 id="HydraPlus-Net"><a href="#HydraPlus-Net" class="headerlink" title="HydraPlus-Net"></a>HydraPlus-Net</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1709.09930.pdf" target="_blank" rel="noopener">HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis</a></li>
<li>git <a href="https://github.com/xh-liu/HydraPlus-Net" target="_blank" rel="noopener">https://github.com/xh-liu/HydraPlus-Net</a></li>
<li>提出了PA-100K</li>
<li><img src="/images/PAR/HydraPlus-Net1.png" alt="HydraPlus-Net1.png"></li>
<li>在行人分析里，不同的分析对象所需要的特征级别不同，尺度不同<ul>
<li>语义级 不同人之间有所区分，但是咋看上去还挺相似例如 长头发vs短头发 长袖vs短袖</li>
<li>低层级 例如 clothing stride 就可以很好地在低层特征算出，比高层算出的结果要好</li>
<li>尺度差异 有些任务的关注点在手部而有些是全身，尺度变化大</li>
</ul>
</li>
<li><img src="/images/PAR/HydraPlus-Net2.png" alt="HydraPlus-Net2.png"></li>
<li>由 MainNet AttentiveFeatureNet 构成，由MNet生成特征由AFNet生成attention mask，其中F函数对应的incept模块是MNet中三个incept模块的复制（MNet会先被预训练，主体是当年流行的inception-v2）</li>
<li><img src="/images/PAR/HydraPlus-Net3.png" alt="HydraPlus-Net3.png"></li>
<li>一个MDA的例子，对于每一个在MNet中的incept模块，通过和三层incept的相互关联得到新的结果</li>
<li><img src="/images/PAR/HydraPlus-Net4.png" alt="HydraPlus-Net4.png"></li>
<li>举个例子如上图所示，原始输出结果结合毗邻特征，结合低层特征就变得更加惊喜噪音也更多，结合高层特征整体更加突出完整信息也变少更集中，对于不同的任务可以各取所需</li>
<li><img src="/images/PAR/HydraPlus-Net5.png" alt="HydraPlus-Net5.png"></li>
</ul>
<h3 id="VeSPA"><a href="#VeSPA" class="headerlink" title="VeSPA"></a>VeSPA</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1707.06089.pdf" target="_blank" rel="noopener">Deep viewsensitive pedestrian attribute inference in an end-to-end model</a></li>
<li><img src="/images/PAR/VeSPA1.png" alt="VeSPA1.png"></li>
<li>We adapt a deep neural network for joint pose and multi-label attribute classification. The overall design of our approach is shown in Figure 1. The main network is based on the GoogleNet inception architecture [20]. As shown, the network contains a view classification branch and three view-specific attribute predictor units. The view classifier and attribute predictors are both trained with separate loss functions. Prediction scores from weighted view-specific predictors are aggregated to generate the final multi-class attribute predictions. The whole network is a unified framework and is trained in an end-to-end manner.<ul>
<li>只要使用了GoogleNet inception architecture</li>
<li>分成了正面 背面 侧面三个分支预测属性</li>
<li>引入额外分支用于分类是正面背面侧面，并将其分值分别乘在三个分支上，最终综合得到最后结果</li>
</ul>
</li>
<li>这里引入了front back side，下面引用一张图说明</li>
<li><img src="/images/PAR/VeSPA2.png" alt="VeSPA2.png"></li>
<li><img src="/images/PAR/VeSPA3.png" alt="VeSPA3.png"></li>
<li>从结果上看效果很理想，甚至超过了花里胡哨的HPNET</li>
</ul>
<h3 id="DIAA"><a href="#DIAA" class="headerlink" title="DIAA"></a>DIAA</h3><ul>
<li>paper <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Nikolaos_Sarafianos_Deep_Imbalanced_Attribute_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Deep Imbalanced Attribute Classification using Visual Attention Aggregation</a></li>
<li><img src="/images/PAR/DIAA1.png" alt="DIAA1.png"></li>
<li>简单直接，且看看具体有什么不同</li>
<li><img src="/images/PAR/DIAA2.png" alt="DIAA2.png"></li>
<li>特殊的attention机制，一边是sigmoid的权重层，另一边是常规卷积层加上空间的正则化</li>
<li><img src="/images/PAR/DIAA3.png" alt="DIAA3.png"></li>
<li>通过spatial softmax达到单层的normalization，结果在iv中展示，确实能使attention更集中了</li>
<li><img src="/images/PAR/DIAA4.png" alt="DIAA4.png"></li>
<li>和focal loss的变种，其中Wc被设定为与类别属性分布相关的权重值，是每一类不同的，作者称其为 Deep Imbalanced Classification</li>
<li><img src="/images/PAR/DIAA5.png" alt="DIAA5.png"></li>
<li>对于非主分支的两个分支使用了常规的分类loss，并记录前值计算标准差作为系数加大loss</li>
<li><img src="/images/PAR/DIAA6.png" alt="DIAA6.png"></li>
<li>PETA上实验结果比较理想</li>
</ul>
<h3 id="CAM"><a href="#CAM" class="headerlink" title="CAM"></a>CAM</h3><ul>
<li>paper <a href="https://cse.sc.edu/~songwang/document/prl17.pdf" target="_blank" rel="noopener">Human attribute recognition by refining attention heat map</a></li>
<li>git <a href="https://github.com/hguosc/Human-attribute-recognition-by-refining-attention-heat-map" target="_blank" rel="noopener">https://github.com/hguosc/Human-attribute-recognition-by-refining-attention-heat-map</a></li>
<li><img src="/images/PAR/CAM1.png" alt="CAM1.png"></li>
<li>展示了效果</li>
<li><img src="/images/PAR/CAM2.png" alt="CAM2.png"></li>
<li><img src="/images/PAR/CAM3.png" alt="CAM3.png"></li>
<li>在backbone各attribute共享，在FC开始分离，通过weight average layer（The customization of the weighted average layer is used to embed the attention heat map into the network training and the modification of the fully-connected layer is used to output both features and weights.）处理得到attention map，继续常规操作得到结果</li>
<li><img src="/images/PAR/CAM4.png" alt="CAM4.png"></li>
<li><img src="/images/PAR/CAM5.png" alt="CAM5.png"></li>
<li>可见这个exponential loss主要是为了使最大值更大，使关注点更集中</li>
<li><img src="/images/PAR/CAM6.png" alt="CAM6.png"></li>
<li>只有 wider 的结果，较baseline有所提升</li>
</ul>
<h2 id="Curriculum-Learning-Method"><a href="#Curriculum-Learning-Method" class="headerlink" title="Curriculum Learning Method"></a>Curriculum Learning Method</h2><h3 id="MTCT"><a href="#MTCT" class="headerlink" title="MTCT"></a>MTCT</h3><ul>
<li>paper <a href="http://www.eecs.qmul.ac.uk/~xiatian/papers/WACV17/DongEtAl_WACV2017.pdf" target="_blank" rel="noopener">Multi-Task Curriculum Transfer Deep Learning of Clothing Attributes</a></li>
<li><img src="/images/PAR/MTCT1.png" alt="MTCT1.png"></li>
<li>训练分两个阶段，一阶段是常规的属性学习MTN，使用常规的分类loss；第二阶段将一阶段训练完成的MTN重新构建成为三元组，使用e t-distribution Stochastic Triplet Embedding (t-STE) loss进行训练</li>
<li>文章不是针对行人属性的，不做拓展</li>
</ul>
<h3 id="CILICIA"><a href="#CILICIA" class="headerlink" title="CILICIA"></a>CILICIA</h3><ul>
<li>paper <a href="https://nsarafianos.github.io/assets/Curriculum_Learning_Clusters.pdf" target="_blank" rel="noopener">Curriculum learning for multi-task classification of visual attributes</a></li>
<li><img src="/images/PAR/CILICIA1.png" alt="CILICIA1.png"></li>
<li>这篇文章的主要思想就是先学习得到task间的相关性，然后定义下个阶段的task组，循环学习到只有2个任务</li>
<li><img src="/images/PAR/CILICIA2.png" alt="CILICIA2.png"></li>
<li>类似这样，是多stage的做法，不是现在的主流</li>
</ul>
<h2 id="Graph-based-Method"><a href="#Graph-based-Method" class="headerlink" title="Graph based Method"></a>Graph based Method</h2><h3 id="DCSA"><a href="#DCSA" class="headerlink" title="DCSA"></a>DCSA</h3><ul>
<li>paper <a href="http://chenlab.ece.cornell.edu/people/Andy/publications/ECCV2012_ClothingAttributes.pdf" target="_blank" rel="noopener">Describing clothing by semantic attributes</a></li>
<li><img src="/images/PAR/DCSA1.png" alt="DCSA1.png"></li>
<li>整体的思路即使放在现在也不过时，12年的文章。先通过pose estimation得到大致的区域，然后划分区域特征抽取，得到各属性分类结果，然后构建多属性图使用CRF进行推论得到结果。具体方法如SIFT SVM等已不适用现在，不展开</li>
</ul>
<h3 id="A-AOG"><a href="#A-AOG" class="headerlink" title="A-AOG"></a>A-AOG</h3><ul>
<li>paper <a href="http://www.stat.ucla.edu/~sczhu/papers/PAMI_Attribute_Grammar.pdf" target="_blank" rel="noopener"> Attribute and-or grammar for joint parsing of human pose, parts and attributes</a></li>
<li><img src="/images/PAR/A-AOG1.png" alt="A-AOG1.png"></li>
<li>人体分割，按约定的树构建层级关系，各部位分别出各自的属性，效果很炫酷</li>
<li><img src="/images/PAR/A-AOG2.png" alt="A-AOG2.png"></li>
<li>低层组件定义了14个，中间件2个，root件一个。每个部件拥有9个相关属性，通过属性关联链连接各部分属性。</li>
<li><img src="/images/PAR/A-AOG3.png" alt="A-AOG3.png"></li>
<li>如果结果理想的话，单个属性会被某几个部分突出地代表</li>
<li>本文对于loss只是一句 CE 带过，整体结果也没有在常见的数据集上，只有思路供参考</li>
</ul>
<h3 id="PAR-w-GCN"><a href="#PAR-w-GCN" class="headerlink" title="PAR w/ GCN"></a>PAR w/ GCN</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1904.03582.pdf" target="_blank" rel="noopener">Multi-Label Image Recognition with Graph Convolutional Networks</a></li>
<li>git <a href="https://github.com/2014gaokao/pedestrian-attribute-recognition-with-GCN" target="_blank" rel="noopener">https://github.com/2014gaokao/pedestrian-attribute-recognition-with-GCN</a></li>
<li><img src="/images/PAR/PARGCN1.png" alt="PARGCN1.png"></li>
<li><img src="/images/PAR/PARGCN2.jpg" alt="PARGCN2.png"></li>
<li>b图是原始论文网络框架图，c图是git项目的图，原理是相同的。上面分支是backbone出特征，下面是GCN分支出关联关系，点乘得到结果。那么这个GCN的输入输出是什么呢？究竟完成了一个什么事情？</li>
<li>理解GCN 推荐 <a href="https://www.zhihu.com/question/54504471/answer/332657604" target="_blank" rel="noopener">https://www.zhihu.com/question/54504471/answer/332657604</a></li>
<li>介绍下GCN代码和glove<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># GCN方法，通过矩阵乘改变输出维度</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GraphConvolution</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, out_features, bias=False)</span>:</span></span><br><span class="line">        super(GraphConvolution, self).__init__()</span><br><span class="line">        self.in_features = in_features</span><br><span class="line">        self.out_features = out_features</span><br><span class="line">        self.weight = Parameter(torch.Tensor(in_features, out_features))</span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.bias = Parameter(torch.Tensor(<span class="number">1</span>, <span class="number">1</span>, out_features))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_parameter(<span class="string">'bias'</span>, <span class="literal">None</span>)</span><br><span class="line">        self.reset_parameters()</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_parameters</span><span class="params">(self)</span>:</span></span><br><span class="line">        stdv = <span class="number">1.</span> / math.sqrt(self.weight.size(<span class="number">1</span>))</span><br><span class="line">        self.weight.data.uniform_(-stdv, stdv)</span><br><span class="line">        <span class="keyword">if</span> self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.bias.data.uniform_(-stdv, stdv)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, adj)</span>:</span></span><br><span class="line">        support = torch.matmul(input, self.weight)</span><br><span class="line">        output = torch.matmul(adj, support)</span><br><span class="line">        <span class="keyword">if</span> self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> output + self.bias</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> output</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__class__.__name__ + <span class="string">' ('</span> \</span><br><span class="line">               + str(self.in_features) + <span class="string">' -&gt; '</span> \</span><br><span class="line">               + str(self.out_features) + <span class="string">')'</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># GLOVE其实指的是 原始label word2vec 后得到的库，文章里称glove</span></span><br><span class="line">word_to_ix = &#123;j: i <span class="keyword">for</span> i, j <span class="keyword">in</span> enumerate(select_name)&#125;</span><br><span class="line">embeds=nn.Embedding(<span class="number">60</span>,<span class="number">300</span>)</span><br><span class="line">word2vec=torch.tensor([])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(select)):</span><br><span class="line">    lookup_tensor=torch.tensor([word_to_ix[select_name[i]]],dtype=torch.long)</span><br><span class="line">    embed=embeds(lookup_tensor)</span><br><span class="line">    word2vec=torch.cat((word2vec,embed),<span class="number">0</span>)</span><br></pre></td></tr></table></figure></li>
<li><img src="/images/PAR/PARGCN3.png" alt="PARGCN3.png"></li>
<li>这张图说明了GCN想要的结果，主要是想要这个条件概率。以下是adj的初始化方式<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 初始化</span></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> partition[<span class="string">'train'</span>][<span class="number">0</span>]:</span><br><span class="line">    t=np.array(dataset[<span class="string">'att'</span>][idx])[dataset[<span class="string">'selected_attribute'</span>]]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(np.array(dataset[<span class="string">'att'</span>][idx])[dataset[<span class="string">'selected_attribute'</span>]])):</span><br><span class="line">        <span class="keyword">if</span> t[i]==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(len(np.array(dataset[<span class="string">'att'</span>][idx])[dataset[<span class="string">'selected_attribute'</span>]])):</span><br><span class="line">                <span class="keyword">if</span> t[j]==<span class="number">1</span> <span class="keyword">and</span> j!=i:</span><br><span class="line">                    concur[i][j]+=<span class="number">1</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 初始化A norm化卡阈值 + 1 得到初始值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_A</span><span class="params">(num_classes, t, adj_file)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    result = pickle.load(open(adj_file, <span class="string">'rb'</span>))</span><br><span class="line">    _adj = result[<span class="string">'adj'</span>]</span><br><span class="line">    _nums = result[<span class="string">'nums'</span>]</span><br><span class="line">    _nums = _nums[:, np.newaxis]</span><br><span class="line">    _adj = _adj / _nums</span><br><span class="line">    _adj[_adj &lt; t] = <span class="number">0</span></span><br><span class="line">    _adj[_adj &gt;= t] = <span class="number">1</span></span><br><span class="line">    <span class="comment">#_adj = _adj * 0.9 / (_adj.sum(0) + 1e-6)</span></span><br><span class="line">    _adj = _adj + np.identity(num_classes, np.int)</span><br><span class="line">    <span class="keyword">return</span> _adj</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 真实使用时，对应矩阵运算就是 D^(-1)AD</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_adj</span><span class="params">(A)</span>:</span></span><br><span class="line">    D = torch.pow(A.sum(<span class="number">1</span>).float(), <span class="number">-0.5</span>)</span><br><span class="line">    D = torch.diag(D)</span><br><span class="line">    adj = torch.matmul(torch.matmul(A, D).t(), D)</span><br><span class="line">    <span class="keyword">return</span> adj</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># adj其实指的是 adjacency matrix, 用于描述图节点的矩阵。预处理后使用</span></span><br></pre></td></tr></table></figure></li>
<li><img src="/images/PAR/PARGCN4.png" alt="PARGCN4.png"></li>
<li>rap的结果，原始论文不是拿来做这个用途没有相关的结果。从结果上看还是很顶的，基本达到了sota水平</li>
</ul>
<h3 id="VSGR"><a href="#VSGR" class="headerlink" title="VSGR"></a>VSGR</h3><ul>
<li>paper <a href="http://doc.startdt.net/download/attachments/37640056/4884-Article%20Text-7950-1-10-20190709.pdf?version=1&amp;modificationDate=1571743975000&amp;api=v2" target="_blank" rel="noopener">Visua-semantic graph reasoning for pedestrian attribute recognition</a></li>
<li><img src="/images/PAR/VSGR1.png" alt="VSGR1.png"></li>
<li>Visual-to-semantic Sub-network + Semantic-to-visual Sub-network fuse得到结果<ul>
<li>目前无开源代码，原文里操作多且复杂，难以非常清晰地理解</li>
</ul>
</li>
<li>Experiment<ul>
<li><img src="/images/PAR/VSGR2.png" alt="VSGR2.png"></li>
</ul>
</li>
<li>有待开源后进一步研究</li>
</ul>
<h2 id="Loss-Function-based-Method"><a href="#Loss-Function-based-Method" class="headerlink" title="Loss Function based Method"></a>Loss Function based Method</h2><h3 id="WPAL"><a href="#WPAL" class="headerlink" title="WPAL"></a>WPAL</h3><ul>
<li>paper <a href="http://www.bmva.org/bmvc/2017/papers/paper069/paper069.pdf" target="_blank" rel="noopener">Weakly-supervised Learning of Mid-level Features for Pedestrian Attribute Recognition and Localization</a></li>
<li><img src="/images/PAR/WPAL1.png" alt="WPAL1.png"></li>
<li>从结构上看主要的贡献是 multi-level feature + FSPP + weighted-CE（Flexible Spatial Pyramid Pooling ）</li>
<li><img src="/images/PAR/WPAL2.png" alt="WPAL2.png"></li>
<li>level 1是全图的conv，level 2是用了预设的9个bins去框特征得到的结果</li>
<li><img src="/images/PAR/WPAL3.png" alt="WPAL3.png"></li>
<li>Wi是label在数据集中相对数量的百分比，平衡数据</li>
<li><img src="/images/PAR/WPAL4.png" alt="WPAL4.png"></li>
<li>提出了新的衡量指标 IoP，然而并没有多少人用</li>
<li><img src="/images/PAR/WPAL5.png" alt="WPAL5.png"></li>
<li>在RAP上表现不错，但是这个方法对头发极为不鲁邦</li>
</ul>
<h3 id="AWMT"><a href="#AWMT" class="headerlink" title="AWMT"></a>AWMT</h3><ul>
<li>paper <a href="https://craigie1996.github.io/2018/05/11/Pedestrian-Attribute-Recognition-%E8%B0%83%E7%A0%94%E7%AC%94%E8%AE%B0/papers/Adaptively%20Weighted.pdf" target="_blank" rel="noopener">Adaptively weighted multi-task deep network for person attribute classification</a></li>
<li>git <a href="https://github.com/qiexing/adaptive_weighted_attribute" target="_blank" rel="noopener">https://github.com/qiexing/adaptive_weighted_attribute</a></li>
<li><img src="/images/PAR/AWMT1.png" alt="AWMT1.png"></li>
<li>结构上只是常规的res50，输入同时有train和val，在loss上有所技巧</li>
<li><img src="/images/PAR/AWMT2.png" alt="AWMT2.png"></li>
<li>任务目标是使 Θ 最小，其中 λ 为可更新超参（权重），Lkj是阈值</li>
<li><img src="/images/PAR/AWMT3.png" alt="AWMT3.png"></li>
<li>主题算法一共有两个部分<ul>
<li>输入 train val 进行forward，train_loss * λ 作为backward的loss; 权重λ在到达更新period时更新</li>
<li>λ 的 更新方法</li>
</ul>
</li>
<li>其实还是用了 val的数据，还间接参与了backward</li>
</ul>
<h2 id="Part-based-Method"><a href="#Part-based-Method" class="headerlink" title="Part-based Method"></a>Part-based Method</h2><h3 id="Poselets"><a href="#Poselets" class="headerlink" title="Poselets"></a>Poselets</h3><ul>
<li>paper <a href="https://people.cs.umass.edu/~smaji/papers/attributes-iccv11.pdf" target="_blank" rel="noopener">Describing People: A Poselet-Based Approach to Attribute Classification</a></li>
<li><img src="/images/PAR/Poselets1.png" alt="Poselets1.png"></li>
<li>先使用poselets讲身体分为几张图<ul>
<li>poselet paper <a href="http://www.cs.utexas.edu/~cv-fall2012/slides/dinesh-paper.pdf" target="_blank" rel="noopener">http://www.cs.utexas.edu/~cv-fall2012/slides/dinesh-paper.pdf</a></li>
<li>We use the method of Bourdev et al. [3] to train 1200 poselets using images from the training and validation sets. Instead of all poselets having the same aspect ratios, we used four aspect ratios: 96x64, 64x64, 64x96 and 64x128 and trained 300 poselets of each. For each poselet, during training, we build a soft mask for the probability of each body component (such as hair, face, upper clothes, lower clothes, etc) at each location within the normalized poselet patch (Figure 5) using body component annotations on the H3D dataset [4].</li>
<li><img src="/images/PAR/Poselets2.png" alt="Poselets2.png"></li>
<li><img src="/images/PAR/Poselets3.png" alt="Poselets3.png"></li>
<li>后接3个SVM用于分类</li>
</ul>
</li>
</ul>
<h3 id="RAD"><a href="#RAD" class="headerlink" title="RAD"></a>RAD</h3><ul>
<li>paper <a href="https://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Joo_Human_Attribute_Recognition_2013_ICCV_paper.pdf" target="_blank" rel="noopener">Human Attribute Recognition by Rich Appearance Dictionary</a></li>
<li><img src="/images/PAR/RAD1.png" alt="RAD1.png"></li>
<li><img src="/images/PAR/RAD2.png" alt="RAD2.png"></li>
</ul>
<h3 id="PANDA"><a href="#PANDA" class="headerlink" title="PANDA"></a>PANDA</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1311.5591.pdf" target="_blank" rel="noopener">PANDA: Pose Aligned Networks for Deep Attribute Modeling</a></li>
<li><img src="/images/PAR/PANDA1.png" alt="PANDA1.png"></li>
<li>与Poselets很相似，但是有了独立的CNN支持</li>
<li><img src="/images/PAR/PANDA2.png" alt="PANDA2.png"></li>
<li>在有独立CNN支持后性能改善较多</li>
<li><img src="/images/PAR/PANDA3.png" alt="PANDA3.png"></li>
</ul>
<h3 id="MLCNN"><a href="#MLCNN" class="headerlink" title="MLCNN"></a>MLCNN</h3><ul>
<li>paper <a href="http://www.cbsr.ia.ac.cn/users/zlei/papers/ICB2015/Zhu-ICB-15.pdf" target="_blank" rel="noopener">Multi-label CNN Based Pedestrian Attribute Learning for Soft Biometrics</a></li>
<li><img src="/images/PAR/MLCNN1.png" alt="MLCNN1.png"></li>
<li>直接把一张图划分为有overlap的15个块，对不同的块进行卷积操作，使用特定块结果得到特定标签结果，如mid-hair标签使用了1,2,3块。</li>
</ul>
<h3 id="AAWP"><a href="#AAWP" class="headerlink" title="AAWP"></a>AAWP</h3><ul>
<li>paper <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Gkioxari_Actions_and_Attributes_ICCV_2015_paper.pdf" target="_blank" rel="noopener">Actions and Attributes from Wholes and Parts</a></li>
<li><img src="/images/PAR/AAWP1.png" alt="AAWP1.png"></li>
<li>这篇文章开始使用 R-CNN 检测，从图像中得到 全身，头部，上身，下身 的具体位置，切割出来进行属性识别</li>
<li><img src="/images/PAR/AAWP2.png" alt="AAWP2.png"></li>
<li>对检测部分，使用高斯金字塔处理过的图像作为输入，通过多尺度提高结果，已经非常类似与FPN</li>
<li><img src="/images/PAR/AAWP3.png" alt="AAWP3.png"></li>
<li>分类器还是很普通的 CNN + SVM 的做派</li>
</ul>
<h3 id="ARAP"><a href="#ARAP" class="headerlink" title="ARAP"></a>ARAP</h3><ul>
<li>paper <a href="http://www.bmva.org/bmvc/2016/papers/paper081/paper081.pdf" target="_blank" rel="noopener">Attribute Recognition from Adaptive Parts</a></li>
<li><img src="/images/PAR/ARAP1.png" alt="ARAP1.png"></li>
<li>通过关键点得到各部件，然后得到各部分属性</li>
<li><img src="/images/PAR/ARAP2.png" alt="ARAP2.png"></li>
<li>关键点检测和属性识别共享基础backbone，先进行关键点检测，得到感兴趣的部件区域，将特征层上采样与部件区域对其，最后fc分类得到结果</li>
<li><img src="/images/PAR/ARAP3.png" alt="ARAP3.png"></li>
<li>值得一提的是，作者不仅对关键点设置了回归loss，也对bbox的结果进行了 loss 约束，虽然形式有些怪异，不过确实改进了性能</li>
<li><img src="/images/PAR/ARAP4.png" alt="ARAP4.png"></li>
<li>毫无疑问的是，精准定位会带来的属性识别性能提升，尤其是ratio loss带来的对bbox的修正使整体方法更完整</li>
<li><img src="/images/PAR/ARAP5.png" alt="ARAP5.png"></li>
</ul>
<h3 id="DeepCAMP"><a href="#DeepCAMP" class="headerlink" title="DeepCAMP"></a>DeepCAMP</h3><ul>
<li>paper <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Diba_DeepCAMP_Deep_Convolutional_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Deepcamp: Deep convolutional action &amp; attribute mid-level patterns</a></li>
<li><img src="/images/PAR/DeepCAMP1.png" alt="DeepCAMP1.png"></li>
<li>看着这个示意图青一块紫一块的，其实是 聚类(cluster) 的结果，继续往下看</li>
<li><img src="/images/PAR/DeepCAMP2.png" alt="DeepCAMP2.png"></li>
<li>不同于后来的基于attention的无监督注意力机制，本文使用了聚类方法，对各patch特征进行无监督处理</li>
<li><img src="/images/PAR/DeepCAMP3.png" alt="DeepCAMP3.png"></li>
<li>伪代码简单明了很清晰，先抽特征，更新聚类器，得到分值，最后得到结果。值得一提的是，本文使用了LDA作为降维聚类工具</li>
<li><img src="/images/PAR/DeepCAMP4.png" alt="DeepCAMP4.png"></li>
<li>Mid-level Deep Patterns Network 具体如上图所示，图上没有标出的是作者使用了fast-RCNN，在conv4后面是roipooling层，对两个关注的patch点及整个人进行roipooling，concat后得到结果</li>
<li><img src="/images/PAR/DeepCAMP5.png" alt="DeepCAMP5.png"></li>
<li>大幅超过了PANDA</li>
</ul>
<h3 id="PGDM"><a href="#PGDM" class="headerlink" title="PGDM"></a>PGDM</h3><ul>
<li>paper <a href="http://dangweili.github.io/misc/pdfs/icme18.pdf" target="_blank" rel="noopener">POSE GUIDED DEEP MODEL FOR PEDESTRIAN ATTRIBUTE RECOGNITION IN SURVEILLANCE SCENARIOS</a></li>
<li><img src="/images/PAR/PGDM1.png" alt="PGDM1.png"></li>
<li>整体思路和16年的ARAP基本一致，在网络设计上做了改进</li>
<li><img src="/images/PAR/PGDM2.png" alt="PGDM2.png"></li>
<li>Lm:MainNet分类loss*热力weight </li>
<li>Lr:关键点回归smooth-l1loss </li>
<li>Lp:fuse后分类loss</li>
<li><img src="/images/PAR/PGDM3.png" alt="PGDM3.png"></li>
<li><img src="/images/PAR/PGDM4.png" alt="PGDM4.png"></li>
<li>展示了PETA RAP上对HP-Net的全方位压制</li>
</ul>
<h3 id="DHC"><a href="#DHC" class="headerlink" title="DHC"></a>DHC</h3><ul>
<li>paper <a href="http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2016_human.pdf" target="_blank" rel="noopener">Human Attribute Recognition by Deep Hierarchical Contexts</a></li>
<li>发布了 WIDER Attribute dataset，多人多属性数据集</li>
<li><img src="/images/PAR/DHC1.png" alt="DHC1.png"></li>
<li>对于一张input，将其经过高斯金字塔处理，一起送入网络。得到 1.目标人整体 2.目标人各部分检测 3.多尺度各部件近邻特征 4.全图</li>
<li><img src="/images/PAR/DHC2.png" alt="DHC2.png"></li>
<li>连乘概率得到最终结果</li>
</ul>
<h3 id="LGNet"><a href="#LGNet" class="headerlink" title="LGNet"></a>LGNet</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1808.09102.pdf" target="_blank" rel="noopener">Localization guided learning for pedestrian attribute recognition</a></li>
<li>git <a href="https://github.com/lpzjerry/Pedestrian-Attribute-LGNet" target="_blank" rel="noopener">https://github.com/lpzjerry/Pedestrian-Attribute-LGNet</a></li>
<li><img src="/images/PAR/LGNET1.png" alt="LGNET1.png"></li>
<li>介绍地很好，a.原图 b.按预设模板直接分块干 c.基于关键点 d.激活图方法(类似attention) e.依赖定位的方法</li>
<li><img src="/images/PAR/LGNET2.png" alt="LGNET2.png"></li>
<li>1.上面的是全局分支，得到属性  </li>
<li>2.1 特征层卷积得到激活图，联合Edge Boxes结果对结果进行多尺度多比例扩展，得到分类层特征  2.2 卷积 + Edge Boxes结果 进行roipooling得到基于边缘框的特征  2.3 联合Edge Boxes特征和分类层特征得到最终属性结果 </li>
<li>3 联合全局分支和Edge Boxes激活图分支，得到最终结果</li>
<li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2014/09/ZitnickDollarECCV14edgeBoxes.pdf" target="_blank" rel="noopener">Edge Boxes</a></li>
<li><img src="/images/PAR/LGNET3.png" alt="LGNET3.png"></li>
</ul>
<h2 id="Sequential-Prediction-based-Method"><a href="#Sequential-Prediction-based-Method" class="headerlink" title="Sequential Prediction based Method"></a>Sequential Prediction based Method</h2><h3 id="CNN-RNN"><a href="#CNN-RNN" class="headerlink" title="CNN-RNN"></a>CNN-RNN</h3><ul>
<li>paper <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Wang_CNN-RNN_A_Unified_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Cnn-rnn: A unified framework for multi-label image classification</a></li>
<li><img src="/images/PAR/CNNRNN1.png" alt="CNNRNN1.png"></li>
<li>提出了用 联合嵌入空间对标签关联关系的学习 The framework learns a joint embedding space to characterize the image-label relationship as well as label dependency</li>
<li><img src="/images/PAR/CNNRNN2.png" alt="CNNRNN2.png"></li>
<li>主要是通过标签隐向量间的关联关系加强结果，本文不是行人属性专门的文章，没有对应的结果</li>
</ul>
<h3 id="JRL"><a href="#JRL" class="headerlink" title="JRL"></a>JRL</h3><ul>
<li>paper <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Wang_Attribute_Recognition_by_ICCV_2017_paper.pdf" target="_blank" rel="noopener">Attribute recognition by joint recurrent learning of context and correlation</a></li>
<li><img src="/images/PAR/JRL1.png" alt="JRL1.png"></li>
<li>一方面是通过LSTM改善暴力切片分析中的上下文信息的关联性，另一方面使用了类似CNN-RNN类似想法的属性关联关系的LSTM，loss就是常规的CE</li>
<li><img src="/images/PAR/JRL2.png" alt="JRL2.png"></li>
<li>结果上看，相对于CNN-RNN的各种改版有较大优势</li>
</ul>
<h3 id="GRL"><a href="#GRL" class="headerlink" title="GRL"></a>GRL</h3><ul>
<li>paper <a href="https://www.ijcai.org/proceedings/2018/0441.pdf" target="_blank" rel="noopener">Grouping attribute recognition for pedestrian with joint recurrent learning</a></li>
<li>git <a href="https://github.com/slf12/GRLModel" target="_blank" rel="noopener">https://github.com/slf12/GRLModel</a></li>
<li><img src="/images/PAR/GRL1.png" alt="GRL1.png"></li>
<li>一个分支通过FCN生成骨骼点，然后通过区域生成层生成 头 上身 下身 三个区域，生成特征；另一个分支直接原图进入backbone得到全身特征，再对应FC出几个分支。通过LSTM对特征进行相互关联，最终得到结果</li>
<li><img src="/images/PAR/GRL2.png" alt="GRL2.png"></li>
<li>loss上带了正项的权重用于解决标签不平衡的问题</li>
<li><img src="/images/PAR/GRL3.png" alt="GRL3.png"></li>
<li>GRL但模型干过了JRL ensemble的结果，主要优势猜测还是骨骼点带来的更精准的语义信息比起JRL的暴力分割</li>
</ul>
<h3 id="JCM"><a href="#JCM" class="headerlink" title="JCM"></a>JCM</h3><ul>
<li>paper <a href="https://arxiv.org/pdf/1811.08115.pdf" target="_blank" rel="noopener">Sequence-based person attribute recognition with joint ctc-attention model </a></li>
<li><img src="/images/PAR/JCM1.png" alt="JCM1.png"></li>
<li><img src="/images/PAR/JCM2.png" alt="JCM2.png"></li>
<li>很简略的overview和structure，主要的contribution在于CTC loss和attention model</li>
<li><img src="/images/PAR/JCM3.png" alt="JCM3.png"></li>
<li>CTC loss是联立条件概率loss，是通过关联关系提高标签准确率的思路</li>
<li><img src="/images/PAR/JCM4.png" alt="JCM4.png"></li>
<li>没有单独画图，只有这一段话说明attention model，感觉不是很清楚</li>
<li><img src="/images/PAR/JCM5.png" alt="JCM5.png"></li>
<li><img src="/images/PAR/JCM6.png" alt="JCM6.png"></li>
<li>不仅在PETA上表现优秀，在re-id上同样有不俗的表现，同时也用实验证实了re-id任务和attention recognition任务的相辅相成</li>
<li>全文很有借鉴意义，但是attention model没有非常详细的说明，而且没有开源代码，是遗憾</li>
</ul>
<h3 id="RCRA"><a href="#RCRA" class="headerlink" title="RCRA"></a>RCRA</h3><ul>
<li>paper <a href="http://doc.startdt.net/download/attachments/37640056/AAAI2019-Recurrent%20Attention%20Model%20for%20Pedestrian%20Attribute%20Recognition.pdf?version=1&amp;modificationDate=1571726400000&amp;api=v2" target="_blank" rel="noopener">Recurrent attention model for pedestrian attribute recognition</a></li>
<li><img src="/images/PAR/RCRA1.png" alt="RCRA1.png"></li>
<li><img src="/images/PAR/RCRA2.png" alt="RCRA2.png"></li>
<li>上来先介绍了 ConvLSTM，和LSTM的主要区别是用 Conv 取代 Linear 以保留spatial 信息</li>
<li><img src="/images/PAR/RCRA3.png" alt="RCRA3.png"></li>
<li>这是作者提出的第一个网络结果叫做 RC ，是一个常规的recurrent 结构。作者设计这个结构的初衷是从中级别卷积特征图中去挖掘标签相关性（A Recurrent Convolutional (RC) framework is proposed to mine the attribute correlations from mid-level convolutional feature maps of attribute groups.）</li>
<li><img src="/images/PAR/RCRA4.png" alt="RCRA4.png"></li>
<li>这是作者提出的第二个网络称之为 RA，和RC非常相似。作者设计这个结构的初衷是希望同时在组间和组内的attention机制能帮助属性识别（And a Recurrent Attention (RA) framework is formulated to recognise pedestrian attributes by group step by step in order to pay attention to both the intra-group and inter-group attention relationship.）</li>
<li><img src="/images/PAR/RCRA5.png" alt="RCRA5.png"></li>
<li>效果都挺不错的，在不同的指标下RC RA各有千秋</li>
<li><img src="/images/PAR/RCRA6.png" alt="RCRA6.png"></li>
<li>这个图的意思是，预测label的顺序，实验证明，从全局到局部的预测顺序比随机预测的效果要好</li>
</ul>
<hr>
<h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><hr>
<h2 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h2><ul>
<li><img src="/images/PAR/dataset_overview.png" alt="dataset_overview.png"></li>
<li>按学术界喜爱排序<ol>
<li>PETA RAP RAP2.0(19年新出，新的paper会用)  PA-100K</li>
<li>Market-1501 DukeMTMC (主要用于联合reid使用)</li>
<li>其他</li>
</ol>
</li>
</ul>
<h2 id="PETA"><a href="#PETA" class="headerlink" title="PETA"></a>PETA</h2><ul>
<li>home <a href="http://mmlab.ie.cuhk.edu.hk/projects/PETA.html" target="_blank" rel="noopener">http://mmlab.ie.cuhk.edu.hk/projects/PETA.html</a> （港中文信息工程系14发布）</li>
<li>paper <a href="http://mmlab.ie.cuhk.edu.hk/projects/PETA_files/Pedestrian%20Attribute%20Recognition%20At%20Far%20Distance.pdf" target="_blank" rel="noopener">http://mmlab.ie.cuhk.edu.hk/projects/PETA_files/Pedestrian%20Attribute%20Recognition%20At%20Far%20Distance.pdf</a></li>
<li>download(drop box需要翻墙) <a href="https://www.dropbox.com/s/52ylx522hwbdxz6/PETA.zip?dl=0" target="_blank" rel="noopener">https://www.dropbox.com/s/52ylx522hwbdxz6/PETA.zip?dl=0</a></li>
<li>The capability of recognizing pedestrian attributes, such as gender and clothing style, at far distance, is of practical interest in far-view video surveillance scenarios where face and body close-shots are hardly available. We make two contributions in this paper. First, we release a new pedestrian attribute dataset, which is by far the largest and most diverse of its kind. We show that the large-scale dataset facilitates the learning of robust attribute detectors with good generalization performance. Second, we present the benchmark performance by SVM-based method and propose an alternative approach that exploits context of neighboring pedestrian images for improved attribute inference.</li>
<li><img src="/images/PAR/PETA1.png" alt="PETA1.png"></li>
<li><img src="/images/PAR/PETA2.png" alt="PETA2.png"></li>
</ul>
<h2 id="RAP-Richly-Annotated-Pedestrian"><a href="#RAP-Richly-Annotated-Pedestrian" class="headerlink" title="RAP Richly Annotated Pedestrian"></a>RAP Richly Annotated Pedestrian</h2><ul>
<li>home(无法正常访问) <a href="http://rap.idealtest.org/" target="_blank" rel="noopener">http://rap.idealtest.org/</a></li>
<li>git <a href="https://github.com/dangweili/RAP" target="_blank" rel="noopener">https://github.com/dangweili/RAP</a></li>
<li>1.0-paper <a href="https://arxiv.org/pdf/1603.07054.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.07054.pdf</a><ul>
<li>申请数据集的pdf RAP V1.0 Database License Agreement.pdf</li>
<li>申请数据集邮箱 jiajian2018@ia.ac.cn </li>
<li>RAP has in total 41,585 pedestrian samples, each of which is annotated with 72 attributes as well as viewpoints, occlusions, body parts information. </li>
</ul>
</li>
<li>2.0-paper <a href="https://ieeexplore.ieee.org/document/8510891" target="_blank" rel="noopener">https://ieeexplore.ieee.org/document/8510891</a><ul>
<li>申请数据集的pdf RAP V2.0 Database License Agreement.pdf</li>
<li>申请数据集邮箱 jiajian2018@ia.ac.cn </li>
<li>RAP is a large-scale dataset which contains 84928 images with 72 types of attributes and additional tags of viewpoint, occlusion, body parts, and 2589 person identities.</li>
</ul>
</li>
</ul>
<h2 id="PA-100K"><a href="#PA-100K" class="headerlink" title="PA-100K"></a>PA-100K</h2><ul>
<li>from sensetime</li>
<li>paper <a href="https://arxiv.org/pdf/1709.09930.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1709.09930.pdf</a></li>
<li>git <a href="https://github.com/xh-liu/HydraPlus-Net" target="_blank" rel="noopener">https://github.com/xh-liu/HydraPlus-Net</a></li>
<li>download-baiduyun <a href="https://pan.baidu.com/s/1l-5a__OTwZVkhm_A16HraQ#list/path=%2F" target="_blank" rel="noopener">https://pan.baidu.com/s/1l-5a__OTwZVkhm_A16HraQ#list/path=%2F</a></li>
<li>download-googledrive <a href="https://drive.google.com/drive/folders/0B5_Ra3JsEOyOUlhKM0VPZ1ZWR2M" target="_blank" rel="noopener">https://drive.google.com/drive/folders/0B5_Ra3JsEOyOUlhKM0VPZ1ZWR2M</a></li>
<li>we construct a new large-scale pedestrian attribute (PA) dataset named as PA-100K with 100, 000 pedestrian images from 598 scenes, and therefore offer a superiorly comprehensive dataset for pedestrian attribute recognition. To our best knowledge, it is to-date the largest dataset for pedestrian attribute recognition.</li>
</ul>
<h2 id="Market-1501-Attribute-amp-DukeMTMC-attribute"><a href="#Market-1501-Attribute-amp-DukeMTMC-attribute" class="headerlink" title="Market-1501 Attribute &amp; DukeMTMC-attribute"></a>Market-1501 Attribute &amp; DukeMTMC-attribute</h2><ul>
<li>paper <a href="https://arxiv.org/pdf/1703.07220.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1703.07220.pdf</a></li>
<li>We have manually labeled a set of pedestrian attributes for the Market-1501 dataset and the DukeMTMC-reID dataset.</li>
<li>download Market-1501 <a href="https://drive.google.com/file/d/1kbDAPetylhb350LX3EINoEtFsXeXB0uW/view" target="_blank" rel="noopener">https://drive.google.com/file/d/1kbDAPetylhb350LX3EINoEtFsXeXB0uW/view</a></li>
<li>download-annotation-git <a href="https://github.com/vana77/Market-1501_Attribute" target="_blank" rel="noopener">https://github.com/vana77/Market-1501_Attribute</a> (The annotations are contained in the file market_attribute.mat. “gallery_market.mat” is one prediction example. Then download the code “evaluate_market_attribute.m” in this repository, change the image path and run it to evaluate.)</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">attribute</th>
<th style="text-align:center">representation in file</th>
<th style="text-align:center">label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">gender</td>
<td style="text-align:center">gender</td>
<td style="text-align:center">male(1), female(2)</td>
</tr>
<tr>
<td style="text-align:center">hair length</td>
<td style="text-align:center">hair</td>
<td style="text-align:center">short hair(1), long hair(2)</td>
</tr>
<tr>
<td style="text-align:center">sleeve length</td>
<td style="text-align:center">up</td>
<td style="text-align:center">long sleeve(1), short sleeve(2)</td>
</tr>
<tr>
<td style="text-align:center">length of lower-body clothing</td>
<td style="text-align:center">down</td>
<td style="text-align:center">long lower body clothing(1), short(2)</td>
</tr>
<tr>
<td style="text-align:center">type of lower-body clothing</td>
<td style="text-align:center">clothes</td>
<td style="text-align:center">dress(1), pants(2)</td>
</tr>
<tr>
<td style="text-align:center">wearing hat</td>
<td style="text-align:center">hat</td>
<td style="text-align:center">no(1), yes(2)</td>
</tr>
<tr>
<td style="text-align:center">carrying backpack</td>
<td style="text-align:center">backpack</td>
<td style="text-align:center">no(1), yes(2)</td>
</tr>
<tr>
<td style="text-align:center">carrying bag</td>
<td style="text-align:center">bag</td>
<td style="text-align:center">no(1), yes(2)</td>
</tr>
<tr>
<td style="text-align:center">carrying handbag</td>
<td style="text-align:center">handbag</td>
<td style="text-align:center">no(1), yes(2)</td>
</tr>
<tr>
<td style="text-align:center">age</td>
<td style="text-align:center">age</td>
<td style="text-align:center">young(1), teenager(2), adult(3), old(4)</td>
</tr>
<tr>
<td style="text-align:center">8 color of upper-body clothing</td>
<td style="text-align:center">upblack, upwhite, upred, uppurple, upyellow, upgray, upblue, upgreen</td>
<td style="text-align:center">no(1), yes(2)</td>
</tr>
<tr>
<td style="text-align:center">9 color of lower-body clothing</td>
<td style="text-align:center">downblack, downwhite, downpink, downpurple, downyellow, downgray, downblue, downgreen,downbrown</td>
<td style="text-align:center">no(1), yes(2)</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>download  DukeMTMC baiduyun <a href="https://pan.baidu.com/s/1jS0XM7Var5nQGcbf9xUztw" target="_blank" rel="noopener">https://pan.baidu.com/s/1jS0XM7Var5nQGcbf9xUztw</a> (密码 bhbh)</li>
<li>download DukeMTMC Google Drive <a href="https://drive.google.com/open?id=1jjE85dRCMOgRtvJ5RQV9-Afs-2_5dY3O" target="_blank" rel="noopener">https://drive.google.com/open?id=1jjE85dRCMOgRtvJ5RQV9-Afs-2_5dY3O</a></li>
<li>download-annotation-git <a href="https://github.com/vana77/DukeMTMC-attribute" target="_blank" rel="noopener">https://github.com/vana77/DukeMTMC-attribute</a> （The annotations are contained in the file duke_attribute.mat.）</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">attribute</th>
<th style="text-align:center">representation in file</th>
<th style="text-align:center">label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">gender</td>
<td style="text-align:center">gender</td>
<td style="text-align:center">male(1), female(2)</td>
</tr>
<tr>
<td style="text-align:center">length of upper-body clothing</td>
<td style="text-align:center">top</td>
<td style="text-align:center">short upper body clothing(1), long(2)</td>
</tr>
<tr>
<td style="text-align:center">wearing boots</td>
<td style="text-align:center">boots</td>
<td style="text-align:center">no(1), yes(2)</td>
</tr>
<tr>
<td style="text-align:center">wearing hat</td>
<td style="text-align:center">hat</td>
<td style="text-align:center">no(1), yes(2)</td>
</tr>
<tr>
<td style="text-align:center">carrying backpack</td>
<td style="text-align:center">backpack</td>
<td style="text-align:center">no(1), yes(2)</td>
</tr>
<tr>
<td style="text-align:center">carrying bag</td>
<td style="text-align:center">bag</td>
<td style="text-align:center">no(1), yes(2)</td>
</tr>
<tr>
<td style="text-align:center">carrying handbag</td>
<td style="text-align:center">handbag</td>
<td style="text-align:center">no(1), yes(2)</td>
</tr>
<tr>
<td style="text-align:center">color of shoes</td>
<td style="text-align:center">shoes</td>
<td style="text-align:center">dark(1), light(2)</td>
</tr>
<tr>
<td style="text-align:center">8 color of upper-body clothing</td>
<td style="text-align:center">upblack, upwhite, upred, uppurple, upgray, upblue, upgreen, upbrown</td>
<td style="text-align:center">no(1), yes(2)</td>
</tr>
<tr>
<td style="text-align:center">7 color of lower-body clothing</td>
<td style="text-align:center">downblack, downwhite, downred, downgray, downblue, downgreen, downbrown</td>
<td style="text-align:center">no(1), yes(2)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="WIDER"><a href="#WIDER" class="headerlink" title="WIDER"></a>WIDER</h2><ul>
<li>home <a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERAttribute.html" target="_blank" rel="noopener">http://mmlab.ie.cuhk.edu.hk/projects/WIDERAttribute.html</a></li>
<li>paper <a href="http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2016_human.pdf" target="_blank" rel="noopener">http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2016_human.pdf</a></li>
<li>download-images <a href="https://drive.google.com/file/d/0B-PXtfvNMLanWEVCaHZnR0RHSlE/view" target="_blank" rel="noopener">https://drive.google.com/file/d/0B-PXtfvNMLanWEVCaHZnR0RHSlE/view</a></li>
<li>download-annotation <a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERAttribute_files/wider_attribute_annotation.zip" target="_blank" rel="noopener">http://mmlab.ie.cuhk.edu.hk/projects/WIDERAttribute_files/wider_attribute_annotation.zip</a></li>
<li>WIDER Attribute is a large-scale human attribute dataset. It contains 13789 images belonging to 30 scene categories, and 57524 human bounding boxes each annotated with 14 binary attributes.</li>
<li><img src="/images/PAR/WIDER1.png" alt="WIDER1.png"></li>
</ul>
]]></content>
      <categories>
        <category>cv</category>
      </categories>
      <tags>
        <tag>cv</tag>
        <tag>PAR</tag>
      </tags>
  </entry>
  <entry>
    <title>Human Parsing Survey</title>
    <url>/2020/02/22/Human_Parsing_Survey/</url>
    <content><![CDATA[<p>梳理 Human Parsing(人体解析) 相关介绍，数据集，算法<br><a id="more"></a><br><!-- toc --></p>
<hr>
<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><hr>
<h2 id="Foreword"><a href="#Foreword" class="headerlink" title="Foreword"></a>Foreword</h2><ul>
<li>人体解析(Human Parsing)是细粒度的语义分割任务，旨在识别像素级别的人类图像的组成部分（例如，身体部位和服装）。</li>
</ul>
<h2 id="What-is-Human-Parsing"><a href="#What-is-Human-Parsing" class="headerlink" title="What is Human Parsing"></a>What is Human Parsing</h2><ul>
<li>different from pedestrian segmentation<ul>
<li>行人分割任务关注从图中抠出像素级的行人，目标可以是单人或者多人，往往是多人</li>
<li>人体解析关注将身体各部分像素级抠出，目标往往是单人</li>
</ul>
</li>
<li>semantic understanding of person<ul>
<li>像素级理解人体</li>
</ul>
</li>
<li>mul-label segmentation track<ul>
<li>是多类别分割任务</li>
</ul>
</li>
</ul>
<h2 id="Usage-of-Human-Parsing"><a href="#Usage-of-Human-Parsing" class="headerlink" title="Usage of Human Parsing"></a>Usage of Human Parsing</h2><ul>
<li>pedestrian attribution <ul>
<li>more precising than label learning when using as a front method of pedestrian attribute learning</li>
<li>现在行人属性学习有两种主思路<ul>
<li>mul-label learning<ul>
<li>使用标签对全图直接进行分类，使用attention进行unsupervised learning</li>
<li>优点：打标成本低，方便大规模使用</li>
<li>缺点：可靠性有待商榷</li>
</ul>
</li>
<li>mul-task learning <ul>
<li>使用人体解析作为前置，扣出后进行分析</li>
<li>优点：可靠，后续操作灵活，且自带粗略属性</li>
<li>缺点：打标成本极高</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Clothing Recommend<ul>
<li>Human Parsing 能够扣出<br>  -Hat<br>  -Hair<br>  -Glove<br>  -Sunglasses<br>  -Upper-clothes<br>  -Dress<br>  -Coat<br>  -Socks<br>  -Pants<br>  -Jumpsuits<br>  -Scarf<br>  -Skirt</li>
<li>在这个任务的基础上服饰的进一步解析推荐都是可期的</li>
</ul>
</li>
<li>pose estimation <ul>
<li>human parsing 能够扣出<ul>
<li>Face</li>
<li>Left-arm</li>
<li>Right-arm</li>
<li>Left-leg</li>
<li>Right-leg</li>
<li>Left-shoe</li>
<li>Right-shoe</li>
</ul>
</li>
<li>理论上是可以拿来做pose estimation和single frame的action recognition</li>
<li>有些数据集例如 LIP 同时拥有 人体关键点 和 人体解析 的标注，联合优化目前做的人不多，理论上可行，有待研究</li>
</ul>
</li>
<li>Re-ID<ul>
<li>使用 human parsing 做对其然后进行re-id</li>
<li>有人这么玩过效果不错但是消耗资源大，成本也高</li>
</ul>
</li>
</ul>
<hr>
<h1 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h1><hr>
<h2 id="JPPNet"><a href="#JPPNet" class="headerlink" title="JPPNet"></a>JPPNet</h2><ul>
<li>paper <a href="https://arxiv.org/pdf/1804.01984.pdf" target="_blank" rel="noopener">Look into Person: Joint Body Parsing &amp; Pose Estimation Network and A New Benchmark</a></li>
<li>github <a href="https://github.com/Engineering-Course/LIP_JPPNet" target="_blank" rel="noopener">https://github.com/Engineering-Course/LIP_JPPNet</a> </li>
<li><img src="/images/human_parsing/JPPNET1.png" alt="JPPNET1.png"></li>
<li><img src="/images/human_parsing/JPPNET2.png" alt="JPPNET2.png"></li>
<li>本文提出了 重量级的 human parsing benchmark — LIP</li>
<li>从数据集内容丰富度上看，之前的数据集中，MPII 和 LSP 就只有线，ATR就站着的，Pascaljiu 6种标，而LIP啥都有</li>
<li>从数据集本身数量上看， LIP也领先其他数据集许多</li>
<li>文章中提出了两种结构 JPPNet 以及 没有pose帮助的 SS-JPPNet，来看主结构</li>
<li><img src="/images/human_parsing/JPPNET3.png" alt="JPPNET3.png"></li>
<li><p>主要思想就是，共享backbone，fuse结果进行refine，具体是如何做的呢</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#backbone中提取对应特征 </span></span><br><span class="line">resnet_fea_100 = net_100.layers[<span class="string">'res4b22_relu'</span>]</span><br><span class="line">parsing_fea1_100 = net_100.layers[<span class="string">'res5d_branch2b_parsing'</span>]</span><br><span class="line">parsing_out1_100 = net_100.layers[<span class="string">'fc1_human'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#backbone中提取对应特征 </span></span><br><span class="line">resnet_fea_100 = net_100.layers[<span class="string">'res4b22_relu'</span>]</span><br><span class="line">parsing_fea1_100 = net_100.layers[<span class="string">'res5d_branch2b_parsing'</span>]</span><br><span class="line">parsing_out1_100 = net_100.layers[<span class="string">'fc1_human'</span>]</span><br><span class="line">    </span><br><span class="line"><span class="comment">#fc1_human是ASPP的output，出未refine的parsing结果。下面是源代码，是tf1.*的写法</span></span><br><span class="line">(self.feed(<span class="string">'res5b_relu'</span>,<span class="string">'bn5c_branch2c'</span>)</span><br><span class="line">.add(name=<span class="string">'res5c'</span>)</span><br><span class="line">.relu(name=<span class="string">'res5c_relu'</span>)</span><br><span class="line">.atrous_conv(<span class="number">3</span>, <span class="number">3</span>, n_classes, <span class="number">6</span>, padding=<span class="string">'SAME'</span>, relu=<span class="literal">False</span>, name=<span class="string">'fc1_human_c0'</span>))</span><br><span class="line">    </span><br><span class="line">(self.feed(<span class="string">'res5c_relu'</span>)</span><br><span class="line">.atrous_conv(<span class="number">3</span>, <span class="number">3</span>, n_classes, <span class="number">12</span>, padding=<span class="string">'SAME'</span>, relu=<span class="literal">False</span>, name=<span class="string">'fc1_human_c1'</span>))</span><br><span class="line"></span><br><span class="line">(self.feed(<span class="string">'res5c_relu'</span>)</span><br><span class="line">.atrous_conv(<span class="number">3</span>, <span class="number">3</span>, n_classes, <span class="number">18</span>, padding=<span class="string">'SAME'</span>, relu=<span class="literal">False</span>, name=<span class="string">'fc1_human_c2'</span>))</span><br><span class="line"></span><br><span class="line">(self.feed(<span class="string">'res5c_relu'</span>)</span><br><span class="line">.atrous_conv(<span class="number">3</span>, <span class="number">3</span>, n_classes, <span class="number">24</span>, padding=<span class="string">'SAME'</span>, relu=<span class="literal">False</span>, name=<span class="string">'fc1_human_c3'</span>))</span><br><span class="line"> </span><br><span class="line">(self.feed(<span class="string">'fc1_human_c0'</span>,</span><br><span class="line"><span class="string">'fc1_human_c1'</span>,</span><br><span class="line"><span class="string">'fc1_human_c2'</span>,</span><br><span class="line"><span class="string">'fc1_human_c3'</span>)</span><br><span class="line">.add(name=<span class="string">'fc1_human'</span>))</span><br><span class="line"> </span><br><span class="line"><span class="comment">#再结合posenet进行refine</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pose_net</span><span class="params">(image, name)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(name) <span class="keyword">as</span> scope:</span><br><span class="line">        is_BN = <span class="literal">False</span></span><br><span class="line">        pose_conv1 = conv2d(image, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, relu=<span class="literal">True</span>, bn=is_BN, name=<span class="string">'pose_conv1'</span>)</span><br><span class="line">        pose_conv2 = conv2d(pose_conv1, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, relu=<span class="literal">True</span>, bn=is_BN, name=<span class="string">'pose_conv2'</span>)</span><br><span class="line">        pose_conv3 = conv2d(pose_conv2, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, relu=<span class="literal">True</span>, bn=is_BN, name=<span class="string">'pose_conv3'</span>)</span><br><span class="line">        pose_conv4 = conv2d(pose_conv3, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, relu=<span class="literal">True</span>, bn=is_BN, name=<span class="string">'pose_conv4'</span>)</span><br><span class="line">        pose_conv5 = conv2d(pose_conv4, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, relu=<span class="literal">True</span>, bn=is_BN, name=<span class="string">'pose_conv5'</span>)</span><br><span class="line">        pose_conv6 = conv2d(pose_conv5, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, relu=<span class="literal">True</span>, bn=is_BN, name=<span class="string">'pose_conv6'</span>)</span><br><span class="line">    </span><br><span class="line">        pose_conv7 = conv2d(pose_conv6, <span class="number">512</span>, <span class="number">1</span>, <span class="number">1</span>, relu=<span class="literal">True</span>, bn=is_BN, name=<span class="string">'pose_conv7'</span>)</span><br><span class="line">        pose_conv8 = conv2d(pose_conv7, <span class="number">16</span>, <span class="number">1</span>, <span class="number">1</span>, relu=<span class="literal">False</span>, bn=is_BN, name=<span class="string">'pose_conv8'</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> pose_conv8, pose_conv6   <span class="comment">#FCN结果，context</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pose_refine</span><span class="params">(pose, parsing, pose_fea, name)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(name) <span class="keyword">as</span> scope:</span><br><span class="line">        is_BN = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># 1*1 convolution remaps the heatmaps to match the number of channels of the intermediate features.</span></span><br><span class="line">        pose = conv2d(pose, <span class="number">128</span>, <span class="number">1</span>, <span class="number">1</span>, relu=<span class="literal">True</span>, bn=is_BN, name=<span class="string">'pose_remap'</span>)</span><br><span class="line">        parsing = conv2d(parsing, <span class="number">128</span>, <span class="number">1</span>, <span class="number">1</span>, relu=<span class="literal">True</span>, bn=is_BN, name=<span class="string">'parsing_remap'</span>)</span><br><span class="line">        <span class="comment"># concat </span></span><br><span class="line">        pos_par = tf.concat([pose, parsing, pose_fea], <span class="number">3</span>)</span><br><span class="line">        conv1 = conv2d(pos_par, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, relu=<span class="literal">True</span>, bn=is_BN, name=<span class="string">'conv1'</span>)</span><br><span class="line">        conv2 = conv2d(conv1, <span class="number">256</span>, <span class="number">5</span>, <span class="number">1</span>, relu=<span class="literal">True</span>, bn=is_BN, name=<span class="string">'conv2'</span>)</span><br><span class="line">        conv3 = conv2d(conv2, <span class="number">256</span>, <span class="number">7</span>, <span class="number">1</span>, relu=<span class="literal">True</span>, bn=is_BN, name=<span class="string">'conv3'</span>)</span><br><span class="line">        conv4 = conv2d(conv3, <span class="number">256</span>, <span class="number">9</span>, <span class="number">1</span>, relu=<span class="literal">True</span>, bn=is_BN, name=<span class="string">'conv4'</span>)</span><br><span class="line">        conv5 = conv2d(conv4, <span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>, relu=<span class="literal">True</span>, bn=is_BN, name=<span class="string">'conv5'</span>)</span><br><span class="line">        conv6 = conv2d(conv5, <span class="number">16</span>, <span class="number">1</span>, <span class="number">1</span>, relu=<span class="literal">False</span>, bn=is_BN, name=<span class="string">'conv6'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> conv6, conv4 <span class="comment">#FCN结果，context</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parsing_refine</span><span class="params">(parsing, pose, parsing_fea, name)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(name) <span class="keyword">as</span> scope:</span><br><span class="line">        is_BN = <span class="literal">False</span></span><br><span class="line">        pose = conv2d(pose, <span class="number">128</span>, <span class="number">1</span>, <span class="number">1</span>, relu=<span class="literal">True</span>, bn=is_BN, name=<span class="string">'pose_remap'</span>)</span><br><span class="line">        parsing = conv2d(parsing, <span class="number">128</span>, <span class="number">1</span>, <span class="number">1</span>, relu=<span class="literal">True</span>, bn=is_BN, name=<span class="string">'parsing_remap'</span>)</span><br><span class="line">      </span><br><span class="line">        par_pos = tf.concat([parsing, pose, parsing_fea], <span class="number">3</span>)</span><br><span class="line">        parsing_conv1 = conv2d(par_pos, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, relu=<span class="literal">True</span>, bn=is_BN, name=<span class="string">'parsing_conv1'</span>)</span><br><span class="line">        parsing_conv2 = conv2d(parsing_conv1, <span class="number">256</span>, <span class="number">5</span>, <span class="number">1</span>, relu=<span class="literal">True</span>, bn=is_BN, name=<span class="string">'parsing_conv2'</span>)</span><br><span class="line">        parsing_conv3 = conv2d(parsing_conv2, <span class="number">256</span>, <span class="number">7</span>, <span class="number">1</span>, relu=<span class="literal">True</span>, bn=is_BN, name=<span class="string">'parsing_conv3'</span>)</span><br><span class="line">        parsing_conv4 = conv2d(parsing_conv3, <span class="number">256</span>, <span class="number">9</span>, <span class="number">1</span>, relu=<span class="literal">True</span>, bn=is_BN, name=<span class="string">'parsing_conv4'</span>)</span><br><span class="line">      </span><br><span class="line">        parsing_conv5 = conv2d(parsing_conv4, <span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>, relu=<span class="literal">True</span>, bn=is_BN, name=<span class="string">'parsing_conv5'</span>)</span><br><span class="line">        parsing_human1 = atrous_conv2d(parsing_conv5, <span class="number">20</span>, <span class="number">3</span>, rate=<span class="number">6</span>, relu=<span class="literal">False</span>, name=<span class="string">'parsing_human1'</span>)</span><br><span class="line">        parsing_human2 = atrous_conv2d(parsing_conv5, <span class="number">20</span>, <span class="number">3</span>, rate=<span class="number">12</span>, relu=<span class="literal">False</span>, name=<span class="string">'parsing_human2'</span>)</span><br><span class="line">        parsing_human3 = atrous_conv2d(parsing_conv5, <span class="number">20</span>, <span class="number">3</span>, rate=<span class="number">18</span>, relu=<span class="literal">False</span>, name=<span class="string">'parsing_human3'</span>)</span><br><span class="line">        parsing_human4 = atrous_conv2d(parsing_conv5, <span class="number">20</span>, <span class="number">3</span>, rate=<span class="number">24</span>, relu=<span class="literal">False</span>, name=<span class="string">'parsing_human4'</span>)</span><br><span class="line">        parsing_human = tf.add_n([parsing_human1, parsing_human2, parsing_human3, parsing_human4], name=<span class="string">'parsing_human'</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> parsing_human, parsing_conv4 <span class="comment">#FCN结果，context</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>总体来说分以下几个步骤</p>
<ol>
<li>pose_out1_100, pose_fea1_100 = pose_net(resnet_fea_100, ‘fc1_pose’)  #先出一次pose结果</li>
<li>pose_out2_100, pose_fea2_100 = pose_refine(pose_out1_100, parsing_out1_100, pose_fea1_100, name=’fc2_pose’)  # 结合第一次parsing结果出第二次pose结果</li>
<li>parsing_out2_100, parsing_fea2_100 = parsing_refine( parsing_out1_100, pose_out1_100, parsing_fea1_100, name=’fc2_parsing’) # 结合第一次pose结果出第二次parsing结果</li>
<li>parsing_out3_100, parsing_fea3_100 = parsing_refine(parsing_out2_100, pose_out2_100, parsing_fea2_100, name=’fc3_parsing’) # 结合第二次pose结果出第三次parsing结果</li>
<li>pose_out3_100, pose_fea3_100 = pose_refine(pose_out2_100, parsing_out2_100, pose_fea2_100, name=’fc3_pose’) # 结合第二次parsing结果出第三次pose结果</li>
</ol>
</li>
<li>从代码上看，和论文里的图略有不同</li>
<li>loss<ul>
<li><img src="/images/human_parsing/JPPNET4.png" alt="JPPNET4.png"></li>
</ul>
</li>
<li>experiment<ul>
<li><img src="/images/human_parsing/JPPNET5.png" alt="JPPNET5.png"></li>
<li>从实验结果上看效果提升很明显，值得一赞</li>
</ul>
</li>
<li>more<ul>
<li>值得一提的是，SS-JPPNet 咋就和DeepLab差不多呢？上图</li>
<li><img src="/images/human_parsing/JPPNET6.png" alt="JPPNET6.png"></li>
<li>通过human parsing的9个部位，取其中心作为关键点进行训练，结果也是意料之中的没什么卵用。猜测是关键点结果还是来自于human parsing的结果，监督信息耦合。结果也是和deeplab相近</li>
</ul>
</li>
</ul>
<h2 id="CE2P"><a href="#CE2P" class="headerlink" title="CE2P"></a>CE2P</h2><ul>
<li>paper <a href="https://arxiv.org/pdf/1809.05996.pdf" target="_blank" rel="noopener">Devil in the Details: Towards Accurate Single and Multiple Human Parsing</a></li>
<li>github <a href="https://github.com/liutinglt/CE2P" target="_blank" rel="noopener">https://github.com/liutinglt/CE2P</a></li>
<li>参考链接 <a href="https://blog.csdn.net/siyue0211/article/details/90927712" target="_blank" rel="noopener">https://blog.csdn.net/siyue0211/article/details/90927712</a></li>
<li>标题很风骚，内容还是很硬核的</li>
<li>人体解析现在主要有两大类解决方案：<ol>
<li>High-resolution Maintenance，这种方法通过获得高分辨率的特征来恢复细节信息。它存在的问题是，由于卷积中的池化操作和卷积中的步长，会让最终生成的特征较小。解决方法是，删除一些下采样操作（max pooling etc.）或从一些低层特征中获取信息。</li>
<li>Context Information Embedding. 这种方法通过捕获丰富的上下文信息来处理多尺度的对象。ASPP和PSP是使用这一方式解决问题的主要结构。</li>
</ol>
</li>
<li>CE2P主要包括三大模块：<ol>
<li>一个高分辨率的embedding 模块，作用是放大特征图以恢复细节</li>
<li>一个全局上下文embedding 模块，作用是编码多尺度的上下文信息</li>
<li>一个边缘感知模块，用于整合对象轮廓的特征，以细化解析预测的边界</li>
</ol>
</li>
<li>Contribution<ol>
<li>作者分析了一些人脸解析方法，验证其有效性。并说明如何使用这些方法来达到更好的效果</li>
<li>作者利用了人脸解析中一些有效的方法，构建了CE2P框架</li>
<li>CE2P框架达到了公开数据集state-of-art的效果</li>
<li>代码开源，可以作为baseline使用</li>
</ol>
</li>
<li>下面来看看结构<ul>
<li><img src="/images/human_parsing/CE2P1.png" alt="CE2P1.png"></li>
<li>图中红色部分是PSPModule   <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PSPModule</span><span class="params">(nn.Module)</span>:</span>  <span class="comment"># Pyramid scene parsing network</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Reference: </span></span><br><span class="line"><span class="string">    Zhao, Hengshuang, et al. *"Pyramid scene parsing network."*</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, out_features=<span class="number">512</span>, sizes=<span class="params">(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>)</span>)</span>:</span></span><br><span class="line">        super(PSPModule, self).__init__()</span><br><span class="line">        self.stages = []</span><br><span class="line">        self.stages = nn.ModuleList([self._make_stage(features, out_features, size) <span class="keyword">for</span> size <span class="keyword">in</span> sizes])</span><br><span class="line">        self.bottleneck = nn.Sequential(</span><br><span class="line">            nn.Conv2d(features+len(sizes)*out_features, out_features, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, dilation=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            InPlaceABNSync(out_features),</span><br><span class="line">            )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_stage</span><span class="params">(self, features, out_features, size)</span>:</span></span><br><span class="line">        prior = nn.AdaptiveAvgPool2d(output_size=(size, size))</span><br><span class="line">        conv = nn.Conv2d(features, out_features, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        bn = InPlaceABNSync(out_features)</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(prior, conv, bn)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, feats)</span>:</span></span><br><span class="line">        h, w = feats.size(<span class="number">2</span>), feats.size(<span class="number">3</span>)</span><br><span class="line">        priors = [ F.interpolate(input=stage(feats), size=(h, w), mode=<span class="string">'bilinear'</span>, align_corners=<span class="literal">True</span>) <span class="keyword">for</span> stage <span class="keyword">in</span> self.stages] + [feats]</span><br><span class="line">        bottle = self.bottleneck(torch.cat(priors, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> bottle</span><br></pre></td></tr></table></figure></li>
<li>黄色部分是 high-res 的module，常规的conv cat操作  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder_Module</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes)</span>:</span></span><br><span class="line">        super(Decoder_Module, self).__init__()</span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">512</span>, <span class="number">256</span>, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            InPlaceABNSync(<span class="number">256</span>)</span><br><span class="line">            )</span><br><span class="line">        self.conv2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">48</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            InPlaceABNSync(<span class="number">48</span>)</span><br><span class="line">            )</span><br><span class="line">        self.conv3 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">304</span>, <span class="number">256</span>, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            InPlaceABNSync(<span class="number">256</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            InPlaceABNSync(<span class="number">256</span>)</span><br><span class="line">            )</span><br><span class="line">        self.conv4 = nn.Conv2d(<span class="number">256</span>, num_classes, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, xt, xl)</span>:</span></span><br><span class="line">        _, _, h, w = xl.size()</span><br><span class="line"></span><br><span class="line">        xt = F.interpolate(self.conv1(xt), size=(h, w), mode=<span class="string">'bilinear'</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">        xl = self.conv2(xl)</span><br><span class="line">        x = torch.cat([xt, xl], dim=<span class="number">1</span>)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        seg = self.conv4(x)</span><br><span class="line">        <span class="keyword">return</span> seg, x</span><br></pre></td></tr></table></figure></li>
<li>绿色部分是本文的亮点 Edge_Module，一方面提取了边缘特征信息，另一方面得到了edge图用于计算loss，从代码上看也是非常普通的做法  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Edge_Module</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,in_fea=[<span class="number">256</span>,<span class="number">512</span>,<span class="number">1024</span>], mid_fea=<span class="number">256</span>, out_fea=<span class="number">2</span>)</span>:</span></span><br><span class="line">        super(Edge_Module, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.conv1 =  nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_fea[<span class="number">0</span>], mid_fea, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            InPlaceABNSync(mid_fea)</span><br><span class="line">            ) </span><br><span class="line">        self.conv2 =  nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_fea[<span class="number">1</span>], mid_fea, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            InPlaceABNSync(mid_fea)</span><br><span class="line">            )  </span><br><span class="line">        self.conv3 =  nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_fea[<span class="number">2</span>], mid_fea, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            InPlaceABNSync(mid_fea)</span><br><span class="line">        )</span><br><span class="line">        self.conv4 = nn.Conv2d(mid_fea,out_fea, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, dilation=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        self.conv5 = nn.Conv2d(out_fea*<span class="number">3</span>,out_fea, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x1, x2, x3)</span>:</span></span><br><span class="line">        _, _, h, w = x1.size()</span><br><span class="line">        </span><br><span class="line">        edge1_fea = self.conv1(x1)</span><br><span class="line">        edge1 = self.conv4(edge1_fea)</span><br><span class="line">        edge2_fea = self.conv2(x2)</span><br><span class="line">        edge2 = self.conv4(edge2_fea)</span><br><span class="line">        edge3_fea = self.conv3(x3)</span><br><span class="line">        edge3 = self.conv4(edge3_fea)        </span><br><span class="line">        </span><br><span class="line">        edge2_fea =  F.interpolate(edge2_fea, size=(h, w), mode=<span class="string">'bilinear'</span>,align_corners=<span class="literal">True</span>) </span><br><span class="line">        edge3_fea =  F.interpolate(edge3_fea, size=(h, w), mode=<span class="string">'bilinear'</span>,align_corners=<span class="literal">True</span>) </span><br><span class="line">        edge2 =  F.interpolate(edge2, size=(h, w), mode=<span class="string">'bilinear'</span>,align_corners=<span class="literal">True</span>)</span><br><span class="line">        edge3 =  F.interpolate(edge3, size=(h, w), mode=<span class="string">'bilinear'</span>,align_corners=<span class="literal">True</span>) </span><br><span class="line"> </span><br><span class="line">        edge = torch.cat([edge1, edge2, edge3], dim=<span class="number">1</span>)</span><br><span class="line">        edge = self.conv5(edge)</span><br><span class="line"></span><br><span class="line">        edge_fea = torch.cat([edge1_fea, edge2_fea, edge3_fea], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> edge, edge_fea</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>Note that the edge annotation used in the edge perceiving module is directly generated from the parsing annotation by extracting border between different semantics.<ul>
<li>仅是使用parsing的annotation，就生成了edge的annotation<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_edge</span><span class="params">(label, edge_width=<span class="number">3</span>)</span>:</span></span><br><span class="line">    h, w = label.shape</span><br><span class="line">    edge = np.zeros(label.shape)    </span><br><span class="line">    <span class="comment"># right</span></span><br><span class="line">    edge_right = edge[<span class="number">1</span>:h, :]</span><br><span class="line">        edge_right[(label[<span class="number">1</span>:h, :] != label[:h - <span class="number">1</span>, :]) &amp; (label[<span class="number">1</span>:h, :] != <span class="number">255</span>)</span><br><span class="line">                  &amp; (label[:h - <span class="number">1</span>, :] != <span class="number">255</span>)] = <span class="number">1</span>  </span><br><span class="line">    <span class="comment"># up</span></span><br><span class="line">    edge_up = edge[:, :w - <span class="number">1</span>]</span><br><span class="line">        edge_up[(label[:, :w - <span class="number">1</span>] != label[:, <span class="number">1</span>:w])</span><br><span class="line">               &amp; (label[:, :w - <span class="number">1</span>] != <span class="number">255</span>)</span><br><span class="line">               &amp; (label[:, <span class="number">1</span>:w] != <span class="number">255</span>)] = <span class="number">1</span>    </span><br><span class="line">    <span class="comment"># upright</span></span><br><span class="line">    edge_upright = edge[:h - <span class="number">1</span>, :w - <span class="number">1</span>]</span><br><span class="line">        edge_upright[(label[:h - <span class="number">1</span>, :w - <span class="number">1</span>] != label[<span class="number">1</span>:h, <span class="number">1</span>:w])</span><br><span class="line">                    &amp; (label[:h - <span class="number">1</span>, :w - <span class="number">1</span>] != <span class="number">255</span>)</span><br><span class="line">                    &amp; (label[<span class="number">1</span>:h, <span class="number">1</span>:w] != <span class="number">255</span>)] = <span class="number">1</span> </span><br><span class="line">    <span class="comment"># bottomright</span></span><br><span class="line">    edge_bottomright = edge[:h - <span class="number">1</span>, <span class="number">1</span>:w]</span><br><span class="line">        edge_bottomright[(label[:h - <span class="number">1</span>, <span class="number">1</span>:w] != label[<span class="number">1</span>:h, :w - <span class="number">1</span>])</span><br><span class="line">                        &amp; (label[:h - <span class="number">1</span>, <span class="number">1</span>:w] != <span class="number">255</span>)</span><br><span class="line">                        &amp; (label[<span class="number">1</span>:h, :w - <span class="number">1</span>] != <span class="number">255</span>)] = <span class="number">1</span>  </span><br><span class="line">    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (edge_width, edge_width))</span><br><span class="line">    edge = cv2.dilate(edge, kernel)</span><br><span class="line">    <span class="keyword">return</span> edge</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>LOSS<ul>
<li><img src="/images/human_parsing/CE2P2.png" alt="CE2P2.png"></li>
<li>CE三连</li>
</ul>
</li>
<li>Experiment<ul>
<li><img src="/images/human_parsing/CE2P3.png" alt="CE2P3.png"></li>
<li>在不适用point的监督信息的情况下mIOU还提升了不少，比deeplab提升近9个点(20%)，在test集上，结果非常惊艳，远超JPP</li>
<li><img src="/images/human_parsing/CE2P4.png" alt="CE2P4.png"></li>
</ul>
</li>
<li>非常强大的结构，非常值得作为baseline使用</li>
</ul>
<h2 id="ACE2P-pp"><a href="#ACE2P-pp" class="headerlink" title="ACE2P-pp"></a>ACE2P-pp</h2><ul>
<li>git <a href="https://github.com/PaddlePaddle/PaddleSeg/tree/release/v0.1.0/contrib/ACE2P" target="_blank" rel="noopener">https://github.com/PaddlePaddle/PaddleSeg/tree/release/v0.1.0/contrib/ACE2P</a></li>
<li>目前的总统山榜首是 paddleseg 的 Augmented Context Embedding with Edge Perceiving(ACE2P)</li>
<li>ACE2P有两个版本，此处版本为rank 1的paddleseg版本，只有inference版本，paddlepaddle是静态图，具体信息的放出也比较有限</li>
<li>改编自 Devil in the Details: Towards Accurate Single and Multiple Human Parsing <a href="https://arxiv.org/abs/1809.05996" target="_blank" rel="noopener">https://arxiv.org/abs/1809.05996</a></li>
<li>结构图 </li>
<li><img src="/images/human_parsing/ACE2P-pp1.jpg" alt="ACE2P1.jpg"></li>
<li>ACE2P模型包含三个分支:<ol>
<li>语义分割分支</li>
<li>边缘检测分支</li>
<li>融合分支</li>
</ol>
</li>
<li>语义分割分支采用resnet101作为backbone,通过Pyramid Scene Parsing Network融合上下文信息以获得更加精确的特征表征</li>
<li>边缘检测分支采用backbone的中间层特征作为输入，预测二值边缘信息</li>
<li>融合分支将语义分割分支以及边缘检测分支的特征进行融合，以获得边缘细节更加准确的分割图像。</li>
<li>分割问题一般采用mIoU作为评价指标，特别引入了IoU loss结合cross-entropy loss以针对性优化这一指标</li>
<li>测试阶段，采用多尺度以及水平翻转的结果进行融合生成最终预测结果</li>
<li>训练阶段，采用余弦退火的学习率策略， 并且在学习初始阶段采用线性warm up</li>
<li>数据预处理方面，保持图片比例并进行随机缩放，随机旋转，水平翻转作为数据增强策略</li>
<li>LIP指标<ul>
<li>该模型在测试尺度为’377,377,473,473,567,567’且水平翻转的情况下，meanIoU为62.63</li>
<li>多模型ensemble后meanIoU为65.18, 居LIP Single-Person Human Parsing Track榜单第一</li>
</ul>
</li>
<li><img src="/images/human_parsing/ACE2P-pp2.jpg" alt="ACE2P2.jpg"></li>
<li>主要思想来自于 CE2P</li>
<li>和CE2P相比，几乎没有太多的变化，仅仅多了从 edge 分支 到fuse分支一条线路</li>
<li>目前的LIP第一，使用paddlepaddle架构，放出了模型和推论脚本，没有训练脚本</li>
</ul>
<h2 id="ACE2P"><a href="#ACE2P" class="headerlink" title="ACE2P"></a>ACE2P</h2><ul>
<li>paper <a href="https://arxiv.org/pdf/1910.09777.pdf" target="_blank" rel="noopener">Self-Correction for Human Parsing</a></li>
<li>该篇文章为ACE2P rank3版本，学术版本，10月22日首次挂在arxiv</li>
<li>git <a href="https://github.com/PeikeLi/Self-Correction-Human-Parsing" target="_blank" rel="noopener">https://github.com/PeikeLi/Self-Correction-Human-Parsing</a></li>
<li>git版本只有inference，不过值得一提的是，由于使用的是pytorch，model是开放的可见的，对比ce2p后发现基本结构基本没有不同，区别在于loss和schp方法</li>
<li><img src="/images/human_parsing/ACE2P1.png" alt="ACE2P1.png"></li>
<li>展示了schp随cycle改进pred的效果</li>
<li><img src="/images/human_parsing/ACE2P2.png" alt="ACE2P2.png"></li>
<li>framework和paddleseg的基本一直，只是将loss function展开了，其中consistency constraint是一个新玩意。其中还有一点没有确定的是，ce2p是明显有三个输出的framework，这里的这个看样子仅有两个输出，有待实验确定</li>
<li><img src="/images/human_parsing/ACE2P3.png" alt="ACE2P3.png"></li>
<li>loss的第一部分是objective loss，是cls loss（bce） + miou loss。文中的[1] The Lovasz-Softmax loss: A tractable surrogate for the optimization of the ´ intersection-over-union measure in neural networks 是针对miou进行优化的loss，git <a href="https://github.com/bermanmaxim/LovaszSoftmax，有兴趣的同学可以点进去看看" target="_blank" rel="noopener">https://github.com/bermanmaxim/LovaszSoftmax，有兴趣的同学可以点进去看看</a></li>
<li><img src="/images/human_parsing/ACE2P4.png" alt="ACE2P4.png"></li>
<li>一致性约束。主要是为了回应他前文所说的<ul>
<li>Second, CE2P only implicitly facilitates the parsing results with the edge predictions by feature-level fusion. There is no explicit constraint to ensure the parsing results maintaining the same geometry shape of the boundary predictions.</li>
</ul>
</li>
<li>这个edge module是否真的是帮助了pred，这里比对了pred中的edge和pred中fuse后gen出的edge，以保证这个fuse是靠谱的</li>
<li><img src="/images/human_parsing/ACE2P5.png" alt="ACE2P5.png"></li>
<li>loss<ul>
<li>We choose the ResNet-set. 101 [12] as the backbone of the feature extractor and use an ImageNet [8] pre-trained weights. Specifically, we fix the first three residual layers and set the stride size of last residual layer to 1 with a dilation rate of 2. In this way, the final output is enlarged to 1/16 resolution size w.r.t the original image. We adopt pyramid scene parsing network [33] as the context encoding module. We use 473 × 473 as the input resolution. Training is done with a total batch size of 36. For our joint loss function, we set the weight of each term as λ1 = 1, λ2 = 1, λ3 = 0.1. The initial learning rate is set as 7e-3 with a linear increasing warm-up strategy for 10 epochs. We train our network for 150 epochs in total for a fair comparison, the first 100 epochs as initialization following 5 cycles each contains 10 epochs of the self-correction process.</li>
</ul>
</li>
<li>训练细节<ul>
<li><img src="/images/human_parsing/ACE2P6.png" alt="ACE2P6.png"></li>
<li>展示了schp的策略，先训练了100个epoch作为Cycle 0 base，然后使用 anneal cosine decay lr策略restart4次，将权重结合，模型效果就会像是ensemble了一样，越来越好</li>
<li><img src="/images/human_parsing/ACE2P7.png" alt="ACE2P7.png"></li>
<li>这个weight aggregation也是很直白的，w0是init的，m=1 w1 = m/(m+1) <em> w0 + 1/(m+1)w = 1/2</em>w0 + 1/2*w，就是一个移动平均</li>
<li>After updating the current model weight with the former optimal one from the last cycle, we forward all the training data for one epoch to re-estimate the statistics of the parameters (i.e. moving average and standard deviation) in all batch normalization [14] layers. During these successive cycles of model aggregation, the network leads to wider model optima as well as improved model’s generalization ability.</li>
<li>作者也提到说模型融合的时候需要一个epoch来使BN层适应</li>
<li><img src="/images/human_parsing/ACE2P8.png" alt="ACE2P8.png"></li>
<li>这个是说，怕这个label坑爹，使用原始的label init y0，使用pred结果更新label，机制和weight一样</li>
<li><img src="/images/human_parsing/ACE2P9.png" alt="ACE2P9.png"></li>
<li>schp的核心算法，每经过一次cycle，更新w，BN的parameter（mean var）不算w，额外更新下，使用新的w计算pred的y，更新y</li>
</ul>
</li>
<li>Experiment<ul>
<li><img src="/images/human_parsing/ACE2P10.png" alt="ACE2P10.png"></li>
<li>ACE2P效果就很棒，加上SCHP，效果超CE2P一大截</li>
<li><img src="/images/human_parsing/ACE2P11.png" alt="ACE2P11.png"></li>
<li>在 Pascal-Person-Part val上表现也同样不俗，使用简单的test技巧后达到了sota</li>
<li><img src="/images/human_parsing/ACE2P12.png" alt="ACE2P12.png"></li>
<li>作者还在LIP上做了不同backbone的SCHP实验，使用SCHP能普涨一个点以上；当使用不同context encoding模块，使用了SCHP后，PSP ，ASPP，OCNet都得到了一个点以上的提升，差距缩小</li>
<li><img src="/images/human_parsing/ACE2P13.png" alt="ACE2P13.png"></li>
<li>说明了3个额外loss的可靠性；说明了schp中模型融合和label的refine都是有效果的</li>
</ul>
</li>
<li>CE2P是正式在semantic segmentation中开启了human parsing分支，ACE2P是大幅改进了性能，都是非常推荐精读细读的文章</li>
</ul>
<hr>
<h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><hr>
<h2 id="LIP"><a href="#LIP" class="headerlink" title="LIP"></a>LIP</h2><ul>
<li>look into person</li>
<li>home <a href="http://sysu-hcp.net/lip/overview.php" target="_blank" rel="noopener">http://sysu-hcp.net/lip/overview.php</a></li>
<li>Human-Cyber-Physical Intelligence Integration Lab of Sun Yat-sen University （中山大学人机物智能融合实验室出品）</li>
<li>Overview<ul>
<li>Look into Person (LIP) is a new large-scale dataset, focus on semantic understanding of person. Following are the detailed descriptions.<ul>
<li>Volume<ul>
<li>The dataset contains 50,000 images with elaborated pixel-wise annotations with 19 semantic human part labels and 2D human poses with 16 key points.</li>
</ul>
</li>
<li>Diversity<ul>
<li>The annotated 50,000 images are cropped person instances from COCO dataset with size larger than 50 * 50.The images collected from the real-world scenarios contain human appearing with challenging poses and views, heavily occlusions, various appearances and low-resolutions. We are working on collecting and annotating more images to increase diversity.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Four Track<ol>
<li>Single Person （Main）<ul>
<li>We have divided images into three sets. 30462 images for training set, 10000 images for validation set and 10000 for testing set.The dataset is available at <a href="https://drive.google.com/drive/folders/0BzvH3bSnp3E9ZW9paE9kdkJtM3M?usp=sharing" target="_blank" rel="noopener">Google Drive</a> and <a href="http://pan.baidu.com/s/1nvqmZBN" target="_blank" rel="noopener">Baidu Drive</a>.</li>
<li>Besides we have another large dataset mentioned in “<a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Liang_Human_Parsing_With_ICCV_2015_paper.html" target="_blank" rel="noopener">Human parsing with contextualized convolutional neural network.” ICCV’15</a>, which focuses on fashion images. You can download the dataset including 17000 images as extra training data.</li>
</ul>
</li>
<li>Multi-Person  (CHIP)<ul>
<li>To stimulate the multiple-human parsing research, we collect the images with multiple person instances to establish the first standard and comprehensive benchmark for instance-level human parsing. Our Crowd Instance-level Human Parsing Dataset (CIHP) contains 28280 training, 5000 validation and 5000 test images, in which there are 38280 multiple-person images in total.</li>
<li>You can also downlod this dataset at <a href="https://drive.google.com/drive/folders/0BzvH3bSnp3E9ZW9paE9kdkJtM3M?usp=sharing" target="_blank" rel="noopener">Google Drive</a> and <a href="http://pan.baidu.com/s/1nvqmZBN" target="_blank" rel="noopener">Baidu Drive</a>.</li>
</ul>
</li>
<li>Video Multi-Person Human Parsing<ul>
<li>VIP(Video instance-level Parsing) dataset, the first video multi-person human parsing benchmark, consists of 404 videos covering various scenarios. For every 25 consecutive frames in each video, one frame is annotated densely with pixel-wise semantic part categories and instance-level identification. There are 21247 densely annotated images in total. We divide these 404 sequences into 304 train sequences, 50 validation sequences and 50 test sequences.</li>
<li>You can also downlod this dataset at <a href="https://1drv.ms/f/s!ArFSFaZzVErwgSHRpiJNJTzgMR8j" target="_blank" rel="noopener">OneDrive</a> and <a href="https://pan.baidu.com/s/18_PVNy7FCh4T74nVzRXbtA" target="_blank" rel="noopener">Baidu Drive</a>.<ul>
<li>VIP_Fine: All annotated images and fine annotations for train and val sets.</li>
<li>VIP_Sequence: 20-frame surrounding each VIP_Fine image (-10 | +10).</li>
<li>VIP_Videos: 404 video sequences of VIP dataset.</li>
</ul>
</li>
</ul>
</li>
<li>Image-based Multi-pose Virtual Try On<ul>
<li>MPV (Multi-Pose Virtual try on) dataset, which consists of 35,687/13,524 person/clothes images, with the resolution of 256x192. Each person has different poses. We split them into the train/test set 52,236/10,544 three-tuples, respectively.</li>
</ul>
</li>
</ol>
</li>
</ul>
<h2 id="MHP"><a href="#MHP" class="headerlink" title="MHP"></a>MHP</h2><ul>
<li>Multi-Human Parsing</li>
<li>home <a href="https://lv-mhp.github.io/" target="_blank" rel="noopener">https://lv-mhp.github.io/</a></li>
<li>Learning and Vision (LV) Group, National University of Singapore (NUS) (新加坡国立大学机器学习与视觉小组)</li>
<li>Statistics<ul>
<li>MHP v1.0<ul>
<li>The MHP v1.0 dataset contains 4,980 images, each with at least two persons (average is 3). We randomly choose 980 images and their corresponding annotations as the testing set. The rest form a training set of 3,000 images and a validation set of 1,000 images. For each instance, 18 semantic categories are defined and annotated except for the “background” category, i.e. “hat”, “hair”, “sunglasses”, “upper clothes”, “skirt”, “pants”, “dress”, “belt”, “left shoe”, “right shoe”, “face”, “left leg”, “right leg”, “left arm”, “right arm”, “bag”, “scarf” and “torso skin”. Each instance has a complete set of annotations whenever the corresponding category appears in the current image.</li>
</ul>
</li>
<li>MHP v2.0<ul>
<li>The MHP v2.0 dataset contains 25,403 images, each with at least two persons (average is 3). We randomly choose 5,000 images and their corresponding annotations as the testing set. The rest form a training set of 15,403 images and a validation set of 5,000 images. For each instance, 58 semantic categories are defined and annotated except for the “background” category, i.e. “cap/hat”, “helmet”, “face”, “hair”, “left- arm”, “right-arm”, “left-hand”, “right-hand”, “protector”, “bikini/bra”, “jacket/windbreaker/hoodie”, “t-shirt”, “polo-shirt”, “sweater”, “sin- glet”, “torso-skin”, “pants”, “shorts/swim-shorts”, “skirt”, “stock- ings”, “socks”, “left-boot”, “right-boot”, “left-shoe”, “right-shoe”, “left- highheel”, “right-highheel”, “left-sandal”, “right-sandal”, “left-leg”, “right-leg”, “left-foot”, “right-foot”, “coat”, “dress”, “robe”, “jumpsuits”, “other-full-body-clothes”, “headwear”, “backpack”, “ball”, “bats”, “belt”, “bottle”, “carrybag”, “cases”, “sunglasses”, “eyewear”, “gloves”, “scarf”, “umbrella”, “wallet/purse”, “watch”, “wristband”, “tie”, “other-accessaries”, “other-upper-body-clothes”, and “other-lower-body-clothes”. Each instance has a complete set of annotations whenever the corresponding category appears in the current image.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Pascal-Person-Part-Dataset"><a href="#Pascal-Person-Part-Dataset" class="headerlink" title="Pascal-Person-Part Dataset"></a>Pascal-Person-Part Dataset</h2><ul>
<li>1,716 images for training and 1,817 for testing</li>
<li>first mentioned paper <a href="https://arxiv.org/pdf/1406.2031.pdf" target="_blank" rel="noopener">Detect What You Can: Detecting and Representing Objects using Holistic Models and Body Parts</a><ul>
<li>没找到下载的地方= =。。标注了part</li>
</ul>
</li>
<li>second paper <a href="https://arxiv.org/pdf/1708.03383.pdf" target="_blank" rel="noopener">Joint Multi-Person Pose Estimation and Semantic Part Segmentation</a><ul>
<li><a href="https://sukixia.github.io/materials/pascal_data.zip" target="_blank" rel="noopener">download link</a></li>
<li>附加标注了key-point</li>
<li>提出了联合学习方法<ul>
<li><img src="/images/human_parsing/Pascal-Person-Part-Dataset.png" alt="Pascal-Person-Part-Dataset.png"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="ATR"><a href="#ATR" class="headerlink" title="ATR"></a>ATR</h2><ul>
<li>project url <a href="http://www.sysu-hcp.net/deep-human-parsing/" target="_blank" rel="noopener">link</a></li>
<li>paper <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Liang_Human_Parsing_With_ICCV_2015_paper.pdf" target="_blank" rel="noopener">Human Parsing with Contextualized Convolutional Neural Network</a></li>
<li>Human parsing is to predict every pixel with 18 labels: face, sunglass, hat, scarf, hair, upperclothes, left-arm, right-arm, belt, pants, left-leg, right-leg, skirt, left-shoe, right-shoe, bag, dress and null. Totally, 7,700 images are included in the ATR dataset [15], 6,000 for training, 1,000 for testing and 700 for validation1 . T</li>
<li>download link <a href="https://github.com/lemondan/HumanParsing-Dataset" target="_blank" rel="noopener">link</a></li>
</ul>
]]></content>
      <categories>
        <category>cv</category>
      </categories>
      <tags>
        <tag>cv</tag>
        <tag>human_parsing</tag>
      </tags>
  </entry>
  <entry>
    <title>Crowd Counting Survey </title>
    <url>/2020/02/22/Crowd_Counting_Survey/</url>
    <content><![CDATA[<p>梳理 Crowd Counting(人群密度估计) 相关介绍，数据集，算法<br><a id="more"></a><br><!-- toc --></p>
<hr>
<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><hr>
<h2 id="Foreword"><a href="#Foreword" class="headerlink" title="Foreword"></a>Foreword</h2><ul>
<li>(mainly forked from <a href="https://github.com/CommissarMa/Crowd_counting_from_scratch" target="_blank" rel="noopener">https://github.com/CommissarMa/Crowd_counting_from_scratch</a>)</li>
<li>Crowd counting has a long research history. About twenty years ago or even earlier, researchers have been interested in developing the method to count the number of pedestrians in the image automatically.</li>
<li>There are mainly three categories of methods to count pedestrians in crowd.<ul>
<li>Pedestrian detector. You can use traditional HOG-based detector or deeplearning-based detector like YOLOs or RCNNs. But effect of this category of methods are seriously affected by occlusion in crowd scenes. （检测器作为拥挤场景下人数估计受遮挡影响巨大）</li>
<li>Number regression. This category of methods just capture some features from original images and use machine-learning models to map the relation between features and numbers. An improved version via deep-learning directly map the relation between original image and its numbers. Before deep-learning, regression-based methods were SOTA and researchers are focus on finding more effective features to estimate more accuracy results. But when deep-learning get popular and achieve better results, regression-based methods get less attention because it is hard to capture effective hand-crafted features. （使用手工特征 + ml方法进行人数统计，在deep learning之前是最work的方法）</li>
<li>Density-map. This category of methods are the mainstream methods in crowd counting nowadays. Compared with detector-based methods and regression-based methods, density-map can not only give the information of pedestrian numbers, but also can reflect the distribution of pedestrians, which can make the models to fit original images with opposite density better.（将人群估计转化为 密度图|热力图 的逐像素回归问题是当下最常见的方案）</li>
</ul>
</li>
</ul>
<h2 id="What-is-density-map"><a href="#What-is-density-map" class="headerlink" title="What is density-map?"></a>What is density-map?</h2><ul>
<li>Simply speaking, we use a gaussian kernel to simulate a head in corresponding position of the original image. After do this action for all heads in the image, we then perform normalization in matrix which is composed by all these gaussian kernels. The sample picture is as follows:（密集人群统计的标注都是json化的点，通过映射回原图进行高斯滤波得到gt的热力图）<ul>
<li><img src="/images/crowd_counting/crowd_counting_density_map_sample.png" alt="crowd_counting_density_map_sample.png"></li>
</ul>
</li>
<li>Further, there are three strategies to generate density-map.<ol>
<li>use the same gaussian kernel to simulate all heads. This method applies to scene without severe perspective distortion. [fixed_kernel_code]</li>
<li>use the perspective map(which is generated by linear regression of pedestrians’ height) to generate gaussian kernels with different sizes to different heads. This method applies to fixed scene. [perspective_kernel_code] And [paper-zhang-CVPR2015] give detailed instruction about how to generate perspective density-map.</li>
<li>use the k-nearest heads to generate gaussian kernels with different sizes to different heads. This method applies to very crowded scenes. [k_nearset_kernel_code] And [paper-MCNN-CVPR2016] give detailed instruction about how to generate k-nearest density-map.  </li>
<li>（具体来说就是 1:形变较小的远距离安防场景用一样尺寸大小的高斯核就行了  2:有形变的就用随着高度变化的高斯核  3:如果说极度密集的话最好使用k近邻方法生成不同尺寸的高斯核）</li>
</ol>
</li>
</ul>
<h2 id="Model-for-beginner"><a href="#Model-for-beginner" class="headerlink" title="Model for beginner"></a>Model for beginner</h2><ul>
<li>For beginner, [paper-MCNN-CVPR2016] is the most suitable model to learn crowd counting. The model is not complex and have an acceptable accuracy. We provide an easy [MCNN_model_code] to let you know MCNN rapidly and an easy full realization of [MCNN-pytorch]. （MCNN作为人群估计最为经典文章之一，非常适合作为入门首选）</li>
</ul>
<hr>
<h1 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h1><hr>
<h2 id="MCNN"><a href="#MCNN" class="headerlink" title="MCNN"></a>MCNN</h2><ul>
<li>paper <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhang_Single-Image_Crowd_Counting_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Single-Image Crowd Counting via Multi-Column Convolutional Neural Network</a></li>
<li>Contributions of this paper<ul>
<li>In this paper, we aim to conduct accurate crowd counting from an arbitrary still image, with an arbitrary camera perspective and crowd density. At first sight this seems to be a rather daunting task, since we obviously need to conquer series of challenges:<ul>
<li>Foreground segmentation is indispensable in most existing work. However foreground segmentation is a challenging task all by itself and inaccurate segmentation will have irreversible bad effect on the final count. In our task, the viewpoint of an image can be arbitrary. Without information about scene geometry or motion, it is almost impossible to segment the crowd from its background accurately. Hence, we have to estimate the number of crowd without segmenting the foreground first. （抠出前景不是必须的了）</li>
<li>The density and distribution of crowd vary significantly in our task (or datasets) and typically there are tremendous occlusions for most people in each image. Hence traditional detection-based methods do not work well on such images and situations.（热力图特好使）</li>
<li>As there might be significant variation in the scale of the people in the images, we need to utilize features at different scales all together in order to accurately estimate crowd counts for different images. Since we do not have tracked features and it is difficult to handcraft features for all different scales, we have to resort to methods that can automatically learn effective features.（deep learning来啦小老弟）</li>
</ul>
</li>
</ul>
</li>
<li>To overcome above challenges, in this work, we propose a novel framework based on convolutional neural network (CNN) [9, 16] for crowd counting in an arbitrary still image. More specifically, we propose a multi-column convolutional neural network (MCNN) inspired by the work of [8], which has proposed multi-column deep neural networks for image classification. In their model, an arbitrary number of columns can be trained on inputs preprocessed in different ways. Then final predictions are obtained by averaging individual predictions of all deep neural networks. Our MCNN contains three columns of convolutional neural networks whose filters have different sizes. Input of the MCNN is the image, and its output is a crowd density map whose integral gives the overall crowd count. Contributions of this paper are summarized as follows:<ul>
<li>The reason for us to adopt a multi-column architecture here is rather natural: the three columns correspond to filters with receptive fields of different sizes (large, medium, small) so that the features learned by each column CNN is adaptive to (hence the overall network is robust to) large variation in people/head size due to perspective effect or across different image resolutions.（多分辨率玩法）</li>
<li>In our MCNN, we replace the fully connected layer with a convolution layer whose filter size is 1 × 1. Therefore the input image of our model can be of arbitrary size to avoid distortion. The immediate output of the network is an estimate of the density.（全卷积替代FC）</li>
<li><img src="/images/crowd_counting/MCNN_1.png" alt="MCNN_1.png"></li>
</ul>
</li>
<li>We collect a new dataset for evaluation of crowd counting methods. Existing crowd counting datasets cannot fully test the performance of an algorithm in the diverse scenarios considered by this work because their limitations in the variation in viewpoints (UCSD, WorldExpo’10), crowd counts (UCSD), the scale of dataset (UCSD, UCF CC 50), or the variety of scenes (UCF CC 50). In this work we introduce a new large-scale crowd dataset named Shanghaitech of nearly 1,200 images with around 330,000 accurately labeled heads. As far as we know, it is the largest crowd counting dataset in terms of number annotated heads. No two images in this dataset are taken from the same viewpoint. This dataset consists of two parts: Part A and Part B. Images in Part A are randomly crawled from the Internet, most of them have a large number of people. Part B are taken from busy streets of metropolitan areas in Shanghai. We have manually annotated both parts of images and will share this dataset by request. Figure 1 shows some representative samples of this dataset.（发布了shanghaitech dataset）</li>
<li>Density map via geometry-adaptive kernels <ul>
<li>For each head xi in a given image, we denote the distances to its k nearest neighbors as {d i 1 , di 2 , . . . , di m}. The average distance is therefore ¯d i = 1 m ∑m j=1 d i j . Thus, the pixel associated with xi corresponds to an area on the ground in the scene roughly of a radius proportional to ¯d i . Therefore, to estimate the crowd density around the pixel xi , we need to convolve δ(x − xi) with a Gaussian kernel with variance σi proportional to ¯d i .More precisely, the density F should be</li>
<li><img src="/images/crowd_counting/MCNN_2.png" alt="MCNN_2.png"></li>
<li>for some parameter β. In other words, we convolve the labels H with density kernels adaptive to the local geometry around each data point, referred to as geometry-adaptive kernels. In our experiment, we have found empirically β = 0.3 gives the best result. In Figure 2, we have shown so-obtained density maps of two exemplar images in our dataset.</li>
<li><img src="/images/crowd_counting/MCNN_3.png" alt="MCNN_3.png"></li>
</ul>
</li>
<li>启发性里程碑意义的工作，标志着Crowd Counting进入深度时代。从这以后的工作基本就离不开热力图和shanghaitech数据集，基本就是在这篇工作的基础上改进model，大框架基本定性</li>
</ul>
<h2 id="Cascaded-MTL"><a href="#Cascaded-MTL" class="headerlink" title="Cascaded-MTL"></a>Cascaded-MTL</h2><ul>
<li>paper <a href="https://arxiv.org/pdf/1707.09605.pdf" target="_blank" rel="noopener">CNN-based Cascaded Multi-task Learning of High-level Prior and Density Estimation for Crowd Counting</a></li>
<li>git <a href="https://github.com/svishwa/crowdcount-cascaded-mtl" target="_blank" rel="noopener">https://github.com/svishwa/crowdcount-cascaded-mtl</a></li>
<li><img src="/images/crowd_counting/MTL1.png" alt="MTL1.png"></li>
<li><img src="/images/crowd_counting/MTL2.png" alt="MTL2.png"></li>
<li><img src="/images/crowd_counting/MTL3.png" alt="MTL3.png"></li>
<li>Proposed method<ol>
<li>Shared convolutional layers</li>
<li>High-level prior stage<ul>
<li>Classifying the crowd into several groups is an easier problem as compared to directly performing classification or regression for the whole count range which requires a larger amount of training data. Hence, we quantize the crowd count into ten groups and learn a crowd count group classifier which also performs the task of incorporating high-level prior into the network. Cross-entropy error is used as the loss layer for this stage. （将人群计数任务映射成为10类拥挤程度的分类问题）</li>
</ul>
</li>
<li>Density estimation<ul>
<li>Standard pixel-wise Euclidean loss is used as the loss layer for this stage. Note that this loss depends on intermediate output of the earlier cascade, thereby enforcing a causal relationship between count classification and density estimation.</li>
</ul>
</li>
<li>Objective function<ul>
<li><img src="/images/crowd_counting/MTL4.png" alt="MTL4.png"></li>
</ul>
</li>
</ol>
</li>
<li>Experiment<ul>
<li><img src="/images/crowd_counting/MTL5.png" alt="MTL5.png"></li>
</ul>
</li>
<li>这是第一篇提出将分类计数和热力图联合训练提高效果的文章，相较MCNN提升比较明显。后面几年没有人在这个方向继续深挖，直到19年出现一个人用类似思路在part B的MAE刷到了7以内，那就是后话了</li>
</ul>
<h2 id="CSRNet"><a href="#CSRNet" class="headerlink" title="CSRNet"></a>CSRNet</h2><ul>
<li>paper <a href="https://arxiv.org/pdf/1802.10062.pdf" target="_blank" rel="noopener">CSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes</a></li>
<li><a href="https://github.com/leeyeehoo/CSRNet-pytorch" target="_blank" rel="noopener">https://github.com/leeyeehoo/CSRNet-pytorch</a></li>
<li>Proposed Solution<ol>
<li>CSRNet architecture<ol>
<li>Dilated convolution<ul>
<li><img src="/images/crowd_counting/CSRNET1.png" alt="CSRNET1.png"> </li>
</ul>
</li>
<li>Network Configuration  <ul>
<li><img src="/images/crowd_counting/CSRNET2.png" alt="CSRNET2.png"> </li>
</ul>
</li>
</ol>
</li>
<li>Training method<ol>
<li>Ground truth generation<ul>
<li>follow mcnn</li>
</ul>
</li>
<li>Data augmentation<ul>
<li>We crop 9 patches from each image at different locations with 1/4 size of the original image. The first four patches contain four quarters of the image without overlapping while the other five patches are randomly cropped from the input image. After that, we mirror the patches so that we double the training set.</li>
</ul>
</li>
<li>Training details<ul>
<li><img src="/images/crowd_counting/CSRNET3.png" alt="CSRNET3.png"> </li>
</ul>
</li>
</ol>
</li>
</ol>
</li>
<li>Experiment<ul>
<li><img src="/images/crowd_counting/CSRNET4.png" alt="CSRNET4.png"> </li>
</ul>
</li>
<li>CSR的思路和Deeplab ASPP，RFB类似，都是通过不同的dilation rate进行不同感受野融合来加强结果的做法，整体来说比较work简单易理解。从这篇开始，dilation成为了crowd counting的标配</li>
</ul>
<h2 id="SANET"><a href="#SANET" class="headerlink" title="SANET"></a>SANET</h2><ul>
<li>paper <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Xinkun_Cao_Scale_Aggregation_Network_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Scale Aggregation Network for Accurate and Efficient Crowd Counting</a></li>
<li><img src="/images/crowd_counting/SANET1.png" alt="SANET1.png"></li>
<li>This section presents the details of the Scale Aggregation Network (SANet). We first introduce our network architecture and then give descriptions of the proposed loss function.<ol>
<li>Architecture<ul>
<li>Feature Map Encoder (FME) DECODE<ul>
<li><img src="/images/crowd_counting/SANET2.png" alt="SANET2.png"></li>
</ul>
</li>
<li>ensity Map Estimator (DME) ENCODE</li>
<li>Normalization Layers —&gt; Instance Normalization</li>
</ul>
</li>
<li>Loss Function<ul>
<li>Euclidean Loss – Euclidean between pred and gt per pixel</li>
<li>Local Pattern Consistency Loss<ul>
<li>Beyond the pixel-wise loss function, we also incorporate the local correlation in density maps to improve the quality of results.We utilize SSIM index to measure the local pattern consistency of estimated density maps and ground truths. SSIM index is usually used in image quality assessment.</li>
<li><img src="/images/crowd_counting/SANET3.png" alt="SANET3.png"></li>
<li><img src="/images/crowd_counting/SANET4.png" alt="SANET4.png"></li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
<li>Experiment<ul>
<li><img src="/images/crowd_counting/SANET5.png" alt="SANET5.png"></li>
</ul>
</li>
<li>这其实就是使用inception block进行encode，普通deconv，外加使用in取代bn，在seg任务中IN的使用频率往往是更高也是更有效的。</li>
<li>提出了SSIM loss，提出了patch base的训练测试方法 –-–-– 个人感觉和SNIPER一样把图拆小处理是充满争议的做法</li>
<li>总体来说，在当时抛弃了VGG使用全新结构，改进了MCNN，简单高效，也是很有影响力的文章</li>
</ul>
<h2 id="SFCN"><a href="#SFCN" class="headerlink" title="SFCN"></a>SFCN</h2><ul>
<li>paper <a href="https://arxiv.org/pdf/1903.03303.pdf" target="_blank" rel="noopener">Learning from Synthetic Data for Crowd Counting in the Wild</a></li>
<li>home <a href="https://gjy3035.github.io/GCC-CL/" target="_blank" rel="noopener">https://gjy3035.github.io/GCC-CL/</a></li>
<li>In summary, this paper’s contributions are three-fold:<ol>
<li>We are the first to develop a data collector and labeler for crowd counting, which can automatically collect and annotate images without any labor costs. By using them, we create the first large-scale, synthetic and diverse crowd counting dataset. (使用GTA5开发了一套Crowd Counting Dataset合成器)<ul>
<li>Data Collection<ol>
<li>Scene Selection</li>
<li>Person Model</li>
<li>Scenes Synthesis for Congested Crowd</li>
<li>Summary</li>
</ol>
</li>
<li>Properties of GCC<ul>
<li>GCC dataset consists of 15,212 images, with resolution of 1080 × 1920, containing 7,625,843 persons.</li>
</ul>
</li>
<li><img src="/images/crowd_counting/SFCN1.png" alt="SFCN1.png"></li>
</ul>
</li>
<li>We present a pretrained scheme to facilitate the original method’s performance on the real data, which can more effectively reduce the estimation errors compared with random initialization and ImageNet model. Further, through the strategy, our proposed SFCN achieves the state-of-the-art results. （使用GCC训练的预训练模型比直接使用imagenet的预训练模型diao多了，而且我们还开发了SFCN使效果更进一步）<ul>
<li><img src="/images/crowd_counting/SFCN2.png" alt="SFCN2.png"></li>
<li>Network Architecture<ul>
<li>In this paper, we design a spatial FCN (SFCN) to produce the density map, which adopt VGG-16 [34] or ResnNet-101 [12] as the backbone. To be specific, the spatial encoder is added to the top of the backbone. The feature map flow is illustrated as in Fig. 6. After the spatial encoder, a regression layer is added, which directly outputs the density map with input’s 1/8 size. Here, we do not review the spatial encoder because of the limited space. During the training phase, the objective is minimizing standard Mean Squared Error at the pixel-wise level; the learning rate is set as 10−5 ; and Adam algorithm is used to optimize SFCN.</li>
<li><img src="/images/crowd_counting/SFCN3.png" alt="SFCN3.png"></li>
</ul>
</li>
</ul>
</li>
<li>We are the first to propose a crowd counting method via domain adaptation, which does not use any label of the real data. By our designed SE Cycle GAN, the domain gap between the synthetic and real data can be significantly reduced. Finally, the proposed method outperforms the two baselines.（propose了一种SE-CycleGAN，即使只在GCC数据集上训练也可以通过GAN使结果大幅提高，减少了real data和synthetic data的domain gap）<ul>
<li><img src="/images/crowd_counting/SFCN4.png" alt="SFCN4.png"></li>
<li><img src="/images/crowd_counting/SFCN5.png" alt="SFCN5.png"></li>
</ul>
</li>
</ol>
</li>
<li>Experiment<ul>
<li><img src="/images/crowd_counting/SFCN6.png" alt="SFCN6.png"></li>
<li><img src="/images/crowd_counting/SFCN7.png" alt="SFCN7.png"></li>
</ul>
</li>
<li>这是一篇非常全面的文章，兼顾数据集，模型，domain问题</li>
<li>GCC-pretrained model非常work，值得推荐</li>
<li>SFCN也非常简单好用，去除了patch的操作依然有不错的acc</li>
</ul>
<h2 id="CFF"><a href="#CFF" class="headerlink" title="CFF"></a>CFF</h2><ul>
<li>paper <a href="https://staff.fnwi.uva.nl/z.shi/files/Counting_ICCV__2019.pdf" target="_blank" rel="noopener">Counting with Focus for Free</a></li>
<li>git <a href="https://github.com/shizenglin/Counting-with-Focus-for-Free" target="_blank" rel="noopener">https://github.com/shizenglin/Counting-with-Focus-for-Free</a></li>
<li><img src="/images/crowd_counting/CFF1.png" alt="CFF1.png"></li>
</ul>
<ol>
<li>Focus from segmentation<ul>
<li>Segmentation map<ul>
<li>annotation as a mask like seg</li>
</ul>
</li>
<li>Segmentation focus<ul>
<li>use focal loss</li>
</ul>
</li>
<li>Network detail<ul>
<li>After the output of the base network, we perform a 1 × 1 convolution layer with parameters θs ∈ R C×2×1×1 , followed by a softmax function δ to generate a per-pixel probability map Pi = δ(θsV ) ∈ R 2×W×H. From this probability map, the second value along the first dimension represents the probability of each pixel being part of the segmentation foreground. We furthermore tile this slice C times to construct a separate output tensor Vs ∈ R C×W×H, which will be used in the density estimation branch itself</li>
</ul>
</li>
</ul>
</li>
<li>Focus from global density<ul>
<li>Global density<ul>
<li>compute Global density in each patch</li>
</ul>
</li>
<li>Global density focus<ul>
<li>focal loss<ul>
<li>Network details</li>
</ul>
</li>
<li>For network output V , we first perform an outer product B = V V T ∈ R C×C , followed by a mean pooling along the second dimension to aggregate the bilinear features over the image, i.e. Bˆ = 1 C PC i=1 B[:, i] ∈ R C×1 . The bilinear vector Bˆ is `2-normalized, followed by signed square root normalization, which has shown to be effective in bilinear pooling [18]. Then we use a fully connected layer with parameters θc ∈ R C×M followed by a softmax function δc to make individual prediction C = δc(θcBˆ) ∈ RM×1 for the global density. Furthermore, another fully-connected layer with parameters θd ∈ R C×C followed by sigmoid function δd also on top of the bilinear pooling layer is added to generate global density focus output D = δd(θdBˆ) ∈ R C×1 . We note that this results in a focus over the channel dimensions, complementary to the focus over the spatial dimensions from segmentation. Akin to the focus from segmentation, we tile the output vector into Vd ∈ R C×W×H, also to be used in the density estimation branch.</li>
</ul>
</li>
</ul>
</li>
<li>Non-uniform kernel estimation</li>
</ol>
<ul>
<li>Experiment<ul>
<li><img src="/images/crowd_counting/CFF2.png" alt="CFF2.png"></li>
</ul>
</li>
<li>为model 增加了seg分支用于attention，kernel分支，相较SA（67）提高了少量的结果</li>
</ul>
<h2 id="CAN"><a href="#CAN" class="headerlink" title="CAN"></a>CAN</h2><ul>
<li>paper <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Context-Aware_Crowd_Counting_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Context-Aware Crowd Counting</a> </li>
<li>git <a href="https://github.com/weizheliu/Context-Aware-Crowd-Counting" target="_blank" rel="noopener">https://github.com/weizheliu/Context-Aware-Crowd-Counting</a></li>
<li><img src="/images/crowd_counting/CAN1.png" alt="CAN1.png"></li>
<li><p>Approach</p>
<ol>
<li><p>Scale-Aware Contextual Features</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ContextualModule</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, out_features=<span class="number">512</span>, sizes=<span class="params">(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>)</span>)</span>:</span></span><br><span class="line">        super(ContextualModule, self).__init__()</span><br><span class="line">        self.scales = []</span><br><span class="line">        self.scales = nn.ModuleList([self._make_scale(features, size) <span class="keyword">for</span> size <span class="keyword">in</span> sizes])</span><br><span class="line">        self.bottleneck = nn.Conv2d(features * <span class="number">2</span>, out_features, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.weight_net = nn.Conv2d(features,features,kernel_size=<span class="number">1</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__make_weight</span><span class="params">(self,feature,scale_feature)</span>:</span></span><br><span class="line">        weight_feature = feature - scale_feature</span><br><span class="line">        <span class="keyword">return</span> F.sigmoid(self.weight_net(weight_feature))</span><br><span class="line">   </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_scale</span><span class="params">(self, features, size)</span>:</span></span><br><span class="line">        prior = nn.AdaptiveAvgPool2d(output_size=(size, size))</span><br><span class="line">        conv = nn.Conv2d(features, features, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(prior, conv)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, feats)</span>:</span></span><br><span class="line">        h, w = feats.size(<span class="number">2</span>), feats.size(<span class="number">3</span>)</span><br><span class="line">        multi_scales = [F.upsample(input=stage(feats), size=(h, w), mode=<span class="string">'bilinear'</span>) <span class="keyword">for</span> stage <span class="keyword">in</span> self.scales]</span><br><span class="line">        weights = [self.__make_weight(feats,scale_feature) <span class="keyword">for</span> scale_feature <span class="keyword">in</span> multi_scales]</span><br><span class="line">        overall_features = [(multi_scales[<span class="number">0</span>]*weights[<span class="number">0</span>]+multi_scales[<span class="number">1</span>]*weights[<span class="number">1</span>]+multi_scales[<span class="number">2</span>]*weights[<span class="number">2</span>]+multi_scales[<span class="number">3</span>]*weights[<span class="number">3</span>])/(weights[<span class="number">0</span>]+weights[<span class="number">1</span>]+weights[<span class="number">2</span>]+weights[<span class="number">3</span>])]+ [feats]</span><br><span class="line">        bottle = self.bottleneck(torch.cat(overall_features, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> self.relu(bottle)</span><br></pre></td></tr></table></figure>
<ul>
<li>通过不同程度的avgpool + attention mask + concate，完成全图的多scale attention</li>
</ul>
</li>
</ol>
</li>
</ul>
<h2 id="DSSINet"><a href="#DSSINet" class="headerlink" title="DSSINet"></a>DSSINet</h2><ul>
<li>paper <a href="https://arxiv.org/pdf/1908.08692.pdf" target="_blank" rel="noopener">Crowd Counting with Deep Structured Scale Integration Network</a></li>
<li>git <a href="https://github.com/Legion56/Counting-ICCV-DSSINet" target="_blank" rel="noopener">https://github.com/Legion56/Counting-ICCV-DSSINet</a></li>
<li><img src="/images/crowd_counting/DSSINET1.png" alt="DSSINET1.png"></li>
<li>base CRF构建了一个全新的模块；2.DMS-SSIM loss；3.在4个benchmark上均取得了sota<ul>
<li><img src="/images/crowd_counting/DSSINET2.png" alt="DSSINET2.png"></li>
</ul>
</li>
<li>整体架构图，可以看出，SFEM这个MessagePassing模块是整个网络的核心  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MessagePassing</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, branch_n, input_ncs, bn=False)</span>:</span></span><br><span class="line">        super(MessagePassing, self).__init__()</span><br><span class="line">        self.branch_n = branch_n</span><br><span class="line">        self.iters = <span class="number">2</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(branch_n):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(branch_n):</span><br><span class="line">                <span class="keyword">if</span> i == j:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                setattr(self, <span class="string">"w_0_&#123;&#125;_&#123;&#125;_0"</span>.format(j, i), \</span><br><span class="line">                        nn.Sequential(</span><br><span class="line">                                Conv2d_dilated(input_ncs[j],  input_ncs[i], <span class="number">1</span>, dilation=<span class="number">1</span>, same_padding=<span class="literal">True</span>, NL=<span class="literal">None</span>, bn=bn),</span><br><span class="line">                            )</span><br><span class="line">                        )</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">False</span>)</span><br><span class="line">        self.prelu = nn.PReLU()</span><br><span class="line">         </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        hidden_state = input</span><br><span class="line">        side_state = []</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.iters):</span><br><span class="line">            hidden_state_new = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.branch_n):</span><br><span class="line"> </span><br><span class="line">                unary = hidden_state[i]</span><br><span class="line">                binary = <span class="literal">None</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(self.branch_n):</span><br><span class="line">                    <span class="keyword">if</span> i == j:</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    <span class="keyword">if</span> binary <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                        binary = getattr(self, <span class="string">'w_0_&#123;&#125;_&#123;&#125;_0'</span>.format(j, i))(hidden_state[j])</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        binary = binary + getattr(self, <span class="string">'w_0_&#123;&#125;_&#123;&#125;_0'</span>.format(j, i))(hidden_state[j])</span><br><span class="line"> </span><br><span class="line">                binary = self.prelu(binary)</span><br><span class="line">                hidden_state_new += [self.relu(unary + binary)]</span><br><span class="line">            hidden_state = hidden_state_new</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> hidden_state</span><br></pre></td></tr></table></figure>
<ul>
<li>可以看到，对于 for hidden_state[i] in range(self.branch_n) 会和其他所有的非自己对象进行conv2d_dilated，输出和hidden_state[i]相同的channel数，然后prelu各自结合，再与原始值residual add</li>
<li><img src="/images/crowd_counting/DSSINET3.png" alt="DSSINET3.png"></li>
<li>loss部分，咋一看整个图，还以为作者是说吧所有层都算DMS-SSIM的意思，然而。。</li>
<li><img src="/images/crowd_counting/DSSINET4.png" alt="DSSINET4.png"></li>
<li>这里说的也比较清楚了，DMS-SSIM-m 代表有几个scale的DMS-SSIM loss，从代码实现上看，作者也仅仅用了最后一层算loss。structure示意图中4个SFEM也是对应了5个dilation scale，代码比较清楚<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">t_ssim</span><span class="params">(img1, img2, img11, img22, img12, window, channel, dilation=<span class="number">1</span>, size_average=True)</span>:</span></span><br><span class="line">    window_size = window.size()[<span class="number">2</span>]</span><br><span class="line">    input_shape = list(img1.size())</span><br><span class="line"> </span><br><span class="line">    padding, pad_input = compute_same_padding2d(input_shape, \</span><br><span class="line">                                                kernel_size=(window_size, window_size), \</span><br><span class="line">                                                strides=(<span class="number">1</span>,<span class="number">1</span>), \</span><br><span class="line">                                                dilation=(dilation, dilation))</span><br><span class="line">    <span class="keyword">if</span> img11 <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        img11 = img1 * img1</span><br><span class="line">    <span class="keyword">if</span> img22 <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        img22 = img2 * img2</span><br><span class="line">    <span class="keyword">if</span> img12 <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        img12 = img1 * img2</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">if</span> pad_input[<span class="number">0</span>] == <span class="number">1</span> <span class="keyword">or</span> pad_input[<span class="number">1</span>] == <span class="number">1</span>:</span><br><span class="line">        img1 = F.pad(img1, [<span class="number">0</span>, int(pad_input[<span class="number">0</span>]), <span class="number">0</span>, int(pad_input[<span class="number">1</span>])])</span><br><span class="line">        img2 = F.pad(img2, [<span class="number">0</span>, int(pad_input[<span class="number">0</span>]), <span class="number">0</span>, int(pad_input[<span class="number">1</span>])])</span><br><span class="line">        img11 = F.pad(img11, [<span class="number">0</span>, int(pad_input[<span class="number">0</span>]), <span class="number">0</span>, int(pad_input[<span class="number">1</span>])])</span><br><span class="line">        img22 = F.pad(img22, [<span class="number">0</span>, int(pad_input[<span class="number">0</span>]), <span class="number">0</span>, int(pad_input[<span class="number">1</span>])])</span><br><span class="line">        img12 = F.pad(img12, [<span class="number">0</span>, int(pad_input[<span class="number">0</span>]), <span class="number">0</span>, int(pad_input[<span class="number">1</span>])])</span><br><span class="line"> </span><br><span class="line">    padd = (padding[<span class="number">0</span>] // <span class="number">2</span>, padding[<span class="number">1</span>] // <span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">    mu1 = F.conv2d(img1, window , padding=padd, dilation=dilation, groups=channel)</span><br><span class="line">    mu2 = F.conv2d(img2, window , padding=padd, dilation=dilation, groups=channel)</span><br><span class="line"> </span><br><span class="line">    mu1_sq = mu1.pow(<span class="number">2</span>)</span><br><span class="line">    mu2_sq = mu2.pow(<span class="number">2</span>)</span><br><span class="line">    mu1_mu2 = mu1*mu2</span><br><span class="line"> </span><br><span class="line">    si11 = F.conv2d(img11, window, padding=padd, dilation=dilation, groups=channel)</span><br><span class="line">    si22 = F.conv2d(img22, window, padding=padd, dilation=dilation, groups=channel)</span><br><span class="line">    si12 = F.conv2d(img12, window, padding=padd, dilation=dilation, groups=channel)</span><br><span class="line"> </span><br><span class="line">    sigma1_sq = si11 - mu1_sq</span><br><span class="line">    sigma2_sq = si22 - mu2_sq</span><br><span class="line">    sigma12 = si12 - mu1_mu2</span><br><span class="line"> </span><br><span class="line">    C1 = (<span class="number">0.01</span>*<span class="number">255</span>)**<span class="number">2</span></span><br><span class="line">    C2 = (<span class="number">0.03</span>*<span class="number">255</span>)**<span class="number">2</span></span><br><span class="line"> </span><br><span class="line">    ssim_map = ((<span class="number">2</span>*mu1_mu2 + C1)*(<span class="number">2</span>*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))</span><br><span class="line"> </span><br><span class="line">    v1 = <span class="number">2.0</span> * sigma12 + C2</span><br><span class="line">    v2 = sigma1_sq + sigma2_sq + C2</span><br><span class="line">    cs = torch.mean(v1 / v2)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">if</span> size_average:</span><br><span class="line">        ret = ssim_map.mean()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        ret = ssim_map.mean(<span class="number">1</span>).mean(<span class="number">1</span>).mean(<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> ret, cs</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NORMMSSSIM</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sigma=<span class="number">1.0</span>, levels=<span class="number">5</span>, size_average=True, channel=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(NORMMSSSIM, self).__init__()</span><br><span class="line">        self.sigma = sigma</span><br><span class="line">        self.window_size = <span class="number">5</span></span><br><span class="line">        self.levels = levels</span><br><span class="line">        self.size_average = size_average</span><br><span class="line">        self.channel = channel</span><br><span class="line">        self.register_buffer(<span class="string">'window'</span>, create_window(self.window_size, self.channel, self.sigma))</span><br><span class="line">        self.register_buffer(<span class="string">'weights'</span>, torch.Tensor([<span class="number">0.0448</span>, <span class="number">0.2856</span>, <span class="number">0.3001</span>, <span class="number">0.2363</span>, <span class="number">0.1333</span>]))</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, img1, img2)</span>:</span></span><br><span class="line">        img1 = (img1 + <span class="number">1e-12</span>) / (img2.max() + <span class="number">1e-12</span>)</span><br><span class="line">        img2 = (img2 + <span class="number">1e-12</span>) / (img2.max() + <span class="number">1e-12</span>)</span><br><span class="line"> </span><br><span class="line">        img1 = img1 * <span class="number">255.0</span></span><br><span class="line">        img2 = img2 * <span class="number">255.0</span></span><br><span class="line"> </span><br><span class="line">        msssim_score = self.msssim(img1, img2)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> - msssim_score</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">msssim</span><span class="params">(self, img1, img2)</span>:</span></span><br><span class="line">        levels = self.levels</span><br><span class="line">        mssim = []</span><br><span class="line">        mcs = []</span><br><span class="line"> </span><br><span class="line">        img1, img2, img11, img22, img12 = img1, img2, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(levels):</span><br><span class="line">            l, cs = \</span><br><span class="line">                    t_ssim(img1, img2, img11, img22, img12, \</span><br><span class="line">                                Variable(getattr(self, <span class="string">"window"</span>), requires_grad=<span class="literal">False</span>),\</span><br><span class="line">                                self.channel, size_average=self.size_average, dilation=(<span class="number">1</span> + int(i ** <span class="number">1.5</span>)))</span><br><span class="line"> </span><br><span class="line">            img1 = F.avg_pool2d(img1, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">            img2 = F.avg_pool2d(img2, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">            mssim.append(l)</span><br><span class="line">            mcs.append(cs)</span><br><span class="line"> </span><br><span class="line">        mssim = torch.stack(mssim)</span><br><span class="line">        mcs = torch.stack(mcs)</span><br><span class="line"> </span><br><span class="line">        weights = Variable(self.weights, requires_grad=<span class="literal">False</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> torch.prod(mssim ** weights)</span><br></pre></td></tr></table></figure>
<ul>
<li>这个levels就对应着文章中的m</li>
<li><img src="/images/crowd_counting/DSSINET5.png" alt="DSSINET5.png"></li>
<li>与代码对应的计算方式在文中所示</li>
</ul>
</li>
</ul>
</li>
<li>Experiment<ul>
<li><img src="/images/crowd_counting/DSSINET6.png" alt="DSSINET6.png"></li>
<li><img src="/images/crowd_counting/DSSINET8.png" alt="DSSINET8.png"></li>
</ul>
</li>
<li>exp的结果是really SOTA，几乎是最好的水平</li>
<li>神奇的方法，值得研究研究</li>
</ul>
<hr>
<h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><hr>
<h2 id="ShanghaiTechDataset-ShanghaiTech-SHT-A-amp-B"><a href="#ShanghaiTechDataset-ShanghaiTech-SHT-A-amp-B" class="headerlink" title="ShanghaiTechDataset (ShanghaiTech/SHT A &amp; B)"></a>ShanghaiTechDataset (ShanghaiTech/SHT A &amp; B)</h2><ul>
<li>A Well Konwn BenchMark</li>
<li>Shanghaitech which contains 1198 annotated images, with a total of 330,165 people with centers of their heads annotated. As far as we know, this dataset is the largest one in terms of the number of annotated people. This dataset consists of two parts: there are 482 images in Part A which are randomly crawled from the Internet, and 716 images in Part B which are taken from the busy streets of metropolitan areas in Shanghai. The crowd density varies significantly between the two subsets, making accurate estimation of the crowd more challenging than most existing datasets. Both Part A and Part B are divided into training and testing: 300 images of Part A are used for training and the remaining 182 images for testing;, and 400 images of Part B are for training and 316 for testing</li>
<li>paper <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhang_Single-Image_Crowd_Counting_CVPR_2016_paper.pdf" target="_blank" rel="noopener">https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhang_Single-Image_Crowd_Counting_CVPR_2016_paper.pdf</a></li>
<li>git <a href="https://github.com/desenzhou/ShanghaiTechDataset" target="_blank" rel="noopener">https://github.com/desenzhou/ShanghaiTechDataset</a></li>
<li>Download url<ul>
<li>Dropbox: <a href="https://www.dropbox.com/s/fipgjqxl7uj8hd5/ShanghaiTech.zip?dl=0" target="_blank" rel="noopener">https://www.dropbox.com/s/fipgjqxl7uj8hd5/ShanghaiTech.zip?dl=0</a></li>
<li>Baidu Disk: <a href="http://pan.baidu.com/s/1nuAYslz" target="_blank" rel="noopener">http://pan.baidu.com/s/1nuAYslz</a></li>
</ul>
</li>
</ul>
<h2 id="GCC-Dataset"><a href="#GCC-Dataset" class="headerlink" title="GCC Dataset"></a>GCC Dataset</h2><ul>
<li>A Generated Dataset For Getting Pretrained Model</li>
<li>home <a href="https://gjy3035.github.io/GCC-CL/" target="_blank" rel="noopener">https://gjy3035.github.io/GCC-CL/</a></li>
<li>Download url <ul>
<li><a href="https://share-7a4a1d992bf4e98dee11852a48215193.fangcloud.cn/share/4625d2bfa9427708060b5a5981?folder_id=385000263093" target="_blank" rel="noopener">https://share-7a4a1d992bf4e98dee11852a48215193.fangcloud.cn/share/4625d2bfa9427708060b5a5981?folder_id=385000263093</a></li>
<li><a href="https://mailnwpueducn-my.sharepoint.com/personal/gjy3035_mail_nwpu_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fgjy3035%5Fmail%5Fnwpu%5Fedu%5Fcn%2FDocuments%2F%E8%AE%BA%E6%96%87%E5%BC%80%E6%BA%90%E6%95%B0%E6%8D%AE%2FGCC%20Dataset" target="_blank" rel="noopener">https://mailnwpueducn-my.sharepoint.com/personal/gjy3035_mail_nwpu_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fgjy3035%5Fmail%5Fnwpu%5Fedu%5Fcn%2FDocuments%2F%E8%AE%BA%E6%96%87%E5%BC%80%E6%BA%90%E6%95%B0%E6%8D%AE%2FGCC%20Dataset</a></li>
</ul>
</li>
<li>The data is collected from an electronic game Grand Theft Auto V (GTA5), thus it is named as “GTA5 Crowd Counting” (“GCC” for short) dataset. GCC dataset consists of 15,212 images, with resolution of 1080×1920, containing 7,625,843 persons. Compared with the existing datasets, GCC is a more large-scale crowd counting dataset in both the number of images and the number of persons.</li>
<li><img src="/images/crowd_counting/GCC_dataset_1.png" alt="GCC_dataset_1"></li>
<li><img src="/images/crowd_counting/GCC_dataset_2.png" alt="GCC_dataset_2"></li>
</ul>
<h2 id="Fudan-ShanghaiTech-Dataset"><a href="#Fudan-ShanghaiTech-Dataset" class="headerlink" title="Fudan-ShanghaiTech Dataset"></a>Fudan-ShanghaiTech Dataset</h2><ul>
<li>We collected 100 videos captured from 13 different scenes, and FDST dataset contains 150,000 frames, with a total of 394,081 annotated heads, in particular,the training set of FDST dataset consists of 60 videos, 9000 frames and the testing set contains the remaining 40 videos, 6000 frames.</li>
<li>git <a href="https://github.com/sweetyy83/Lstn_fdst_dataset" target="_blank" rel="noopener">https://github.com/sweetyy83/Lstn_fdst_dataset</a></li>
<li>download url<ul>
<li><a href="https://pan.baidu.com/s/1NNaJ1vtsxCPJUjDNhZ1sHA#list/path=%2F" target="_blank" rel="noopener">https://pan.baidu.com/s/1NNaJ1vtsxCPJUjDNhZ1sHA#list/path=%2F</a></li>
<li><a href="https://drive.google.com/drive/folders/19c2X529VTNjl3YL1EYweBg60G70G2D-w?usp=sharing" target="_blank" rel="noopener">https://drive.google.com/drive/folders/19c2X529VTNjl3YL1EYweBg60G70G2D-w?usp=sharing</a></li>
</ul>
</li>
</ul>
<h2 id="Venice-Dataset"><a href="#Venice-Dataset" class="headerlink" title="Venice Dataset"></a>Venice Dataset</h2><ul>
<li>Venice. The four datasets discussed above have the advantage of being publicly available but do not contain precise calibration information. In practice, however, it can be readily obtained using either standard photogrammetry techniques or onboard sensors, for example when using a drone to acquire the images. To test this kind of scenario, we used a cellphone to film additional sequences of the Piazza San Marco in Venice, as seen from various viewpoints on the second floor of the basilica, as shown in the top two rows of Fig. 5. We then used the white lines on the ground to compute camera models. As shown in the bottom two rows of Fig. 5, this yields a more accurate calibration than in WorldExpo’10. The resulting dataset contains 4 different sequences and in total 167 annotated frames with fixed 1,280 × 720 resolution. 80 images from a single long sequence are taken as training data, and we use the images from the remaining 3 sequences for testing purposes. The ground-truth density maps were generated using fixed Gaussian kernels as in part B of the ShanghaiTech dataset.</li>
<li>paper <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Context-Aware_Crowd_Counting_CVPR_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Context-Aware_Crowd_Counting_CVPR_2019_paper.pdf</a></li>
<li>git <a href="https://github.com/weizheliu/Context-Aware-Crowd-Counting" target="_blank" rel="noopener">https://github.com/weizheliu/Context-Aware-Crowd-Counting</a></li>
<li>download url <a href="https://drive.google.com/file/d/15PUf7C3majy-BbWJSSHaXUlot0SUh3mJ/view" target="_blank" rel="noopener">https://drive.google.com/file/d/15PUf7C3majy-BbWJSSHaXUlot0SUh3mJ/view</a></li>
</ul>
<h2 id="UCF-QNRF"><a href="#UCF-QNRF" class="headerlink" title="UCF-QNRF"></a>UCF-QNRF</h2><ul>
<li>We introduce the largest dataset to-date (in terms of number of annotations) for training and evaluating crowd counting and localization methods. It contains 1535 images which are divided into train and test sets of 1201 and 334 images respectively. </li>
<li>home <a href="https://www.crcv.ucf.edu/data/ucf-qnrf/" target="_blank" rel="noopener">https://www.crcv.ucf.edu/data/ucf-qnrf/</a></li>
<li>paper <a href="https://www.crcv.ucf.edu/papers/eccv2018/2324.pdf" target="_blank" rel="noopener">https://www.crcv.ucf.edu/papers/eccv2018/2324.pdf</a></li>
<li>download url <ul>
<li><a href="https://www.crcv.ucf.edu/data/ucf-qnrf/UCF-QNRF_ECCV18.zip" target="_blank" rel="noopener">https://www.crcv.ucf.edu/data/ucf-qnrf/UCF-QNRF_ECCV18.zip</a></li>
<li><a href="https://drive.google.com/file/d/1fLZdOsOXlv2muNB_bXEW6t-IS9MRziL6/view" target="_blank" rel="noopener">https://drive.google.com/file/d/1fLZdOsOXlv2muNB_bXEW6t-IS9MRziL6/view</a></li>
</ul>
</li>
<li><img src="/images/crowd_counting/UCF_QNRF_dataset_1.png" alt="UCF_QNRF_dataset_1"></li>
</ul>
<h2 id="UCF-CC-50"><a href="#UCF-CC-50" class="headerlink" title="UCF-CC-50"></a>UCF-CC-50</h2><ul>
<li>Only 50 images. This data set contains images of extremely dense crowds. The images are collected mainly from the FLICKR. They are shared only for the research purposes. </li>
<li>home <a href="https://www.crcv.ucf.edu/data/ucf-cc-50/" target="_blank" rel="noopener">https://www.crcv.ucf.edu/data/ucf-cc-50/</a></li>
<li>paper <a href="https://www.crcv.ucf.edu/papers/cvpr2013/Counting_V3o.pdf" target="_blank" rel="noopener">https://www.crcv.ucf.edu/papers/cvpr2013/Counting_V3o.pdf</a></li>
<li>download url <ul>
<li><a href="https://www.crcv.ucf.edu/data/ucf-cc-50/UCFCrowdCountingDataset_CVPR13.rar" target="_blank" rel="noopener">https://www.crcv.ucf.edu/data/ucf-cc-50/UCFCrowdCountingDataset_CVPR13.rar</a></li>
</ul>
</li>
</ul>
<h2 id="WorldExpo’10-Dataset"><a href="#WorldExpo’10-Dataset" class="headerlink" title="WorldExpo’10 Dataset"></a>WorldExpo’10 Dataset</h2><ul>
<li>We introduce a new large-scale cross-scene crowd counting dataset. To the best of our knowledge, this is the largest dataset focusing on cross-scene counting. It includes 1132 annotated video sequences captured by 108 surveillance cameras, all from Shanghai 2010 WorldExpo2. Since most of the cameras have disjoint bird views, they cover a large variety of scenes. We labeled a total of 199,923 pedestrians at the centers of their heads in 3,980 frames. These frames are uniformly sampled from all the video sequences.</li>
<li>home <a href="http://www.ee.cuhk.edu.hk/~xgwang/expo.html" target="_blank" rel="noopener">http://www.ee.cuhk.edu.hk/~xgwang/expo.html</a></li>
<li>paper <a href="http://www.ee.cuhk.edu.hk/~xgwang/Project%20Page%20of%20Cross-scene%20Crowd%20Counting%20via%20Deep%20Convolutional%20Neural%20Networks_files/0994.pdf" target="_blank" rel="noopener">http://www.ee.cuhk.edu.hk/~xgwang/Project%20Page%20of%20Cross-scene%20Crowd%20Counting%20via%20Deep%20Convolutional%20Neural%20Networks_files/0994.pdf</a></li>
<li>download url <ul>
<li>This paper is in cooperation with Shanghai Jiao Tong University. SJTU has the copyright of the dataset. So please email Prof. Xie (xierong@sjtu.edu.cn) with your name and affiliation to get the download link. It’s better to use your official email address. Thank you for your understanding.</li>
<li><a href="https://pan.baidu.com/s/1mgh7W4w#list/path=%2F" target="_blank" rel="noopener">https://pan.baidu.com/s/1mgh7W4w#list/path=%2F</a>   password：765k</li>
<li>Thank you for your attention to download our dataset. The dataset can be downloaded from Baidu disk or Dropbox:</li>
<li>Baidu Disk: <a href="http://pan.baidu.com/s/1mgh7W4w" target="_blank" rel="noopener">http://pan.baidu.com/s/1mgh7W4w</a> password：765k</li>
<li>Dropbox: <a href="https://www.dropbox.com/sh/kx9hctd9begjbn9/AAA65gQXG-xZ4e94wSNBDBrHa?dl=0" target="_blank" rel="noopener">https://www.dropbox.com/sh/kx9hctd9begjbn9/AAA65gQXG-xZ4e94wSNBDBrHa?dl=0</a> </li>
<li>This dataset is ONLY released for academic use. Please do not further distribute the dataset (including the download link), or put any of the videos and images on the public website. The copyrights belongs to Shanghai Jiao Tong University.</li>
<li>Please kindly cite these two papers if you use our data in your research. Thanks and hope you will benefit from our dataset.Cong Zhang, Kai Zhang, Hongsheng Li, Xiaogang Wang, Rong Xie　and Xiaokang Yang: Data-driven Crowd Understanding: a Baseline for a Large-scale Crowd Dataset. IEEE Transactions on Multimedia,  Vol. 18, No.6, pp1048 - 1061, 2016.Cong Zhang, Hongsheng Li, Xiaogang Wang, and Xiaokang Yang. “Cross-scene Crowd Counting via Deep Convolutional Neural Networks”. in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition 2015.If you have any detail questions about the dataset, please feel free to contact us (xierong@sjtu.edu.cnand <a href="&#109;&#x61;&#x69;&#x6c;&#116;&#111;&#x3a;&#x78;&#x69;&#101;&#x72;&#111;&#110;&#103;&#x40;&#115;&#106;&#x74;&#117;&#x2e;&#101;&#x64;&#x75;&#46;&#x63;&#110;&#97;&#110;&#100;">&#x78;&#x69;&#101;&#x72;&#111;&#110;&#103;&#x40;&#115;&#106;&#x74;&#117;&#x2e;&#101;&#x64;&#x75;&#46;&#x63;&#110;&#97;&#110;&#100;</a>zhangcong0929@gmail.com <a href="&#x6d;&#97;&#105;&#x6c;&#x74;&#111;&#58;&#122;&#104;&#97;&#110;&#103;&#x63;&#111;&#x6e;&#x67;&#48;&#57;&#50;&#57;&#64;&#103;&#x6d;&#97;&#105;&#108;&#x2e;&#99;&#111;&#x6d;">&#122;&#104;&#97;&#110;&#103;&#x63;&#111;&#x6e;&#x67;&#48;&#57;&#50;&#57;&#64;&#103;&#x6d;&#97;&#105;&#108;&#x2e;&#99;&#111;&#x6d;</a>).Copyright (c) 2015, Shanghai Jiao Tong University All rights reserved. Best Regards, Rong </li>
</ul>
</li>
</ul>
<h2 id="Mall-Dataset"><a href="#Mall-Dataset" class="headerlink" title="Mall Dataset"></a>Mall Dataset</h2><ul>
<li>The mall dataset was collected from a publicly accessible webcam for crowd counting and profiling research. Ground truth: Over 60,000 pedestrians were labelled in 2000 video frames. We annotated the data exhaustively by labelling the head position of every pedestrian in all frames. Video length: 2000 frames; Frame size: 640x480; Frame rate: &lt; 2 Hz </li>
<li>home <a href="http://personal.ie.cuhk.edu.hk/~ccloy/downloads_mall_dataset.html" target="_blank" rel="noopener">http://personal.ie.cuhk.edu.hk/~ccloy/downloads_mall_dataset.html</a></li>
<li>download url<ul>
<li><a href="http://personal.ie.cuhk.edu.hk/~ccloy/files/datasets/mall_dataset.zip" target="_blank" rel="noopener">http://personal.ie.cuhk.edu.hk/~ccloy/files/datasets/mall_dataset.zip</a></li>
</ul>
</li>
</ul>
<h2 id="UCSD-Pedestrian-Database"><a href="#UCSD-Pedestrian-Database" class="headerlink" title="UCSD Pedestrian Database"></a>UCSD Pedestrian Database</h2><ul>
<li>The database contains video of pedestrians on UCSD walkways, taken from a stationary camera. All videos are 8-bit grayscale, with dimensions 238 × 158 at 10 fps. The database is split into scenes, taken from different viewpoints (currently, only one scene is available…more are coming). Each scene is in its own directory vidX where X is a letter (e.g. vidf), and is split into video clips of length 200 named vidfXY 33 ZZZ.y, where Y and ZZZ are numbers. Finally, each video clip is saved as a set of .png files.</li>
<li>home <a href="http://www.svcl.ucsd.edu/projects/peoplecnt/" target="_blank" rel="noopener">http://www.svcl.ucsd.edu/projects/peoplecnt/</a></li>
<li>pdf <a href="http://www.svcl.ucsd.edu/projects/peoplecnt/db/readme.pdf" target="_blank" rel="noopener">http://www.svcl.ucsd.edu/projects/peoplecnt/db/readme.pdf</a></li>
<li>download url <ul>
<li><a href="http://www.svcl.ucsd.edu/projects/peoplecnt/db/ucsdpeds.zip" target="_blank" rel="noopener">http://www.svcl.ucsd.edu/projects/peoplecnt/db/ucsdpeds.zip</a> &amp;&amp; <a href="http://www.svcl.ucsd.edu/projects/peoplecnt/db/vidf-cvpr.zip" target="_blank" rel="noopener">http://www.svcl.ucsd.edu/projects/peoplecnt/db/vidf-cvpr.zip</a></li>
</ul>
</li>
</ul>
<h2 id="SmartCity-Dataset"><a href="#SmartCity-Dataset" class="headerlink" title="SmartCity Dataset"></a>SmartCity Dataset</h2><ul>
<li>We have collected a new dataset SmartCity in the paper. It consists of 50 images in total collected from ten city scenes including office entrance, sidewalk, atrium, shopping mall etc.. Some examples are shown in Fig. 4 in our arxiv paper. Unlike the existing crowd counting datasets with images of hundreds/thousands of pedestrians and nearly all the images being taken outdoors, SmartCity has few pedestrians in images and consists of both outdoor and indoor scenes: the average number of pedestrians is only 7.4 with minimum being 1 and maximum being 14. We use this set to test the generalization ability of the proposed framework on very sparse crowd scenes.</li>
<li>git <a href="https://github.com/miao0913/SaCNN-CrowdCounting-Tencent_Youtu" target="_blank" rel="noopener">https://github.com/miao0913/SaCNN-CrowdCounting-Tencent_Youtu</a></li>
<li>paper <a href="https://arxiv.org/pdf/1711.04433.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1711.04433.pdf</a></li>
<li>download url <ul>
<li><a href="https://pan.baidu.com/s/1pMuGyNp" target="_blank" rel="noopener">https://pan.baidu.com/s/1pMuGyNp</a></li>
<li><a href="https://drive.google.com/open?id=14SPZPGnLmgE3dtfNlM0UwbQD-MzAISfI" target="_blank" rel="noopener">https://drive.google.com/open?id=14SPZPGnLmgE3dtfNlM0UwbQD-MzAISfI</a></li>
</ul>
</li>
</ul>
<h2 id="AHU-Crowd-Dataset"><a href="#AHU-Crowd-Dataset" class="headerlink" title="AHU-Crowd Dataset"></a>AHU-Crowd Dataset</h2><ul>
<li>The crowd datasets are obtained a variety of sources, such as UCF and Data-driven crowd datasets to evaluate the proposed framework. The sequences are diverse, representing dense crowd in the public spaces in various scenarios such as pilgrimage, station, marathon, rallies and stadium. In addition, the sequences have different field of views, resolutions, and exhibit a multitude of motion behaviors that cover both the obvious and subtle instabilities. </li>
<li>extreme crowd</li>
<li>home <a href="http://cs-chan.com/downloads_crowd_dataset.html" target="_blank" rel="noopener">http://cs-chan.com/downloads_crowd_dataset.html</a></li>
<li>download url <ul>
<li><a href="https://drive.google.com/file/d/1pN35I5MmJA4Ase2dZRdcwsFiOM286fXc/view?usp=sharing" target="_blank" rel="noopener">https://drive.google.com/file/d/1pN35I5MmJA4Ase2dZRdcwsFiOM286fXc/view?usp=sharing</a></li>
</ul>
</li>
</ul>
<h2 id="CityStreet-Multi-View-Crowd-Counting-Dataset"><a href="#CityStreet-Multi-View-Crowd-Counting-Dataset" class="headerlink" title="CityStreet: Multi-View Crowd Counting Dataset"></a>CityStreet: Multi-View Crowd Counting Dataset</h2><ul>
<li>The multi-view crowd counting datasets, used in our “wide-area crowd counting” paper, include our proposed dataset CityStreet, as well as two existing datasets PETS2009 and DukeMTMC repurposed for multi-view crowd counting.</li>
<li>City Street: We collected a multi-view video dataset of a busy city street using 5 synchronized cameras. The videos are about 1 hour long with 2.7k (2704×1520) resolution at 30 fps. We select Cameras 1, 3 and 4 for the experiment (see Fig. 6 bottom). The cameras’ intrinsic and extrinsic parameters are estimated using the calibration algorithm from [52]. 500 multi-view images are uniformly sampled from the videos, and the first 300 are used for training and remaining 200 for testing. The ground-truth 2D and 3D annotations are obtained as follows. The head positions of the first camera-view are annotated manually, and then projected to other views and adjusted manually. Next, for the second camera view, new people (not seen in the first view), are also annotated and then projected to the other views. This process is repeated until all people in the scene are annotated and associated across all camera views. Our dataset has larger crowd numbers (70-150), compared with PETS (20-40) and DukeMTMC (10-30). Our new dataset also contains more crowd scale variations and occlusions due to vehicles and fixed structures.</li>
<li>home <a href="http://visal.cs.cityu.edu.hk/research/citystreet/" target="_blank" rel="noopener">http://visal.cs.cityu.edu.hk/research/citystreet/</a></li>
<li>paper <a href="http://visal.cs.cityu.edu.hk/static/pubs/conf/cvpr19-wacc.pdf" target="_blank" rel="noopener">http://visal.cs.cityu.edu.hk/static/pubs/conf/cvpr19-wacc.pdf</a></li>
<li>download url <ul>
<li><a href="https://drive.google.com/open?id=11hK1REG3P35S9ANXk1YB7C1-_SS_LQGJ" target="_blank" rel="noopener">https://drive.google.com/open?id=11hK1REG3P35S9ANXk1YB7C1-_SS_LQGJ</a></li>
<li><a href="https://pan.baidu.com/share/init?surl=21YyyhLX4ff6iaATHn4hWg" target="_blank" rel="noopener">https://pan.baidu.com/share/init?surl=21YyyhLX4ff6iaATHn4hWg</a>  (提取码5wca)</li>
</ul>
</li>
</ul>
<h2 id="CrowdHuman"><a href="#CrowdHuman" class="headerlink" title="CrowdHuman"></a>CrowdHuman</h2><ul>
<li>MEGVII</li>
<li>CrowdHuman is a benchmark dataset to better evaluate detectors in crowd scenarios. The CrowdHuman dataset is large, rich-annotated and contains high diversity. CrowdHuman contains 15000, 4370 and 5000 images for training, validation, and testing, respectively. There are a total of 470K human instances from train and validation subsets and 23 persons per image, with various kinds of occlusions in the dataset. Each human instance is annotated with a head bounding-box, human visible-region bounding-box and human full-body bounding-box. We hope our dataset will serve as a solid baseline and help promote future research in human detection tasks.</li>
<li>home <a href="http://www.crowdhuman.org/" target="_blank" rel="noopener">http://www.crowdhuman.org/</a></li>
<li>paper <a href="https://arxiv.org/pdf/1805.00123.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1805.00123.pdf</a></li>
<li>download <a href="http://www.crowdhuman.org/download.html" target="_blank" rel="noopener">http://www.crowdhuman.org/download.html</a></li>
<li><img src="/images/crowd_counting/CrowdHuman_dataset_1.png" alt="CrowdHuman_dataset_1"></li>
</ul>
]]></content>
      <categories>
        <category>cv</category>
      </categories>
      <tags>
        <tag>cv</tag>
        <tag>crowd_counting</tag>
      </tags>
  </entry>
  <entry>
    <title>使用 hexo + next 快速搭建github个人主页</title>
    <url>/2020/02/21/begin_with_hexo&amp;next/</url>
    <content><![CDATA[<p>介绍如何使用在github上构建next主题的个人页面<br><a id="more"></a><br><!-- toc --></p>
<h2 id="安装-hexo-和-next"><a href="#安装-hexo-和-next" class="headerlink" title="安装 hexo 和 next"></a>安装 hexo 和 next</h2><ul>
<li><p>hexo是通过npm安装的，npm是nodejs下的包管理器，类似Python里的pip</p>
<ul>
<li>nodejs下载地址: <a href="http://nodejs.cn/download/" target="_blank" rel="noopener">http://nodejs.cn/download/</a></li>
<li>nodejs在win 10下的安装和正常的软件安装无异，安装过程中有一步骤是选择安装依赖，确认后会在powershell中执行</li>
<li><p>安装完成后，打开cmd验证</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">node -v</span><br></pre></td></tr></table></figure>
<p>  v12.16.1</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm -v</span><br></pre></td></tr></table></figure>
<p>  6.13.4</p>
</li>
</ul>
</li>
<li><p>安装npm</p>
<ul>
<li>官方给出的安装指令是<code>npm install hexo-cli -g</code></li>
<li>安装完成后创建一个文件夹blog，然后执行<code>hexo init</code>完成初始化</li>
<li>执行<code>hexo s</code>启动本地预览服务，默认网址为 <a href="http://localhost:4000/" target="_blank" rel="noopener">http://localhost:4000/</a> ，可以使用 -p 指定端口</li>
</ul>
</li>
<li>安装Next主题<ul>
<li>在 hexo init 过的文件夹blog中<code>git clone https://github.com/iissnan/hexo-theme-next themes/next</code></li>
<li>修改站点根目录下的 _config.yml 文件中 theme: next</li>
</ul>
</li>
</ul>
<h2 id="申请-github-io-的公共仓库"><a href="#申请-github-io-的公共仓库" class="headerlink" title="申请 github.io 的公共仓库"></a>申请 github.io 的公共仓库</h2><ul>
<li>到个人github目录下repositories，点击new</li>
<li>Repository name 填和 ${owner}.github.io ，点击 create repository 就会生成个人主页仓库地址</li>
<li>点击进入 项目repo ，点击 clone or download ，选择 http 或 ssh 方式保存repo链接</li>
</ul>
<h2 id="将本地的hexo项目发布到github"><a href="#将本地的hexo项目发布到github" class="headerlink" title="将本地的hexo项目发布到github"></a>将本地的hexo项目发布到github</h2><ul>
<li>需要安装hexo发布到git的插件<code>npm install hexo-deployer-git --save</code></li>
<li>修改站点根目录下的 _config.yml 中的内容 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: $&#123;repo链接&#125;</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure></li>
<li>执行部署指令<code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code></li>
<li>等待执行完毕，即可以访问 ${owner}.github.io 来查看自己的博客主页</li>
</ul>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>help</tag>
        <tag>next</tag>
        <tag>github</tag>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo Help</title>
    <url>/2020/02/20/hexo_origin_readme/</url>
    <content><![CDATA[<p>hexo 原始的readme文件<br><a id="more"></a><br><!-- toc --><br>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<h3 id="Deploy-amp-Update"><a href="#Deploy-amp-Update" class="headerlink" title="Deploy &amp; Update"></a>Deploy &amp; Update</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>help</tag>
      </tags>
  </entry>
</search>
